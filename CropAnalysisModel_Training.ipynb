{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTEJT16S2KkFOMx9rdYOND",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/licialearn/Crop_Analysis/blob/main/CropAnalysisModel_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WEWnUE6FBmm",
        "outputId": "509e3168-3e43-426f-fbd4-b0127d10c9ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://ms-release.obs.cn-north-4.myhuaweicloud.com/2.5.0/MindSpore/cpu/ubuntu_x86/mindspore.html\n",
            "Collecting mindspore\n",
            "  Downloading mindspore-2.5.0-cp311-cp311-manylinux1_x86_64.whl.metadata (18 kB)\n",
            "Collecting numpy<2.0.0,>=1.20.0 (from mindspore)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.13.0 in /usr/local/lib/python3.11/dist-packages (from mindspore) (5.29.4)\n",
            "Collecting asttokens>=2.0.4 (from mindspore)\n",
            "  Downloading asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from mindspore) (11.1.0)\n",
            "Requirement already satisfied: scipy>=1.5.4 in /usr/local/lib/python3.11/dist-packages (from mindspore) (1.14.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from mindspore) (24.2)\n",
            "Requirement already satisfied: psutil>=5.6.1 in /usr/local/lib/python3.11/dist-packages (from mindspore) (5.9.5)\n",
            "Requirement already satisfied: astunparse>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from mindspore) (1.6.3)\n",
            "Requirement already satisfied: safetensors>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mindspore) (0.5.3)\n",
            "Collecting dill>=0.3.7 (from mindspore)\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.3->mindspore) (0.45.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.3->mindspore) (1.17.0)\n",
            "Downloading mindspore-2.5.0-cp311-cp311-manylinux1_x86_64.whl (962.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m962.0/962.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asttokens-3.0.0-py3-none-any.whl (26 kB)\n",
            "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, dill, asttokens, mindspore\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asttokens-3.0.0 dill-0.4.0 mindspore-2.5.0 numpy-1.26.4\n"
          ]
        }
      ],
      "source": [
        "!pip install mindspore -f https://ms-release.obs.cn-north-4.myhuaweicloud.com/2.5.0/MindSpore/cpu/ubuntu_x86/mindspore.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import mindspore as ms\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load your dataset\n",
        "df = pd.read_csv(\"/content/sample_data/agriculture_dataset.csv\")\n",
        "\n",
        "# 2. Drop columns that are not needed for prediction\n",
        "drop_columns = [\n",
        "    'High_Resolution_RGB', 'Multispectral_Images', 'Thermal_Images', 'Temporal_Images',\n",
        "    'GPS_Coordinates', 'Field_Boundaries', 'Pest_Hotspots', 'Ground_Truth_Segmentation',\n",
        "    'Bounding_Boxes', 'Crop_Health_Label'\n",
        "]\n",
        "df_model = df.drop(columns=drop_columns)\n",
        "\n",
        "# 3. Encode the target column: Crop_Type\n",
        "label_encoder = LabelEncoder()\n",
        "df_model['Crop_Type'] = label_encoder.fit_transform(df_model['Crop_Type'])\n",
        "\n",
        "# 4. Separate features and labels\n",
        "X = df_model.drop(columns=['Crop_Type'])\n",
        "y = df_model['Crop_Type']\n",
        "\n",
        "# 5. Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 6. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 7. Convert to MindSpore tensors\n",
        "X_train_tensor = ms.Tensor(X_train, dtype=ms.float32)\n",
        "y_train_tensor = ms.Tensor(y_train.to_numpy(), dtype=ms.int32)\n",
        "X_test_tensor = ms.Tensor(X_test, dtype=ms.float32)\n",
        "y_test_tensor = ms.Tensor(y_test.to_numpy(), dtype=ms.int32)\n",
        "\n",
        "# Optional: Show data shapes\n",
        "print(\"Train shape:\", X_train_tensor.shape, y_train_tensor.shape)\n",
        "print(\"Test shape:\", X_test_tensor.shape, y_test_tensor.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6ZfNChbJJot",
        "outputId": "6475235b-9435-4d1e-bb92-67e6400eeee0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (169615, 21) (169615,)\n",
            "Test shape: (42404, 21) (42404,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let‚Äôs now build and train a MindSpore neural network model to predict the Crop_Type**"
      ],
      "metadata": {
        "id": "MqS2TLAKKdyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mindspore as ms\n",
        "import mindspore.dataset as ds\n",
        "import mindspore.nn as nn\n",
        "import numpy as np\n",
        "from mindspore import Model, Tensor\n",
        "from mindspore.nn import SoftmaxCrossEntropyWithLogits, Momentum, Accuracy\n",
        "from mindspore.train.callback import LossMonitor\n",
        "\n",
        "# ============================================\n",
        "# 1. Prepare Data\n",
        "# ============================================\n",
        "X_train_np = X_train_tensor.asnumpy()\n",
        "y_train_np = y_train_tensor.asnumpy()\n",
        "X_test_np = X_test_tensor.asnumpy()\n",
        "y_test_np = y_test_tensor.asnumpy()\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_dataset = ds.NumpySlicesDataset(\n",
        "    {\"features\": X_train_np, \"labels\": y_train_np}, shuffle=True\n",
        ").batch(batch_size)\n",
        "\n",
        "test_dataset = ds.NumpySlicesDataset(\n",
        "    {\"features\": X_test_np, \"labels\": y_test_np}, shuffle=False\n",
        ").batch(batch_size)\n",
        "\n",
        "# ============================================\n",
        "# 2. Define Model\n",
        "# ============================================\n",
        "class CropNet(nn.Cell):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(CropNet, self).__init__()\n",
        "        self.fc = nn.SequentialCell([\n",
        "            nn.Dense(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dense(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dense(32, output_dim)\n",
        "        ])\n",
        "\n",
        "    def construct(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "input_dim = X_train_np.shape[1]\n",
        "num_classes = len(np.unique(y_train_np))\n",
        "net = CropNet(input_dim=input_dim, output_dim=num_classes)\n",
        "\n",
        "# ============================================\n",
        "# 3. Compile Training Setup\n",
        "# ============================================\n",
        "loss_fn = SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
        "optimizer = Momentum(net.trainable_params(), learning_rate=0.01, momentum=0.9)\n",
        "model = Model(net, loss_fn=loss_fn, optimizer=optimizer, metrics={\"Accuracy\": Accuracy()})\n",
        "\n",
        "# ============================================\n",
        "# 4. Train\n",
        "# ============================================\n",
        "print(\"üöÄ Training...\")\n",
        "model.train(\n",
        "    epoch=20,\n",
        "    train_dataset=train_dataset,\n",
        "    callbacks=[LossMonitor()],\n",
        "    dataset_sink_mode=False\n",
        ")\n",
        "\n",
        "# ============================================\n",
        "# 5. Evaluate\n",
        "# ============================================\n",
        "print(\"üß™ Evaluating...\")\n",
        "accuracy = model.eval(test_dataset, dataset_sink_mode=False)\n",
        "print(\"‚úÖ Test Accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Buls_TZ0S9Ur",
        "outputId": "b650ad68-ea21-47c8-aa9b-a1d4e089fdba"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "epoch: 20 step: 304, loss is 0.9320175051689148\n",
            "epoch: 20 step: 305, loss is 0.989750325679779\n",
            "epoch: 20 step: 306, loss is 1.0826568603515625\n",
            "epoch: 20 step: 307, loss is 0.8756831884384155\n",
            "epoch: 20 step: 308, loss is 0.9403860569000244\n",
            "epoch: 20 step: 309, loss is 0.9436812996864319\n",
            "epoch: 20 step: 310, loss is 0.9298222661018372\n",
            "epoch: 20 step: 311, loss is 0.8649150729179382\n",
            "epoch: 20 step: 312, loss is 0.8255336284637451\n",
            "epoch: 20 step: 313, loss is 0.9096280336380005\n",
            "epoch: 20 step: 314, loss is 0.8864724636077881\n",
            "epoch: 20 step: 315, loss is 0.9062723517417908\n",
            "epoch: 20 step: 316, loss is 0.7365908026695251\n",
            "epoch: 20 step: 317, loss is 0.8825295567512512\n",
            "epoch: 20 step: 318, loss is 1.086747169494629\n",
            "epoch: 20 step: 319, loss is 0.8463107347488403\n",
            "epoch: 20 step: 320, loss is 0.8600242733955383\n",
            "epoch: 20 step: 321, loss is 0.9896159768104553\n",
            "epoch: 20 step: 322, loss is 0.9727312326431274\n",
            "epoch: 20 step: 323, loss is 1.1299818754196167\n",
            "epoch: 20 step: 324, loss is 0.7618857622146606\n",
            "epoch: 20 step: 325, loss is 0.9167613387107849\n",
            "epoch: 20 step: 326, loss is 0.7425330281257629\n",
            "epoch: 20 step: 327, loss is 0.747456431388855\n",
            "epoch: 20 step: 328, loss is 0.8497211337089539\n",
            "epoch: 20 step: 329, loss is 0.7937755584716797\n",
            "epoch: 20 step: 330, loss is 0.8924088478088379\n",
            "epoch: 20 step: 331, loss is 0.8429964184761047\n",
            "epoch: 20 step: 332, loss is 0.893494188785553\n",
            "epoch: 20 step: 333, loss is 0.9187399744987488\n",
            "epoch: 20 step: 334, loss is 0.8779133558273315\n",
            "epoch: 20 step: 335, loss is 0.7691946625709534\n",
            "epoch: 20 step: 336, loss is 1.0435330867767334\n",
            "epoch: 20 step: 337, loss is 0.789690375328064\n",
            "epoch: 20 step: 338, loss is 1.0719387531280518\n",
            "epoch: 20 step: 339, loss is 0.8594577312469482\n",
            "epoch: 20 step: 340, loss is 0.7803994417190552\n",
            "epoch: 20 step: 341, loss is 0.9046805500984192\n",
            "epoch: 20 step: 342, loss is 0.95851731300354\n",
            "epoch: 20 step: 343, loss is 0.9068290591239929\n",
            "epoch: 20 step: 344, loss is 0.895450234413147\n",
            "epoch: 20 step: 345, loss is 0.9405639171600342\n",
            "epoch: 20 step: 346, loss is 0.8075962662696838\n",
            "epoch: 20 step: 347, loss is 1.0946195125579834\n",
            "epoch: 20 step: 348, loss is 0.9590075016021729\n",
            "epoch: 20 step: 349, loss is 0.9383224844932556\n",
            "epoch: 20 step: 350, loss is 0.9860200881958008\n",
            "epoch: 20 step: 351, loss is 0.850568413734436\n",
            "epoch: 20 step: 352, loss is 1.1343019008636475\n",
            "epoch: 20 step: 353, loss is 0.8829046487808228\n",
            "epoch: 20 step: 354, loss is 0.9748778939247131\n",
            "epoch: 20 step: 355, loss is 0.9835113286972046\n",
            "epoch: 20 step: 356, loss is 0.8632205128669739\n",
            "epoch: 20 step: 357, loss is 0.7148558497428894\n",
            "epoch: 20 step: 358, loss is 0.8765313029289246\n",
            "epoch: 20 step: 359, loss is 0.8644236326217651\n",
            "epoch: 20 step: 360, loss is 0.8108803033828735\n",
            "epoch: 20 step: 361, loss is 0.7850539684295654\n",
            "epoch: 20 step: 362, loss is 1.0043044090270996\n",
            "epoch: 20 step: 363, loss is 0.8215892910957336\n",
            "epoch: 20 step: 364, loss is 0.9735835790634155\n",
            "epoch: 20 step: 365, loss is 0.8813152313232422\n",
            "epoch: 20 step: 366, loss is 0.9255030751228333\n",
            "epoch: 20 step: 367, loss is 1.186028242111206\n",
            "epoch: 20 step: 368, loss is 1.0838607549667358\n",
            "epoch: 20 step: 369, loss is 0.8944405913352966\n",
            "epoch: 20 step: 370, loss is 1.0091410875320435\n",
            "epoch: 20 step: 371, loss is 0.883881151676178\n",
            "epoch: 20 step: 372, loss is 0.9712207317352295\n",
            "epoch: 20 step: 373, loss is 0.9276652336120605\n",
            "epoch: 20 step: 374, loss is 0.6966440081596375\n",
            "epoch: 20 step: 375, loss is 0.797425389289856\n",
            "epoch: 20 step: 376, loss is 0.8202710747718811\n",
            "epoch: 20 step: 377, loss is 0.8733103275299072\n",
            "epoch: 20 step: 378, loss is 0.9774816036224365\n",
            "epoch: 20 step: 379, loss is 0.7817606329917908\n",
            "epoch: 20 step: 380, loss is 0.7668846249580383\n",
            "epoch: 20 step: 381, loss is 0.9070655107498169\n",
            "epoch: 20 step: 382, loss is 0.8567019701004028\n",
            "epoch: 20 step: 383, loss is 0.8086033463478088\n",
            "epoch: 20 step: 384, loss is 0.7561507225036621\n",
            "epoch: 20 step: 385, loss is 1.028841495513916\n",
            "epoch: 20 step: 386, loss is 0.9274039268493652\n",
            "epoch: 20 step: 387, loss is 1.0502780675888062\n",
            "epoch: 20 step: 388, loss is 0.8816951513290405\n",
            "epoch: 20 step: 389, loss is 0.9411890506744385\n",
            "epoch: 20 step: 390, loss is 0.8097037672996521\n",
            "epoch: 20 step: 391, loss is 0.8470196723937988\n",
            "epoch: 20 step: 392, loss is 1.0195908546447754\n",
            "epoch: 20 step: 393, loss is 0.8916633725166321\n",
            "epoch: 20 step: 394, loss is 0.8121368885040283\n",
            "epoch: 20 step: 395, loss is 0.9327720403671265\n",
            "epoch: 20 step: 396, loss is 0.8270926475524902\n",
            "epoch: 20 step: 397, loss is 0.7946515679359436\n",
            "epoch: 20 step: 398, loss is 0.9673997163772583\n",
            "epoch: 20 step: 399, loss is 0.8483068346977234\n",
            "epoch: 20 step: 400, loss is 0.8632749319076538\n",
            "epoch: 20 step: 401, loss is 0.7650349140167236\n",
            "epoch: 20 step: 402, loss is 0.7893718481063843\n",
            "epoch: 20 step: 403, loss is 1.000641107559204\n",
            "epoch: 20 step: 404, loss is 0.8865463137626648\n",
            "epoch: 20 step: 405, loss is 0.927192211151123\n",
            "epoch: 20 step: 406, loss is 0.9294880628585815\n",
            "epoch: 20 step: 407, loss is 1.0026397705078125\n",
            "epoch: 20 step: 408, loss is 0.8014716506004333\n",
            "epoch: 20 step: 409, loss is 0.8217422366142273\n",
            "epoch: 20 step: 410, loss is 0.9407460689544678\n",
            "epoch: 20 step: 411, loss is 0.8584550619125366\n",
            "epoch: 20 step: 412, loss is 0.8340737819671631\n",
            "epoch: 20 step: 413, loss is 0.9884310364723206\n",
            "epoch: 20 step: 414, loss is 0.7535676956176758\n",
            "epoch: 20 step: 415, loss is 0.8747957944869995\n",
            "epoch: 20 step: 416, loss is 0.9919583797454834\n",
            "epoch: 20 step: 417, loss is 0.9070767164230347\n",
            "epoch: 20 step: 418, loss is 0.9037564396858215\n",
            "epoch: 20 step: 419, loss is 1.037155032157898\n",
            "epoch: 20 step: 420, loss is 0.8159900903701782\n",
            "epoch: 20 step: 421, loss is 0.889256477355957\n",
            "epoch: 20 step: 422, loss is 0.8753598928451538\n",
            "epoch: 20 step: 423, loss is 0.9517962336540222\n",
            "epoch: 20 step: 424, loss is 0.9844660758972168\n",
            "epoch: 20 step: 425, loss is 1.008330225944519\n",
            "epoch: 20 step: 426, loss is 0.7594220638275146\n",
            "epoch: 20 step: 427, loss is 0.9774497151374817\n",
            "epoch: 20 step: 428, loss is 0.879574716091156\n",
            "epoch: 20 step: 429, loss is 0.8776606321334839\n",
            "epoch: 20 step: 430, loss is 0.8842355608940125\n",
            "epoch: 20 step: 431, loss is 0.844358503818512\n",
            "epoch: 20 step: 432, loss is 1.01087486743927\n",
            "epoch: 20 step: 433, loss is 0.814028799533844\n",
            "epoch: 20 step: 434, loss is 0.9037473797798157\n",
            "epoch: 20 step: 435, loss is 0.8686786890029907\n",
            "epoch: 20 step: 436, loss is 0.8311266899108887\n",
            "epoch: 20 step: 437, loss is 0.9514694213867188\n",
            "epoch: 20 step: 438, loss is 0.9932897090911865\n",
            "epoch: 20 step: 439, loss is 0.855728268623352\n",
            "epoch: 20 step: 440, loss is 1.031518578529358\n",
            "epoch: 20 step: 441, loss is 1.0750797986984253\n",
            "epoch: 20 step: 442, loss is 0.9299677014350891\n",
            "epoch: 20 step: 443, loss is 1.0478135347366333\n",
            "epoch: 20 step: 444, loss is 0.8652669787406921\n",
            "epoch: 20 step: 445, loss is 0.9329288005828857\n",
            "epoch: 20 step: 446, loss is 0.812724769115448\n",
            "epoch: 20 step: 447, loss is 0.9234488010406494\n",
            "epoch: 20 step: 448, loss is 0.7932714223861694\n",
            "epoch: 20 step: 449, loss is 0.8293092250823975\n",
            "epoch: 20 step: 450, loss is 0.978015124797821\n",
            "epoch: 20 step: 451, loss is 0.8504891395568848\n",
            "epoch: 20 step: 452, loss is 0.6797806024551392\n",
            "epoch: 20 step: 453, loss is 0.8701998591423035\n",
            "epoch: 20 step: 454, loss is 0.8102778196334839\n",
            "epoch: 20 step: 455, loss is 0.9310847520828247\n",
            "epoch: 20 step: 456, loss is 0.8780542612075806\n",
            "epoch: 20 step: 457, loss is 0.963887095451355\n",
            "epoch: 20 step: 458, loss is 0.8510035276412964\n",
            "epoch: 20 step: 459, loss is 1.013768196105957\n",
            "epoch: 20 step: 460, loss is 0.7676377892494202\n",
            "epoch: 20 step: 461, loss is 0.6872521638870239\n",
            "epoch: 20 step: 462, loss is 0.8214758038520813\n",
            "epoch: 20 step: 463, loss is 0.7557834982872009\n",
            "epoch: 20 step: 464, loss is 0.9371728897094727\n",
            "epoch: 20 step: 465, loss is 0.898354709148407\n",
            "epoch: 20 step: 466, loss is 0.751096248626709\n",
            "epoch: 20 step: 467, loss is 0.9283171892166138\n",
            "epoch: 20 step: 468, loss is 0.9426819682121277\n",
            "epoch: 20 step: 469, loss is 0.8875777721405029\n",
            "epoch: 20 step: 470, loss is 0.6767378449440002\n",
            "epoch: 20 step: 471, loss is 0.8733722567558289\n",
            "epoch: 20 step: 472, loss is 0.7534492015838623\n",
            "epoch: 20 step: 473, loss is 0.9030376672744751\n",
            "epoch: 20 step: 474, loss is 0.8955218195915222\n",
            "epoch: 20 step: 475, loss is 0.8573427796363831\n",
            "epoch: 20 step: 476, loss is 1.1218841075897217\n",
            "epoch: 20 step: 477, loss is 1.1134949922561646\n",
            "epoch: 20 step: 478, loss is 0.8922209739685059\n",
            "epoch: 20 step: 479, loss is 0.9045654535293579\n",
            "epoch: 20 step: 480, loss is 0.8292733430862427\n",
            "epoch: 20 step: 481, loss is 0.9396079182624817\n",
            "epoch: 20 step: 482, loss is 1.068595290184021\n",
            "epoch: 20 step: 483, loss is 0.9899314045906067\n",
            "epoch: 20 step: 484, loss is 0.9706384539604187\n",
            "epoch: 20 step: 485, loss is 1.0411733388900757\n",
            "epoch: 20 step: 486, loss is 0.9971317052841187\n",
            "epoch: 20 step: 487, loss is 0.8111999034881592\n",
            "epoch: 20 step: 488, loss is 0.7779205441474915\n",
            "epoch: 20 step: 489, loss is 0.8312928080558777\n",
            "epoch: 20 step: 490, loss is 1.0492411851882935\n",
            "epoch: 20 step: 491, loss is 0.7250450849533081\n",
            "epoch: 20 step: 492, loss is 0.7892621755599976\n",
            "epoch: 20 step: 493, loss is 0.861064076423645\n",
            "epoch: 20 step: 494, loss is 1.0991305112838745\n",
            "epoch: 20 step: 495, loss is 0.8888083100318909\n",
            "epoch: 20 step: 496, loss is 0.9476961493492126\n",
            "epoch: 20 step: 497, loss is 1.0204349756240845\n",
            "epoch: 20 step: 498, loss is 1.0073355436325073\n",
            "epoch: 20 step: 499, loss is 0.8704531192779541\n",
            "epoch: 20 step: 500, loss is 1.021803855895996\n",
            "epoch: 20 step: 501, loss is 1.0454298257827759\n",
            "epoch: 20 step: 502, loss is 0.8641626238822937\n",
            "epoch: 20 step: 503, loss is 0.9558558464050293\n",
            "epoch: 20 step: 504, loss is 0.8378947973251343\n",
            "epoch: 20 step: 505, loss is 0.9245526194572449\n",
            "epoch: 20 step: 506, loss is 0.9963658452033997\n",
            "epoch: 20 step: 507, loss is 0.8471539616584778\n",
            "epoch: 20 step: 508, loss is 0.7373877763748169\n",
            "epoch: 20 step: 509, loss is 0.8852820992469788\n",
            "epoch: 20 step: 510, loss is 0.8623536825180054\n",
            "epoch: 20 step: 511, loss is 0.761391282081604\n",
            "epoch: 20 step: 512, loss is 1.024058222770691\n",
            "epoch: 20 step: 513, loss is 1.0900605916976929\n",
            "epoch: 20 step: 514, loss is 0.8863220810890198\n",
            "epoch: 20 step: 515, loss is 1.0021824836730957\n",
            "epoch: 20 step: 516, loss is 0.8069657683372498\n",
            "epoch: 20 step: 517, loss is 1.023898720741272\n",
            "epoch: 20 step: 518, loss is 0.8821718096733093\n",
            "epoch: 20 step: 519, loss is 1.0726101398468018\n",
            "epoch: 20 step: 520, loss is 0.9974689483642578\n",
            "epoch: 20 step: 521, loss is 0.8618784546852112\n",
            "epoch: 20 step: 522, loss is 0.9542112946510315\n",
            "epoch: 20 step: 523, loss is 1.0131851434707642\n",
            "epoch: 20 step: 524, loss is 0.9327718019485474\n",
            "epoch: 20 step: 525, loss is 0.8250648379325867\n",
            "epoch: 20 step: 526, loss is 0.889461100101471\n",
            "epoch: 20 step: 527, loss is 0.8751835823059082\n",
            "epoch: 20 step: 528, loss is 0.7415708303451538\n",
            "epoch: 20 step: 529, loss is 0.8697916269302368\n",
            "epoch: 20 step: 530, loss is 0.8905041813850403\n",
            "epoch: 20 step: 531, loss is 0.8682228326797485\n",
            "epoch: 20 step: 532, loss is 0.8746225833892822\n",
            "epoch: 20 step: 533, loss is 0.8741788268089294\n",
            "epoch: 20 step: 534, loss is 0.8411096930503845\n",
            "epoch: 20 step: 535, loss is 0.7390228509902954\n",
            "epoch: 20 step: 536, loss is 0.9952853322029114\n",
            "epoch: 20 step: 537, loss is 0.9966202974319458\n",
            "epoch: 20 step: 538, loss is 0.8532355427742004\n",
            "epoch: 20 step: 539, loss is 1.0502417087554932\n",
            "epoch: 20 step: 540, loss is 0.8653470873832703\n",
            "epoch: 20 step: 541, loss is 0.9824361205101013\n",
            "epoch: 20 step: 542, loss is 0.8280161619186401\n",
            "epoch: 20 step: 543, loss is 0.665134847164154\n",
            "epoch: 20 step: 544, loss is 0.8392309546470642\n",
            "epoch: 20 step: 545, loss is 0.8047327399253845\n",
            "epoch: 20 step: 546, loss is 0.9366879463195801\n",
            "epoch: 20 step: 547, loss is 0.6168003678321838\n",
            "epoch: 20 step: 548, loss is 0.9165270328521729\n",
            "epoch: 20 step: 549, loss is 0.8300135731697083\n",
            "epoch: 20 step: 550, loss is 0.699195384979248\n",
            "epoch: 20 step: 551, loss is 0.8695901036262512\n",
            "epoch: 20 step: 552, loss is 0.94371497631073\n",
            "epoch: 20 step: 553, loss is 0.7706383466720581\n",
            "epoch: 20 step: 554, loss is 0.9064315557479858\n",
            "epoch: 20 step: 555, loss is 0.8419613242149353\n",
            "epoch: 20 step: 556, loss is 0.9955627918243408\n",
            "epoch: 20 step: 557, loss is 0.9023571610450745\n",
            "epoch: 20 step: 558, loss is 0.9907887578010559\n",
            "epoch: 20 step: 559, loss is 0.7731536626815796\n",
            "epoch: 20 step: 560, loss is 0.909690260887146\n",
            "epoch: 20 step: 561, loss is 0.7685748934745789\n",
            "epoch: 20 step: 562, loss is 1.080198049545288\n",
            "epoch: 20 step: 563, loss is 0.6666034460067749\n",
            "epoch: 20 step: 564, loss is 0.9759523868560791\n",
            "epoch: 20 step: 565, loss is 0.9767337441444397\n",
            "epoch: 20 step: 566, loss is 0.802943229675293\n",
            "epoch: 20 step: 567, loss is 0.692852795124054\n",
            "epoch: 20 step: 568, loss is 0.9274877309799194\n",
            "epoch: 20 step: 569, loss is 0.9270458817481995\n",
            "epoch: 20 step: 570, loss is 1.0069551467895508\n",
            "epoch: 20 step: 571, loss is 0.9030793309211731\n",
            "epoch: 20 step: 572, loss is 0.900288999080658\n",
            "epoch: 20 step: 573, loss is 0.8378617167472839\n",
            "epoch: 20 step: 574, loss is 0.9817587733268738\n",
            "epoch: 20 step: 575, loss is 0.9338572025299072\n",
            "epoch: 20 step: 576, loss is 0.9650478959083557\n",
            "epoch: 20 step: 577, loss is 1.0446363687515259\n",
            "epoch: 20 step: 578, loss is 0.9105804562568665\n",
            "epoch: 20 step: 579, loss is 0.8415812849998474\n",
            "epoch: 20 step: 580, loss is 0.8427075147628784\n",
            "epoch: 20 step: 581, loss is 1.1584405899047852\n",
            "epoch: 20 step: 582, loss is 0.7967082858085632\n",
            "epoch: 20 step: 583, loss is 0.8206236958503723\n",
            "epoch: 20 step: 584, loss is 0.8583388924598694\n",
            "epoch: 20 step: 585, loss is 0.9942949414253235\n",
            "epoch: 20 step: 586, loss is 0.8535592555999756\n",
            "epoch: 20 step: 587, loss is 0.949776291847229\n",
            "epoch: 20 step: 588, loss is 0.7691826224327087\n",
            "epoch: 20 step: 589, loss is 0.9851504564285278\n",
            "epoch: 20 step: 590, loss is 0.8983187079429626\n",
            "epoch: 20 step: 591, loss is 0.8932430148124695\n",
            "epoch: 20 step: 592, loss is 0.7613862156867981\n",
            "epoch: 20 step: 593, loss is 0.8641365766525269\n",
            "epoch: 20 step: 594, loss is 0.8249378204345703\n",
            "epoch: 20 step: 595, loss is 0.9233640432357788\n",
            "epoch: 20 step: 596, loss is 1.003147006034851\n",
            "epoch: 20 step: 597, loss is 0.9738777279853821\n",
            "epoch: 20 step: 598, loss is 0.796275794506073\n",
            "epoch: 20 step: 599, loss is 0.812136709690094\n",
            "epoch: 20 step: 600, loss is 0.8123726844787598\n",
            "epoch: 20 step: 601, loss is 0.9838536381721497\n",
            "epoch: 20 step: 602, loss is 1.0077687501907349\n",
            "epoch: 20 step: 603, loss is 0.944426417350769\n",
            "epoch: 20 step: 604, loss is 0.9872468113899231\n",
            "epoch: 20 step: 605, loss is 1.0252920389175415\n",
            "epoch: 20 step: 606, loss is 0.7326667308807373\n",
            "epoch: 20 step: 607, loss is 0.9579118490219116\n",
            "epoch: 20 step: 608, loss is 0.8500249981880188\n",
            "epoch: 20 step: 609, loss is 0.8959139585494995\n",
            "epoch: 20 step: 610, loss is 0.9761791229248047\n",
            "epoch: 20 step: 611, loss is 0.827516496181488\n",
            "epoch: 20 step: 612, loss is 0.8451956510543823\n",
            "epoch: 20 step: 613, loss is 0.9376654028892517\n",
            "epoch: 20 step: 614, loss is 0.8939263820648193\n",
            "epoch: 20 step: 615, loss is 0.8599885702133179\n",
            "epoch: 20 step: 616, loss is 0.9633074402809143\n",
            "epoch: 20 step: 617, loss is 0.8718596696853638\n",
            "epoch: 20 step: 618, loss is 0.7786031365394592\n",
            "epoch: 20 step: 619, loss is 0.8338825702667236\n",
            "epoch: 20 step: 620, loss is 0.9228546023368835\n",
            "epoch: 20 step: 621, loss is 0.8506183624267578\n",
            "epoch: 20 step: 622, loss is 1.0008002519607544\n",
            "epoch: 20 step: 623, loss is 0.9658383131027222\n",
            "epoch: 20 step: 624, loss is 1.0450390577316284\n",
            "epoch: 20 step: 625, loss is 0.8463846445083618\n",
            "epoch: 20 step: 626, loss is 1.0148606300354004\n",
            "epoch: 20 step: 627, loss is 0.757585346698761\n",
            "epoch: 20 step: 628, loss is 0.7270542979240417\n",
            "epoch: 20 step: 629, loss is 0.8851273655891418\n",
            "epoch: 20 step: 630, loss is 0.7532314658164978\n",
            "epoch: 20 step: 631, loss is 0.8604713082313538\n",
            "epoch: 20 step: 632, loss is 0.9467954039573669\n",
            "epoch: 20 step: 633, loss is 0.765917956829071\n",
            "epoch: 20 step: 634, loss is 0.9738678336143494\n",
            "epoch: 20 step: 635, loss is 0.8691112399101257\n",
            "epoch: 20 step: 636, loss is 0.9462938904762268\n",
            "epoch: 20 step: 637, loss is 0.9005110263824463\n",
            "epoch: 20 step: 638, loss is 0.9521327018737793\n",
            "epoch: 20 step: 639, loss is 0.7505818605422974\n",
            "epoch: 20 step: 640, loss is 0.9036968350410461\n",
            "epoch: 20 step: 641, loss is 1.0520727634429932\n",
            "epoch: 20 step: 642, loss is 0.939315140247345\n",
            "epoch: 20 step: 643, loss is 0.7785881757736206\n",
            "epoch: 20 step: 644, loss is 0.907103955745697\n",
            "epoch: 20 step: 645, loss is 0.9336278438568115\n",
            "epoch: 20 step: 646, loss is 0.922235369682312\n",
            "epoch: 20 step: 647, loss is 0.9450990557670593\n",
            "epoch: 20 step: 648, loss is 0.9055784940719604\n",
            "epoch: 20 step: 649, loss is 0.942104697227478\n",
            "epoch: 20 step: 650, loss is 0.8427484631538391\n",
            "epoch: 20 step: 651, loss is 0.8250396251678467\n",
            "epoch: 20 step: 652, loss is 0.7281754016876221\n",
            "epoch: 20 step: 653, loss is 0.9772194623947144\n",
            "epoch: 20 step: 654, loss is 0.9429886937141418\n",
            "epoch: 20 step: 655, loss is 0.8281605839729309\n",
            "epoch: 20 step: 656, loss is 0.9308212399482727\n",
            "epoch: 20 step: 657, loss is 0.9714750647544861\n",
            "epoch: 20 step: 658, loss is 0.7920414209365845\n",
            "epoch: 20 step: 659, loss is 0.9656150341033936\n",
            "epoch: 20 step: 660, loss is 0.9109866619110107\n",
            "epoch: 20 step: 661, loss is 0.8873842358589172\n",
            "epoch: 20 step: 662, loss is 0.7110317945480347\n",
            "epoch: 20 step: 663, loss is 0.8416198492050171\n",
            "epoch: 20 step: 664, loss is 0.8364012241363525\n",
            "epoch: 20 step: 665, loss is 0.8566057682037354\n",
            "epoch: 20 step: 666, loss is 0.8622781038284302\n",
            "epoch: 20 step: 667, loss is 1.2521655559539795\n",
            "epoch: 20 step: 668, loss is 0.8275720477104187\n",
            "epoch: 20 step: 669, loss is 0.7650033235549927\n",
            "epoch: 20 step: 670, loss is 0.8725613951683044\n",
            "epoch: 20 step: 671, loss is 0.8679159879684448\n",
            "epoch: 20 step: 672, loss is 0.766743540763855\n",
            "epoch: 20 step: 673, loss is 0.9377681612968445\n",
            "epoch: 20 step: 674, loss is 0.829910397529602\n",
            "epoch: 20 step: 675, loss is 0.748976469039917\n",
            "epoch: 20 step: 676, loss is 0.9338254928588867\n",
            "epoch: 20 step: 677, loss is 0.8008543252944946\n",
            "epoch: 20 step: 678, loss is 0.9210131764411926\n",
            "epoch: 20 step: 679, loss is 1.0162004232406616\n",
            "epoch: 20 step: 680, loss is 0.9879795908927917\n",
            "epoch: 20 step: 681, loss is 0.8341332674026489\n",
            "epoch: 20 step: 682, loss is 0.7806459069252014\n",
            "epoch: 20 step: 683, loss is 0.8211333155632019\n",
            "epoch: 20 step: 684, loss is 0.7158180475234985\n",
            "epoch: 20 step: 685, loss is 0.7375452518463135\n",
            "epoch: 20 step: 686, loss is 0.8647980690002441\n",
            "epoch: 20 step: 687, loss is 0.8168356418609619\n",
            "epoch: 20 step: 688, loss is 1.0808384418487549\n",
            "epoch: 20 step: 689, loss is 0.8159446120262146\n",
            "epoch: 20 step: 690, loss is 0.9429975748062134\n",
            "epoch: 20 step: 691, loss is 0.8599317669868469\n",
            "epoch: 20 step: 692, loss is 0.7922641634941101\n",
            "epoch: 20 step: 693, loss is 0.903445839881897\n",
            "epoch: 20 step: 694, loss is 0.8898429274559021\n",
            "epoch: 20 step: 695, loss is 1.026848554611206\n",
            "epoch: 20 step: 696, loss is 0.9528332948684692\n",
            "epoch: 20 step: 697, loss is 1.016394019126892\n",
            "epoch: 20 step: 698, loss is 0.8050567507743835\n",
            "epoch: 20 step: 699, loss is 0.767130434513092\n",
            "epoch: 20 step: 700, loss is 0.8985137343406677\n",
            "epoch: 20 step: 701, loss is 0.8120771050453186\n",
            "epoch: 20 step: 702, loss is 0.706678032875061\n",
            "epoch: 20 step: 703, loss is 1.096185564994812\n",
            "epoch: 20 step: 704, loss is 0.9994707107543945\n",
            "epoch: 20 step: 705, loss is 0.911117434501648\n",
            "epoch: 20 step: 706, loss is 1.0078816413879395\n",
            "epoch: 20 step: 707, loss is 0.7178661823272705\n",
            "epoch: 20 step: 708, loss is 0.93418288230896\n",
            "epoch: 20 step: 709, loss is 0.873559832572937\n",
            "epoch: 20 step: 710, loss is 1.0212315320968628\n",
            "epoch: 20 step: 711, loss is 0.8903168439865112\n",
            "epoch: 20 step: 712, loss is 1.0058614015579224\n",
            "epoch: 20 step: 713, loss is 0.924312949180603\n",
            "epoch: 20 step: 714, loss is 0.7818163633346558\n",
            "epoch: 20 step: 715, loss is 0.6987048983573914\n",
            "epoch: 20 step: 716, loss is 1.091119647026062\n",
            "epoch: 20 step: 717, loss is 0.9649848937988281\n",
            "epoch: 20 step: 718, loss is 1.1150896549224854\n",
            "epoch: 20 step: 719, loss is 1.0245134830474854\n",
            "epoch: 20 step: 720, loss is 0.9915698766708374\n",
            "epoch: 20 step: 721, loss is 0.8195968866348267\n",
            "epoch: 20 step: 722, loss is 0.8146958947181702\n",
            "epoch: 20 step: 723, loss is 0.8289650678634644\n",
            "epoch: 20 step: 724, loss is 0.8629723787307739\n",
            "epoch: 20 step: 725, loss is 0.7934980988502502\n",
            "epoch: 20 step: 726, loss is 0.9084409475326538\n",
            "epoch: 20 step: 727, loss is 0.7775228023529053\n",
            "epoch: 20 step: 728, loss is 1.0242973566055298\n",
            "epoch: 20 step: 729, loss is 0.922200620174408\n",
            "epoch: 20 step: 730, loss is 0.8154472708702087\n",
            "epoch: 20 step: 731, loss is 0.9994379281997681\n",
            "epoch: 20 step: 732, loss is 0.8444728851318359\n",
            "epoch: 20 step: 733, loss is 0.8710474967956543\n",
            "epoch: 20 step: 734, loss is 0.8785355091094971\n",
            "epoch: 20 step: 735, loss is 1.130367398262024\n",
            "epoch: 20 step: 736, loss is 0.7552407383918762\n",
            "epoch: 20 step: 737, loss is 0.71956866979599\n",
            "epoch: 20 step: 738, loss is 0.7914586663246155\n",
            "epoch: 20 step: 739, loss is 1.0353091955184937\n",
            "epoch: 20 step: 740, loss is 0.8368895649909973\n",
            "epoch: 20 step: 741, loss is 0.7256036400794983\n",
            "epoch: 20 step: 742, loss is 0.8863117694854736\n",
            "epoch: 20 step: 743, loss is 0.8102036118507385\n",
            "epoch: 20 step: 744, loss is 0.7261606454849243\n",
            "epoch: 20 step: 745, loss is 0.8678827285766602\n",
            "epoch: 20 step: 746, loss is 1.0209498405456543\n",
            "epoch: 20 step: 747, loss is 0.8697191476821899\n",
            "epoch: 20 step: 748, loss is 0.8444218039512634\n",
            "epoch: 20 step: 749, loss is 0.8919492959976196\n",
            "epoch: 20 step: 750, loss is 0.9030468463897705\n",
            "epoch: 20 step: 751, loss is 0.8970683813095093\n",
            "epoch: 20 step: 752, loss is 0.849945604801178\n",
            "epoch: 20 step: 753, loss is 0.958579957485199\n",
            "epoch: 20 step: 754, loss is 1.0313633680343628\n",
            "epoch: 20 step: 755, loss is 0.7606101632118225\n",
            "epoch: 20 step: 756, loss is 1.104669451713562\n",
            "epoch: 20 step: 757, loss is 0.9003802537918091\n",
            "epoch: 20 step: 758, loss is 0.8974704146385193\n",
            "epoch: 20 step: 759, loss is 1.0125417709350586\n",
            "epoch: 20 step: 760, loss is 0.8495340347290039\n",
            "epoch: 20 step: 761, loss is 0.8685429692268372\n",
            "epoch: 20 step: 762, loss is 0.8207802772521973\n",
            "epoch: 20 step: 763, loss is 0.8957559466362\n",
            "epoch: 20 step: 764, loss is 0.9843084812164307\n",
            "epoch: 20 step: 765, loss is 0.7516411542892456\n",
            "epoch: 20 step: 766, loss is 0.8167832493782043\n",
            "epoch: 20 step: 767, loss is 1.0827218294143677\n",
            "epoch: 20 step: 768, loss is 0.8403708338737488\n",
            "epoch: 20 step: 769, loss is 0.8826384544372559\n",
            "epoch: 20 step: 770, loss is 0.916475772857666\n",
            "epoch: 20 step: 771, loss is 0.8968747854232788\n",
            "epoch: 20 step: 772, loss is 0.7363197803497314\n",
            "epoch: 20 step: 773, loss is 0.7178350687026978\n",
            "epoch: 20 step: 774, loss is 0.8164401650428772\n",
            "epoch: 20 step: 775, loss is 1.047092318534851\n",
            "epoch: 20 step: 776, loss is 0.9655073285102844\n",
            "epoch: 20 step: 777, loss is 0.7584044337272644\n",
            "epoch: 20 step: 778, loss is 0.770780086517334\n",
            "epoch: 20 step: 779, loss is 0.9488361477851868\n",
            "epoch: 20 step: 780, loss is 0.7748854160308838\n",
            "epoch: 20 step: 781, loss is 0.7803337574005127\n",
            "epoch: 20 step: 782, loss is 0.9081764221191406\n",
            "epoch: 20 step: 783, loss is 0.9331798553466797\n",
            "epoch: 20 step: 784, loss is 1.035840630531311\n",
            "epoch: 20 step: 785, loss is 0.951092004776001\n",
            "epoch: 20 step: 786, loss is 0.6815172433853149\n",
            "epoch: 20 step: 787, loss is 1.0216509103775024\n",
            "epoch: 20 step: 788, loss is 0.9287324547767639\n",
            "epoch: 20 step: 789, loss is 0.890313446521759\n",
            "epoch: 20 step: 790, loss is 0.8784453272819519\n",
            "epoch: 20 step: 791, loss is 1.0011470317840576\n",
            "epoch: 20 step: 792, loss is 0.9465519189834595\n",
            "epoch: 20 step: 793, loss is 1.2038249969482422\n",
            "epoch: 20 step: 794, loss is 0.7973429560661316\n",
            "epoch: 20 step: 795, loss is 0.9147063493728638\n",
            "epoch: 20 step: 796, loss is 0.98397296667099\n",
            "epoch: 20 step: 797, loss is 0.9503635764122009\n",
            "epoch: 20 step: 798, loss is 1.1615644693374634\n",
            "epoch: 20 step: 799, loss is 0.9528716802597046\n",
            "epoch: 20 step: 800, loss is 0.8454002737998962\n",
            "epoch: 20 step: 801, loss is 0.8119192719459534\n",
            "epoch: 20 step: 802, loss is 0.9489469528198242\n",
            "epoch: 20 step: 803, loss is 0.8735767006874084\n",
            "epoch: 20 step: 804, loss is 0.9230622053146362\n",
            "epoch: 20 step: 805, loss is 0.979875385761261\n",
            "epoch: 20 step: 806, loss is 0.9268996715545654\n",
            "epoch: 20 step: 807, loss is 0.8550452589988708\n",
            "epoch: 20 step: 808, loss is 0.8726487755775452\n",
            "epoch: 20 step: 809, loss is 0.9974082708358765\n",
            "epoch: 20 step: 810, loss is 0.9607705473899841\n",
            "epoch: 20 step: 811, loss is 0.8818879723548889\n",
            "epoch: 20 step: 812, loss is 0.8715782165527344\n",
            "epoch: 20 step: 813, loss is 1.0207706689834595\n",
            "epoch: 20 step: 814, loss is 0.858983039855957\n",
            "epoch: 20 step: 815, loss is 0.9813162088394165\n",
            "epoch: 20 step: 816, loss is 0.8940231800079346\n",
            "epoch: 20 step: 817, loss is 1.0481863021850586\n",
            "epoch: 20 step: 818, loss is 0.8608688116073608\n",
            "epoch: 20 step: 819, loss is 1.059173345565796\n",
            "epoch: 20 step: 820, loss is 0.8733408451080322\n",
            "epoch: 20 step: 821, loss is 0.8928631544113159\n",
            "epoch: 20 step: 822, loss is 0.8607916831970215\n",
            "epoch: 20 step: 823, loss is 0.9094106554985046\n",
            "epoch: 20 step: 824, loss is 0.8650845885276794\n",
            "epoch: 20 step: 825, loss is 0.8611699342727661\n",
            "epoch: 20 step: 826, loss is 0.903360903263092\n",
            "epoch: 20 step: 827, loss is 0.8755027651786804\n",
            "epoch: 20 step: 828, loss is 0.9348331093788147\n",
            "epoch: 20 step: 829, loss is 0.9039893746376038\n",
            "epoch: 20 step: 830, loss is 1.0416247844696045\n",
            "epoch: 20 step: 831, loss is 0.8224294185638428\n",
            "epoch: 20 step: 832, loss is 0.8862024545669556\n",
            "epoch: 20 step: 833, loss is 0.7541481256484985\n",
            "epoch: 20 step: 834, loss is 1.055044412612915\n",
            "epoch: 20 step: 835, loss is 1.006786823272705\n",
            "epoch: 20 step: 836, loss is 0.8852046728134155\n",
            "epoch: 20 step: 837, loss is 0.8776456713676453\n",
            "epoch: 20 step: 838, loss is 0.7917910814285278\n",
            "epoch: 20 step: 839, loss is 1.1274752616882324\n",
            "epoch: 20 step: 840, loss is 0.7333822250366211\n",
            "epoch: 20 step: 841, loss is 0.9443602561950684\n",
            "epoch: 20 step: 842, loss is 0.8693766593933105\n",
            "epoch: 20 step: 843, loss is 0.9578076004981995\n",
            "epoch: 20 step: 844, loss is 0.9049554467201233\n",
            "epoch: 20 step: 845, loss is 0.8760229349136353\n",
            "epoch: 20 step: 846, loss is 0.8733407258987427\n",
            "epoch: 20 step: 847, loss is 0.861129879951477\n",
            "epoch: 20 step: 848, loss is 0.9387176632881165\n",
            "epoch: 20 step: 849, loss is 0.8155819773674011\n",
            "epoch: 20 step: 850, loss is 1.0244659185409546\n",
            "epoch: 20 step: 851, loss is 0.9028520584106445\n",
            "epoch: 20 step: 852, loss is 0.8707740306854248\n",
            "epoch: 20 step: 853, loss is 0.8293596506118774\n",
            "epoch: 20 step: 854, loss is 0.9943780899047852\n",
            "epoch: 20 step: 855, loss is 0.9245155453681946\n",
            "epoch: 20 step: 856, loss is 0.8856383562088013\n",
            "epoch: 20 step: 857, loss is 0.7332763671875\n",
            "epoch: 20 step: 858, loss is 0.7603334784507751\n",
            "epoch: 20 step: 859, loss is 1.0040507316589355\n",
            "epoch: 20 step: 860, loss is 0.8156127333641052\n",
            "epoch: 20 step: 861, loss is 0.945325493812561\n",
            "epoch: 20 step: 862, loss is 0.8541857600212097\n",
            "epoch: 20 step: 863, loss is 0.9416025876998901\n",
            "epoch: 20 step: 864, loss is 0.7214714288711548\n",
            "epoch: 20 step: 865, loss is 0.8231638669967651\n",
            "epoch: 20 step: 866, loss is 0.8461577296257019\n",
            "epoch: 20 step: 867, loss is 0.9969720840454102\n",
            "epoch: 20 step: 868, loss is 1.0428504943847656\n",
            "epoch: 20 step: 869, loss is 0.9459036588668823\n",
            "epoch: 20 step: 870, loss is 0.8936581015586853\n",
            "epoch: 20 step: 871, loss is 0.7632244229316711\n",
            "epoch: 20 step: 872, loss is 0.8149999976158142\n",
            "epoch: 20 step: 873, loss is 0.8602883219718933\n",
            "epoch: 20 step: 874, loss is 0.9746297597885132\n",
            "epoch: 20 step: 875, loss is 0.6956409215927124\n",
            "epoch: 20 step: 876, loss is 0.9376696348190308\n",
            "epoch: 20 step: 877, loss is 1.0259838104248047\n",
            "epoch: 20 step: 878, loss is 0.9443116784095764\n",
            "epoch: 20 step: 879, loss is 1.0607342720031738\n",
            "epoch: 20 step: 880, loss is 0.8665765523910522\n",
            "epoch: 20 step: 881, loss is 0.8434736132621765\n",
            "epoch: 20 step: 882, loss is 0.8738491535186768\n",
            "epoch: 20 step: 883, loss is 0.8474047780036926\n",
            "epoch: 20 step: 884, loss is 0.982245922088623\n",
            "epoch: 20 step: 885, loss is 0.9796732664108276\n",
            "epoch: 20 step: 886, loss is 0.977162778377533\n",
            "epoch: 20 step: 887, loss is 0.731218159198761\n",
            "epoch: 20 step: 888, loss is 1.017214298248291\n",
            "epoch: 20 step: 889, loss is 0.9731540679931641\n",
            "epoch: 20 step: 890, loss is 0.8570586442947388\n",
            "epoch: 20 step: 891, loss is 0.9435688853263855\n",
            "epoch: 20 step: 892, loss is 0.9426750540733337\n",
            "epoch: 20 step: 893, loss is 0.7824562191963196\n",
            "epoch: 20 step: 894, loss is 0.8966575860977173\n",
            "epoch: 20 step: 895, loss is 0.9783145189285278\n",
            "epoch: 20 step: 896, loss is 0.9126268625259399\n",
            "epoch: 20 step: 897, loss is 1.0111266374588013\n",
            "epoch: 20 step: 898, loss is 1.0599393844604492\n",
            "epoch: 20 step: 899, loss is 0.8842771053314209\n",
            "epoch: 20 step: 900, loss is 0.9582450985908508\n",
            "epoch: 20 step: 901, loss is 0.8747822046279907\n",
            "epoch: 20 step: 902, loss is 0.7470291256904602\n",
            "epoch: 20 step: 903, loss is 0.7810916304588318\n",
            "epoch: 20 step: 904, loss is 0.9347946643829346\n",
            "epoch: 20 step: 905, loss is 0.6895997524261475\n",
            "epoch: 20 step: 906, loss is 0.6918027400970459\n",
            "epoch: 20 step: 907, loss is 0.965351402759552\n",
            "epoch: 20 step: 908, loss is 0.8415425419807434\n",
            "epoch: 20 step: 909, loss is 0.9058869481086731\n",
            "epoch: 20 step: 910, loss is 0.7879512906074524\n",
            "epoch: 20 step: 911, loss is 0.8724572062492371\n",
            "epoch: 20 step: 912, loss is 0.8754639029502869\n",
            "epoch: 20 step: 913, loss is 0.9064119458198547\n",
            "epoch: 20 step: 914, loss is 0.9579159021377563\n",
            "epoch: 20 step: 915, loss is 0.9466496706008911\n",
            "epoch: 20 step: 916, loss is 0.9241189360618591\n",
            "epoch: 20 step: 917, loss is 0.9559369087219238\n",
            "epoch: 20 step: 918, loss is 0.9930694103240967\n",
            "epoch: 20 step: 919, loss is 0.8653972148895264\n",
            "epoch: 20 step: 920, loss is 0.8647624254226685\n",
            "epoch: 20 step: 921, loss is 0.9867553114891052\n",
            "epoch: 20 step: 922, loss is 0.8659158945083618\n",
            "epoch: 20 step: 923, loss is 0.8813779354095459\n",
            "epoch: 20 step: 924, loss is 0.9257727861404419\n",
            "epoch: 20 step: 925, loss is 0.9603274464607239\n",
            "epoch: 20 step: 926, loss is 0.8460010886192322\n",
            "epoch: 20 step: 927, loss is 0.8907068371772766\n",
            "epoch: 20 step: 928, loss is 0.893347442150116\n",
            "epoch: 20 step: 929, loss is 0.847108006477356\n",
            "epoch: 20 step: 930, loss is 0.9183605313301086\n",
            "epoch: 20 step: 931, loss is 0.8311865329742432\n",
            "epoch: 20 step: 932, loss is 0.7761424779891968\n",
            "epoch: 20 step: 933, loss is 0.8050798177719116\n",
            "epoch: 20 step: 934, loss is 0.9519951343536377\n",
            "epoch: 20 step: 935, loss is 0.913201630115509\n",
            "epoch: 20 step: 936, loss is 0.8903418183326721\n",
            "epoch: 20 step: 937, loss is 0.9019522070884705\n",
            "epoch: 20 step: 938, loss is 1.010064959526062\n",
            "epoch: 20 step: 939, loss is 0.8657281398773193\n",
            "epoch: 20 step: 940, loss is 0.8807090520858765\n",
            "epoch: 20 step: 941, loss is 0.7664400339126587\n",
            "epoch: 20 step: 942, loss is 0.8305778503417969\n",
            "epoch: 20 step: 943, loss is 0.9637186527252197\n",
            "epoch: 20 step: 944, loss is 0.8419305086135864\n",
            "epoch: 20 step: 945, loss is 0.8644968271255493\n",
            "epoch: 20 step: 946, loss is 0.8354718089103699\n",
            "epoch: 20 step: 947, loss is 0.7414451241493225\n",
            "epoch: 20 step: 948, loss is 0.9597060084342957\n",
            "epoch: 20 step: 949, loss is 0.9692335724830627\n",
            "epoch: 20 step: 950, loss is 0.8333759307861328\n",
            "epoch: 20 step: 951, loss is 1.0252131223678589\n",
            "epoch: 20 step: 952, loss is 0.8940691947937012\n",
            "epoch: 20 step: 953, loss is 0.8927009701728821\n",
            "epoch: 20 step: 954, loss is 0.911808967590332\n",
            "epoch: 20 step: 955, loss is 0.789000928401947\n",
            "epoch: 20 step: 956, loss is 1.0693153142929077\n",
            "epoch: 20 step: 957, loss is 0.8675438761711121\n",
            "epoch: 20 step: 958, loss is 0.8729537725448608\n",
            "epoch: 20 step: 959, loss is 0.8952082991600037\n",
            "epoch: 20 step: 960, loss is 0.8099725842475891\n",
            "epoch: 20 step: 961, loss is 0.8526303172111511\n",
            "epoch: 20 step: 962, loss is 0.8274995684623718\n",
            "epoch: 20 step: 963, loss is 0.9377956390380859\n",
            "epoch: 20 step: 964, loss is 0.9560523629188538\n",
            "epoch: 20 step: 965, loss is 0.8457435965538025\n",
            "epoch: 20 step: 966, loss is 0.7576147317886353\n",
            "epoch: 20 step: 967, loss is 0.7853874564170837\n",
            "epoch: 20 step: 968, loss is 0.818099856376648\n",
            "epoch: 20 step: 969, loss is 0.9158223867416382\n",
            "epoch: 20 step: 970, loss is 0.8846096396446228\n",
            "epoch: 20 step: 971, loss is 0.8114786148071289\n",
            "epoch: 20 step: 972, loss is 1.0047334432601929\n",
            "epoch: 20 step: 973, loss is 0.9085448384284973\n",
            "epoch: 20 step: 974, loss is 0.8102810978889465\n",
            "epoch: 20 step: 975, loss is 0.8560836315155029\n",
            "epoch: 20 step: 976, loss is 0.8387593030929565\n",
            "epoch: 20 step: 977, loss is 1.0413140058517456\n",
            "epoch: 20 step: 978, loss is 1.0268529653549194\n",
            "epoch: 20 step: 979, loss is 0.8508341908454895\n",
            "epoch: 20 step: 980, loss is 0.9402129650115967\n",
            "epoch: 20 step: 981, loss is 0.9426029920578003\n",
            "epoch: 20 step: 982, loss is 0.8197917938232422\n",
            "epoch: 20 step: 983, loss is 1.11042320728302\n",
            "epoch: 20 step: 984, loss is 0.8756527900695801\n",
            "epoch: 20 step: 985, loss is 0.8661019802093506\n",
            "epoch: 20 step: 986, loss is 0.7329820394515991\n",
            "epoch: 20 step: 987, loss is 0.9770568609237671\n",
            "epoch: 20 step: 988, loss is 0.8879521489143372\n",
            "epoch: 20 step: 989, loss is 0.903254508972168\n",
            "epoch: 20 step: 990, loss is 0.8454189300537109\n",
            "epoch: 20 step: 991, loss is 0.9409234523773193\n",
            "epoch: 20 step: 992, loss is 0.8377318382263184\n",
            "epoch: 20 step: 993, loss is 0.9349061250686646\n",
            "epoch: 20 step: 994, loss is 0.7955326437950134\n",
            "epoch: 20 step: 995, loss is 0.9647762775421143\n",
            "epoch: 20 step: 996, loss is 0.9171631336212158\n",
            "epoch: 20 step: 997, loss is 0.8359877467155457\n",
            "epoch: 20 step: 998, loss is 0.9181085824966431\n",
            "epoch: 20 step: 999, loss is 0.8249437808990479\n",
            "epoch: 20 step: 1000, loss is 0.9306915402412415\n",
            "epoch: 20 step: 1001, loss is 0.9705079793930054\n",
            "epoch: 20 step: 1002, loss is 0.8722172975540161\n",
            "epoch: 20 step: 1003, loss is 1.0958133935928345\n",
            "epoch: 20 step: 1004, loss is 0.7995660305023193\n",
            "epoch: 20 step: 1005, loss is 1.02815580368042\n",
            "epoch: 20 step: 1006, loss is 0.9159979224205017\n",
            "epoch: 20 step: 1007, loss is 0.8058756589889526\n",
            "epoch: 20 step: 1008, loss is 1.0353176593780518\n",
            "epoch: 20 step: 1009, loss is 0.9595222473144531\n",
            "epoch: 20 step: 1010, loss is 0.7768304347991943\n",
            "epoch: 20 step: 1011, loss is 0.8160776495933533\n",
            "epoch: 20 step: 1012, loss is 0.8733500242233276\n",
            "epoch: 20 step: 1013, loss is 0.9497350454330444\n",
            "epoch: 20 step: 1014, loss is 0.8343904614448547\n",
            "epoch: 20 step: 1015, loss is 1.0190339088439941\n",
            "epoch: 20 step: 1016, loss is 0.9782235622406006\n",
            "epoch: 20 step: 1017, loss is 0.8254125714302063\n",
            "epoch: 20 step: 1018, loss is 0.9156010150909424\n",
            "epoch: 20 step: 1019, loss is 0.8974244594573975\n",
            "epoch: 20 step: 1020, loss is 0.8897289037704468\n",
            "epoch: 20 step: 1021, loss is 0.9459632039070129\n",
            "epoch: 20 step: 1022, loss is 1.0613212585449219\n",
            "epoch: 20 step: 1023, loss is 0.9455470442771912\n",
            "epoch: 20 step: 1024, loss is 0.8666425347328186\n",
            "epoch: 20 step: 1025, loss is 1.0705841779708862\n",
            "epoch: 20 step: 1026, loss is 0.8732107877731323\n",
            "epoch: 20 step: 1027, loss is 1.017691731452942\n",
            "epoch: 20 step: 1028, loss is 1.0004738569259644\n",
            "epoch: 20 step: 1029, loss is 0.8025116324424744\n",
            "epoch: 20 step: 1030, loss is 0.7725746631622314\n",
            "epoch: 20 step: 1031, loss is 0.9281256794929504\n",
            "epoch: 20 step: 1032, loss is 0.8815280199050903\n",
            "epoch: 20 step: 1033, loss is 0.8750728964805603\n",
            "epoch: 20 step: 1034, loss is 0.9161163568496704\n",
            "epoch: 20 step: 1035, loss is 0.7683147192001343\n",
            "epoch: 20 step: 1036, loss is 0.7878786325454712\n",
            "epoch: 20 step: 1037, loss is 0.897009015083313\n",
            "epoch: 20 step: 1038, loss is 0.9635321497917175\n",
            "epoch: 20 step: 1039, loss is 0.8907244205474854\n",
            "epoch: 20 step: 1040, loss is 0.9499188661575317\n",
            "epoch: 20 step: 1041, loss is 0.8108473420143127\n",
            "epoch: 20 step: 1042, loss is 0.7714758515357971\n",
            "epoch: 20 step: 1043, loss is 1.016160488128662\n",
            "epoch: 20 step: 1044, loss is 0.8555191159248352\n",
            "epoch: 20 step: 1045, loss is 0.922571063041687\n",
            "epoch: 20 step: 1046, loss is 0.8824751973152161\n",
            "epoch: 20 step: 1047, loss is 0.9069342017173767\n",
            "epoch: 20 step: 1048, loss is 1.1098884344100952\n",
            "epoch: 20 step: 1049, loss is 0.9222726225852966\n",
            "epoch: 20 step: 1050, loss is 1.0404547452926636\n",
            "epoch: 20 step: 1051, loss is 0.8255317211151123\n",
            "epoch: 20 step: 1052, loss is 0.8979833126068115\n",
            "epoch: 20 step: 1053, loss is 0.9986074566841125\n",
            "epoch: 20 step: 1054, loss is 0.8213286399841309\n",
            "epoch: 20 step: 1055, loss is 0.7943645715713501\n",
            "epoch: 20 step: 1056, loss is 0.764714777469635\n",
            "epoch: 20 step: 1057, loss is 1.0212959051132202\n",
            "epoch: 20 step: 1058, loss is 0.8900415301322937\n",
            "epoch: 20 step: 1059, loss is 1.021043062210083\n",
            "epoch: 20 step: 1060, loss is 1.0584126710891724\n",
            "epoch: 20 step: 1061, loss is 0.7831708788871765\n",
            "epoch: 20 step: 1062, loss is 0.7646869421005249\n",
            "epoch: 20 step: 1063, loss is 0.937593400478363\n",
            "epoch: 20 step: 1064, loss is 0.8116377592086792\n",
            "epoch: 20 step: 1065, loss is 0.8482018709182739\n",
            "epoch: 20 step: 1066, loss is 0.8365532755851746\n",
            "epoch: 20 step: 1067, loss is 0.82886803150177\n",
            "epoch: 20 step: 1068, loss is 1.0156198740005493\n",
            "epoch: 20 step: 1069, loss is 0.8388738036155701\n",
            "epoch: 20 step: 1070, loss is 1.0658806562423706\n",
            "epoch: 20 step: 1071, loss is 1.1162548065185547\n",
            "epoch: 20 step: 1072, loss is 0.8930987119674683\n",
            "epoch: 20 step: 1073, loss is 0.7688461542129517\n",
            "epoch: 20 step: 1074, loss is 0.8621180653572083\n",
            "epoch: 20 step: 1075, loss is 0.9493318796157837\n",
            "epoch: 20 step: 1076, loss is 0.8988946080207825\n",
            "epoch: 20 step: 1077, loss is 0.7944658994674683\n",
            "epoch: 20 step: 1078, loss is 0.922111451625824\n",
            "epoch: 20 step: 1079, loss is 0.685982346534729\n",
            "epoch: 20 step: 1080, loss is 0.8404345512390137\n",
            "epoch: 20 step: 1081, loss is 0.866199254989624\n",
            "epoch: 20 step: 1082, loss is 0.8606655597686768\n",
            "epoch: 20 step: 1083, loss is 0.8941585421562195\n",
            "epoch: 20 step: 1084, loss is 0.9237882494926453\n",
            "epoch: 20 step: 1085, loss is 0.8006327748298645\n",
            "epoch: 20 step: 1086, loss is 0.9349406361579895\n",
            "epoch: 20 step: 1087, loss is 0.802809476852417\n",
            "epoch: 20 step: 1088, loss is 0.8853784799575806\n",
            "epoch: 20 step: 1089, loss is 1.0950788259506226\n",
            "epoch: 20 step: 1090, loss is 0.950755774974823\n",
            "epoch: 20 step: 1091, loss is 0.6830577254295349\n",
            "epoch: 20 step: 1092, loss is 0.7791491150856018\n",
            "epoch: 20 step: 1093, loss is 1.1098723411560059\n",
            "epoch: 20 step: 1094, loss is 0.800791323184967\n",
            "epoch: 20 step: 1095, loss is 0.9191569089889526\n",
            "epoch: 20 step: 1096, loss is 0.7860907912254333\n",
            "epoch: 20 step: 1097, loss is 0.9293106198310852\n",
            "epoch: 20 step: 1098, loss is 1.034906268119812\n",
            "epoch: 20 step: 1099, loss is 0.7424380779266357\n",
            "epoch: 20 step: 1100, loss is 0.8913182616233826\n",
            "epoch: 20 step: 1101, loss is 0.9487330317497253\n",
            "epoch: 20 step: 1102, loss is 0.8532918095588684\n",
            "epoch: 20 step: 1103, loss is 0.8698000907897949\n",
            "epoch: 20 step: 1104, loss is 0.8386132717132568\n",
            "epoch: 20 step: 1105, loss is 0.7418642044067383\n",
            "epoch: 20 step: 1106, loss is 0.9256786704063416\n",
            "epoch: 20 step: 1107, loss is 0.777347981929779\n",
            "epoch: 20 step: 1108, loss is 0.8293942213058472\n",
            "epoch: 20 step: 1109, loss is 0.9883599877357483\n",
            "epoch: 20 step: 1110, loss is 0.9447265863418579\n",
            "epoch: 20 step: 1111, loss is 0.903704047203064\n",
            "epoch: 20 step: 1112, loss is 0.8764503002166748\n",
            "epoch: 20 step: 1113, loss is 0.8400821685791016\n",
            "epoch: 20 step: 1114, loss is 1.0451734066009521\n",
            "epoch: 20 step: 1115, loss is 0.9593844413757324\n",
            "epoch: 20 step: 1116, loss is 0.7584894299507141\n",
            "epoch: 20 step: 1117, loss is 0.911191999912262\n",
            "epoch: 20 step: 1118, loss is 0.7291176319122314\n",
            "epoch: 20 step: 1119, loss is 0.7585643529891968\n",
            "epoch: 20 step: 1120, loss is 0.9100766181945801\n",
            "epoch: 20 step: 1121, loss is 0.7330309152603149\n",
            "epoch: 20 step: 1122, loss is 0.895585834980011\n",
            "epoch: 20 step: 1123, loss is 0.7909283638000488\n",
            "epoch: 20 step: 1124, loss is 0.9014632105827332\n",
            "epoch: 20 step: 1125, loss is 0.9147745966911316\n",
            "epoch: 20 step: 1126, loss is 0.9274142980575562\n",
            "epoch: 20 step: 1127, loss is 0.7936742305755615\n",
            "epoch: 20 step: 1128, loss is 0.7656410336494446\n",
            "epoch: 20 step: 1129, loss is 0.9663141965866089\n",
            "epoch: 20 step: 1130, loss is 0.8407151103019714\n",
            "epoch: 20 step: 1131, loss is 0.737264096736908\n",
            "epoch: 20 step: 1132, loss is 0.9483368992805481\n",
            "epoch: 20 step: 1133, loss is 0.7710785865783691\n",
            "epoch: 20 step: 1134, loss is 0.858705461025238\n",
            "epoch: 20 step: 1135, loss is 1.0384504795074463\n",
            "epoch: 20 step: 1136, loss is 0.8444467186927795\n",
            "epoch: 20 step: 1137, loss is 0.7277163863182068\n",
            "epoch: 20 step: 1138, loss is 0.7726790308952332\n",
            "epoch: 20 step: 1139, loss is 1.0006576776504517\n",
            "epoch: 20 step: 1140, loss is 0.8671829700469971\n",
            "epoch: 20 step: 1141, loss is 0.9643229246139526\n",
            "epoch: 20 step: 1142, loss is 1.0075907707214355\n",
            "epoch: 20 step: 1143, loss is 0.9735590815544128\n",
            "epoch: 20 step: 1144, loss is 0.7730305194854736\n",
            "epoch: 20 step: 1145, loss is 0.8931394815444946\n",
            "epoch: 20 step: 1146, loss is 1.0059586763381958\n",
            "epoch: 20 step: 1147, loss is 0.9220021367073059\n",
            "epoch: 20 step: 1148, loss is 0.9293053150177002\n",
            "epoch: 20 step: 1149, loss is 0.9850001335144043\n",
            "epoch: 20 step: 1150, loss is 0.9354666471481323\n",
            "epoch: 20 step: 1151, loss is 0.8959391117095947\n",
            "epoch: 20 step: 1152, loss is 0.9141308069229126\n",
            "epoch: 20 step: 1153, loss is 0.8558385372161865\n",
            "epoch: 20 step: 1154, loss is 1.0708814859390259\n",
            "epoch: 20 step: 1155, loss is 0.7423515319824219\n",
            "epoch: 20 step: 1156, loss is 0.8276274800300598\n",
            "epoch: 20 step: 1157, loss is 0.8147530555725098\n",
            "epoch: 20 step: 1158, loss is 0.7784804105758667\n",
            "epoch: 20 step: 1159, loss is 0.8670382499694824\n",
            "epoch: 20 step: 1160, loss is 0.7764171361923218\n",
            "epoch: 20 step: 1161, loss is 1.0682412385940552\n",
            "epoch: 20 step: 1162, loss is 1.0064325332641602\n",
            "epoch: 20 step: 1163, loss is 0.7796515226364136\n",
            "epoch: 20 step: 1164, loss is 0.9040184617042542\n",
            "epoch: 20 step: 1165, loss is 0.8572795391082764\n",
            "epoch: 20 step: 1166, loss is 0.8808696269989014\n",
            "epoch: 20 step: 1167, loss is 0.8675320148468018\n",
            "epoch: 20 step: 1168, loss is 0.9843067526817322\n",
            "epoch: 20 step: 1169, loss is 0.8861638903617859\n",
            "epoch: 20 step: 1170, loss is 0.9337438344955444\n",
            "epoch: 20 step: 1171, loss is 0.71500563621521\n",
            "epoch: 20 step: 1172, loss is 0.8648077249526978\n",
            "epoch: 20 step: 1173, loss is 0.7863081097602844\n",
            "epoch: 20 step: 1174, loss is 0.8381731510162354\n",
            "epoch: 20 step: 1175, loss is 0.8146785497665405\n",
            "epoch: 20 step: 1176, loss is 0.8594911098480225\n",
            "epoch: 20 step: 1177, loss is 0.9530575275421143\n",
            "epoch: 20 step: 1178, loss is 1.0581786632537842\n",
            "epoch: 20 step: 1179, loss is 0.9743092060089111\n",
            "epoch: 20 step: 1180, loss is 0.9353455305099487\n",
            "epoch: 20 step: 1181, loss is 0.9151513576507568\n",
            "epoch: 20 step: 1182, loss is 0.8326134085655212\n",
            "epoch: 20 step: 1183, loss is 0.9751750230789185\n",
            "epoch: 20 step: 1184, loss is 0.836097002029419\n",
            "epoch: 20 step: 1185, loss is 0.8695334196090698\n",
            "epoch: 20 step: 1186, loss is 0.9015824794769287\n",
            "epoch: 20 step: 1187, loss is 0.9181967973709106\n",
            "epoch: 20 step: 1188, loss is 0.9202211499214172\n",
            "epoch: 20 step: 1189, loss is 0.8459377884864807\n",
            "epoch: 20 step: 1190, loss is 1.0021741390228271\n",
            "epoch: 20 step: 1191, loss is 0.7827654480934143\n",
            "epoch: 20 step: 1192, loss is 0.8010527491569519\n",
            "epoch: 20 step: 1193, loss is 0.9545979499816895\n",
            "epoch: 20 step: 1194, loss is 0.9114250540733337\n",
            "epoch: 20 step: 1195, loss is 0.8598728775978088\n",
            "epoch: 20 step: 1196, loss is 1.1561012268066406\n",
            "epoch: 20 step: 1197, loss is 0.9211065173149109\n",
            "epoch: 20 step: 1198, loss is 0.889570415019989\n",
            "epoch: 20 step: 1199, loss is 0.8720811605453491\n",
            "epoch: 20 step: 1200, loss is 0.8751522302627563\n",
            "epoch: 20 step: 1201, loss is 0.7437132000923157\n",
            "epoch: 20 step: 1202, loss is 1.0714565515518188\n",
            "epoch: 20 step: 1203, loss is 1.0975323915481567\n",
            "epoch: 20 step: 1204, loss is 0.8705806732177734\n",
            "epoch: 20 step: 1205, loss is 1.0454612970352173\n",
            "epoch: 20 step: 1206, loss is 1.0180615186691284\n",
            "epoch: 20 step: 1207, loss is 0.842653751373291\n",
            "epoch: 20 step: 1208, loss is 0.9877905249595642\n",
            "epoch: 20 step: 1209, loss is 0.7859499454498291\n",
            "epoch: 20 step: 1210, loss is 0.9490845203399658\n",
            "epoch: 20 step: 1211, loss is 0.9026298522949219\n",
            "epoch: 20 step: 1212, loss is 0.8961830139160156\n",
            "epoch: 20 step: 1213, loss is 1.0165396928787231\n",
            "epoch: 20 step: 1214, loss is 0.7588775753974915\n",
            "epoch: 20 step: 1215, loss is 0.9204584360122681\n",
            "epoch: 20 step: 1216, loss is 0.8932782411575317\n",
            "epoch: 20 step: 1217, loss is 1.1189547777175903\n",
            "epoch: 20 step: 1218, loss is 1.048067331314087\n",
            "epoch: 20 step: 1219, loss is 0.8244253396987915\n",
            "epoch: 20 step: 1220, loss is 1.0112122297286987\n",
            "epoch: 20 step: 1221, loss is 0.9899259805679321\n",
            "epoch: 20 step: 1222, loss is 0.8910768628120422\n",
            "epoch: 20 step: 1223, loss is 0.9460466504096985\n",
            "epoch: 20 step: 1224, loss is 0.8375720977783203\n",
            "epoch: 20 step: 1225, loss is 0.918026328086853\n",
            "epoch: 20 step: 1226, loss is 0.9494206309318542\n",
            "epoch: 20 step: 1227, loss is 0.9751449227333069\n",
            "epoch: 20 step: 1228, loss is 0.7821177244186401\n",
            "epoch: 20 step: 1229, loss is 0.838481605052948\n",
            "epoch: 20 step: 1230, loss is 0.8634663224220276\n",
            "epoch: 20 step: 1231, loss is 0.9538844227790833\n",
            "epoch: 20 step: 1232, loss is 0.9487878680229187\n",
            "epoch: 20 step: 1233, loss is 0.9442728757858276\n",
            "epoch: 20 step: 1234, loss is 0.8136985898017883\n",
            "epoch: 20 step: 1235, loss is 0.9696766138076782\n",
            "epoch: 20 step: 1236, loss is 0.8709874153137207\n",
            "epoch: 20 step: 1237, loss is 0.8220282196998596\n",
            "epoch: 20 step: 1238, loss is 0.9462910890579224\n",
            "epoch: 20 step: 1239, loss is 0.8287044167518616\n",
            "epoch: 20 step: 1240, loss is 0.8084413409233093\n",
            "epoch: 20 step: 1241, loss is 0.8797329068183899\n",
            "epoch: 20 step: 1242, loss is 0.7364315390586853\n",
            "epoch: 20 step: 1243, loss is 0.9823510646820068\n",
            "epoch: 20 step: 1244, loss is 0.9992629289627075\n",
            "epoch: 20 step: 1245, loss is 0.9669781923294067\n",
            "epoch: 20 step: 1246, loss is 0.7699443101882935\n",
            "epoch: 20 step: 1247, loss is 0.9174919724464417\n",
            "epoch: 20 step: 1248, loss is 1.1897386312484741\n",
            "epoch: 20 step: 1249, loss is 0.7580777406692505\n",
            "epoch: 20 step: 1250, loss is 0.8652198314666748\n",
            "epoch: 20 step: 1251, loss is 0.7493067383766174\n",
            "epoch: 20 step: 1252, loss is 0.7533033490180969\n",
            "epoch: 20 step: 1253, loss is 1.089497685432434\n",
            "epoch: 20 step: 1254, loss is 0.768427848815918\n",
            "epoch: 20 step: 1255, loss is 0.8407502174377441\n",
            "epoch: 20 step: 1256, loss is 0.7221468687057495\n",
            "epoch: 20 step: 1257, loss is 1.0638034343719482\n",
            "epoch: 20 step: 1258, loss is 0.7972235679626465\n",
            "epoch: 20 step: 1259, loss is 0.8034157752990723\n",
            "epoch: 20 step: 1260, loss is 0.8006613254547119\n",
            "epoch: 20 step: 1261, loss is 1.0100817680358887\n",
            "epoch: 20 step: 1262, loss is 0.9231078624725342\n",
            "epoch: 20 step: 1263, loss is 0.9063054323196411\n",
            "epoch: 20 step: 1264, loss is 0.82542484998703\n",
            "epoch: 20 step: 1265, loss is 0.8638570308685303\n",
            "epoch: 20 step: 1266, loss is 0.9320103526115417\n",
            "epoch: 20 step: 1267, loss is 0.6865562796592712\n",
            "epoch: 20 step: 1268, loss is 0.9915333986282349\n",
            "epoch: 20 step: 1269, loss is 0.8769980669021606\n",
            "epoch: 20 step: 1270, loss is 0.8250863552093506\n",
            "epoch: 20 step: 1271, loss is 0.8740336894989014\n",
            "epoch: 20 step: 1272, loss is 0.9601499438285828\n",
            "epoch: 20 step: 1273, loss is 0.7695067524909973\n",
            "epoch: 20 step: 1274, loss is 0.6892480850219727\n",
            "epoch: 20 step: 1275, loss is 0.8730801343917847\n",
            "epoch: 20 step: 1276, loss is 0.8675020933151245\n",
            "epoch: 20 step: 1277, loss is 1.1499089002609253\n",
            "epoch: 20 step: 1278, loss is 0.7495613098144531\n",
            "epoch: 20 step: 1279, loss is 0.8668725490570068\n",
            "epoch: 20 step: 1280, loss is 0.9370177388191223\n",
            "epoch: 20 step: 1281, loss is 0.8698907494544983\n",
            "epoch: 20 step: 1282, loss is 0.9681630730628967\n",
            "epoch: 20 step: 1283, loss is 0.7869001626968384\n",
            "epoch: 20 step: 1284, loss is 0.8977591395378113\n",
            "epoch: 20 step: 1285, loss is 0.9765253067016602\n",
            "epoch: 20 step: 1286, loss is 0.7737833261489868\n",
            "epoch: 20 step: 1287, loss is 0.8893898129463196\n",
            "epoch: 20 step: 1288, loss is 0.8969230055809021\n",
            "epoch: 20 step: 1289, loss is 1.0454230308532715\n",
            "epoch: 20 step: 1290, loss is 0.8729040622711182\n",
            "epoch: 20 step: 1291, loss is 0.8349649310112\n",
            "epoch: 20 step: 1292, loss is 0.7021370530128479\n",
            "epoch: 20 step: 1293, loss is 0.7714956998825073\n",
            "epoch: 20 step: 1294, loss is 0.813532292842865\n",
            "epoch: 20 step: 1295, loss is 0.8391363620758057\n",
            "epoch: 20 step: 1296, loss is 1.0794198513031006\n",
            "epoch: 20 step: 1297, loss is 0.9141718149185181\n",
            "epoch: 20 step: 1298, loss is 0.8784834742546082\n",
            "epoch: 20 step: 1299, loss is 0.8607476949691772\n",
            "epoch: 20 step: 1300, loss is 0.9439142942428589\n",
            "epoch: 20 step: 1301, loss is 0.6882672905921936\n",
            "epoch: 20 step: 1302, loss is 0.8487361073493958\n",
            "epoch: 20 step: 1303, loss is 0.7103687524795532\n",
            "epoch: 20 step: 1304, loss is 0.8970208168029785\n",
            "epoch: 20 step: 1305, loss is 0.8529720902442932\n",
            "epoch: 20 step: 1306, loss is 0.8555554747581482\n",
            "epoch: 20 step: 1307, loss is 0.9889518618583679\n",
            "epoch: 20 step: 1308, loss is 0.7349271178245544\n",
            "epoch: 20 step: 1309, loss is 0.7520914077758789\n",
            "epoch: 20 step: 1310, loss is 0.7820351123809814\n",
            "epoch: 20 step: 1311, loss is 0.9569091200828552\n",
            "epoch: 20 step: 1312, loss is 0.9672515392303467\n",
            "epoch: 20 step: 1313, loss is 0.9399822950363159\n",
            "epoch: 20 step: 1314, loss is 0.9938794374465942\n",
            "epoch: 20 step: 1315, loss is 0.7979747653007507\n",
            "epoch: 20 step: 1316, loss is 0.8438018560409546\n",
            "epoch: 20 step: 1317, loss is 0.81142657995224\n",
            "epoch: 20 step: 1318, loss is 1.0055530071258545\n",
            "epoch: 20 step: 1319, loss is 0.6513985991477966\n",
            "epoch: 20 step: 1320, loss is 0.8145054578781128\n",
            "epoch: 20 step: 1321, loss is 1.1628695726394653\n",
            "epoch: 20 step: 1322, loss is 0.8521610498428345\n",
            "epoch: 20 step: 1323, loss is 0.9891893863677979\n",
            "epoch: 20 step: 1324, loss is 0.708686888217926\n",
            "epoch: 20 step: 1325, loss is 1.1351842880249023\n",
            "epoch: 20 step: 1326, loss is 0.84311842918396\n",
            "epoch: 20 step: 1327, loss is 0.8125884532928467\n",
            "epoch: 20 step: 1328, loss is 0.8635276556015015\n",
            "epoch: 20 step: 1329, loss is 0.8749852180480957\n",
            "epoch: 20 step: 1330, loss is 0.8519446849822998\n",
            "epoch: 20 step: 1331, loss is 0.9744260311126709\n",
            "epoch: 20 step: 1332, loss is 0.8303260207176208\n",
            "epoch: 20 step: 1333, loss is 0.8433212637901306\n",
            "epoch: 20 step: 1334, loss is 0.938866138458252\n",
            "epoch: 20 step: 1335, loss is 0.7723201513290405\n",
            "epoch: 20 step: 1336, loss is 0.9619526863098145\n",
            "epoch: 20 step: 1337, loss is 0.8989071249961853\n",
            "epoch: 20 step: 1338, loss is 0.9340711832046509\n",
            "epoch: 20 step: 1339, loss is 0.7531776428222656\n",
            "epoch: 20 step: 1340, loss is 0.9838073253631592\n",
            "epoch: 20 step: 1341, loss is 0.7830699682235718\n",
            "epoch: 20 step: 1342, loss is 0.9599278569221497\n",
            "epoch: 20 step: 1343, loss is 0.930050790309906\n",
            "epoch: 20 step: 1344, loss is 0.7908263206481934\n",
            "epoch: 20 step: 1345, loss is 0.8792311549186707\n",
            "epoch: 20 step: 1346, loss is 0.894959568977356\n",
            "epoch: 20 step: 1347, loss is 0.7682051658630371\n",
            "epoch: 20 step: 1348, loss is 0.8144946694374084\n",
            "epoch: 20 step: 1349, loss is 0.7607787251472473\n",
            "epoch: 20 step: 1350, loss is 1.0333083868026733\n",
            "epoch: 20 step: 1351, loss is 0.8362671732902527\n",
            "epoch: 20 step: 1352, loss is 0.8982786536216736\n",
            "epoch: 20 step: 1353, loss is 0.7893033623695374\n",
            "epoch: 20 step: 1354, loss is 0.8996987342834473\n",
            "epoch: 20 step: 1355, loss is 0.7595687508583069\n",
            "epoch: 20 step: 1356, loss is 0.8269985914230347\n",
            "epoch: 20 step: 1357, loss is 0.8295051455497742\n",
            "epoch: 20 step: 1358, loss is 0.7312509417533875\n",
            "epoch: 20 step: 1359, loss is 0.8730494976043701\n",
            "epoch: 20 step: 1360, loss is 0.9103966951370239\n",
            "epoch: 20 step: 1361, loss is 0.8021418452262878\n",
            "epoch: 20 step: 1362, loss is 0.7758333683013916\n",
            "epoch: 20 step: 1363, loss is 0.9396806955337524\n",
            "epoch: 20 step: 1364, loss is 0.9778761267662048\n",
            "epoch: 20 step: 1365, loss is 0.8790733814239502\n",
            "epoch: 20 step: 1366, loss is 0.7203831672668457\n",
            "epoch: 20 step: 1367, loss is 0.9520825147628784\n",
            "epoch: 20 step: 1368, loss is 0.6899741291999817\n",
            "epoch: 20 step: 1369, loss is 0.7178443670272827\n",
            "epoch: 20 step: 1370, loss is 1.1086937189102173\n",
            "epoch: 20 step: 1371, loss is 0.8781139850616455\n",
            "epoch: 20 step: 1372, loss is 0.986667275428772\n",
            "epoch: 20 step: 1373, loss is 0.7684839963912964\n",
            "epoch: 20 step: 1374, loss is 0.9449096322059631\n",
            "epoch: 20 step: 1375, loss is 0.8112186789512634\n",
            "epoch: 20 step: 1376, loss is 0.7793132662773132\n",
            "epoch: 20 step: 1377, loss is 1.0056235790252686\n",
            "epoch: 20 step: 1378, loss is 0.8476412296295166\n",
            "epoch: 20 step: 1379, loss is 0.9910832643508911\n",
            "epoch: 20 step: 1380, loss is 0.7816240787506104\n",
            "epoch: 20 step: 1381, loss is 0.8423168063163757\n",
            "epoch: 20 step: 1382, loss is 0.936285674571991\n",
            "epoch: 20 step: 1383, loss is 0.9356130957603455\n",
            "epoch: 20 step: 1384, loss is 0.9437158703804016\n",
            "epoch: 20 step: 1385, loss is 0.8045784831047058\n",
            "epoch: 20 step: 1386, loss is 0.7631319165229797\n",
            "epoch: 20 step: 1387, loss is 1.0459024906158447\n",
            "epoch: 20 step: 1388, loss is 0.6666802763938904\n",
            "epoch: 20 step: 1389, loss is 0.8671260476112366\n",
            "epoch: 20 step: 1390, loss is 0.9351306557655334\n",
            "epoch: 20 step: 1391, loss is 1.0086557865142822\n",
            "epoch: 20 step: 1392, loss is 0.7029914855957031\n",
            "epoch: 20 step: 1393, loss is 0.823682427406311\n",
            "epoch: 20 step: 1394, loss is 0.8008130788803101\n",
            "epoch: 20 step: 1395, loss is 0.8092913031578064\n",
            "epoch: 20 step: 1396, loss is 0.8776513934135437\n",
            "epoch: 20 step: 1397, loss is 0.8125959038734436\n",
            "epoch: 20 step: 1398, loss is 0.9804961085319519\n",
            "epoch: 20 step: 1399, loss is 0.9832355380058289\n",
            "epoch: 20 step: 1400, loss is 0.6270855665206909\n",
            "epoch: 20 step: 1401, loss is 0.9578787088394165\n",
            "epoch: 20 step: 1402, loss is 0.9436472058296204\n",
            "epoch: 20 step: 1403, loss is 0.9586121439933777\n",
            "epoch: 20 step: 1404, loss is 0.7649098038673401\n",
            "epoch: 20 step: 1405, loss is 0.9845772385597229\n",
            "epoch: 20 step: 1406, loss is 1.08895742893219\n",
            "epoch: 20 step: 1407, loss is 0.8721482157707214\n",
            "epoch: 20 step: 1408, loss is 0.9766510128974915\n",
            "epoch: 20 step: 1409, loss is 0.9790735244750977\n",
            "epoch: 20 step: 1410, loss is 0.9606074690818787\n",
            "epoch: 20 step: 1411, loss is 0.8116283416748047\n",
            "epoch: 20 step: 1412, loss is 0.8936641812324524\n",
            "epoch: 20 step: 1413, loss is 0.8113017678260803\n",
            "epoch: 20 step: 1414, loss is 0.7861607074737549\n",
            "epoch: 20 step: 1415, loss is 0.8784499168395996\n",
            "epoch: 20 step: 1416, loss is 0.8094675540924072\n",
            "epoch: 20 step: 1417, loss is 0.6696131825447083\n",
            "epoch: 20 step: 1418, loss is 0.9642546772956848\n",
            "epoch: 20 step: 1419, loss is 0.8907092809677124\n",
            "epoch: 20 step: 1420, loss is 0.9807761907577515\n",
            "epoch: 20 step: 1421, loss is 0.8939075469970703\n",
            "epoch: 20 step: 1422, loss is 1.0348174571990967\n",
            "epoch: 20 step: 1423, loss is 0.9307805299758911\n",
            "epoch: 20 step: 1424, loss is 0.9511244297027588\n",
            "epoch: 20 step: 1425, loss is 0.8065897226333618\n",
            "epoch: 20 step: 1426, loss is 0.8287006616592407\n",
            "epoch: 20 step: 1427, loss is 1.0504062175750732\n",
            "epoch: 20 step: 1428, loss is 0.8467106223106384\n",
            "epoch: 20 step: 1429, loss is 0.9162042737007141\n",
            "epoch: 20 step: 1430, loss is 0.9390068054199219\n",
            "epoch: 20 step: 1431, loss is 0.7946300506591797\n",
            "epoch: 20 step: 1432, loss is 0.7759047150611877\n",
            "epoch: 20 step: 1433, loss is 0.9347022771835327\n",
            "epoch: 20 step: 1434, loss is 0.8865247368812561\n",
            "epoch: 20 step: 1435, loss is 0.7871392965316772\n",
            "epoch: 20 step: 1436, loss is 0.9773743152618408\n",
            "epoch: 20 step: 1437, loss is 0.8968388438224792\n",
            "epoch: 20 step: 1438, loss is 1.0253729820251465\n",
            "epoch: 20 step: 1439, loss is 0.9721828103065491\n",
            "epoch: 20 step: 1440, loss is 0.9673058390617371\n",
            "epoch: 20 step: 1441, loss is 0.8559684157371521\n",
            "epoch: 20 step: 1442, loss is 0.9126074314117432\n",
            "epoch: 20 step: 1443, loss is 0.8283519148826599\n",
            "epoch: 20 step: 1444, loss is 0.8926111459732056\n",
            "epoch: 20 step: 1445, loss is 0.8361481428146362\n",
            "epoch: 20 step: 1446, loss is 0.8579213619232178\n",
            "epoch: 20 step: 1447, loss is 0.8143676519393921\n",
            "epoch: 20 step: 1448, loss is 0.7954702377319336\n",
            "epoch: 20 step: 1449, loss is 0.8812437653541565\n",
            "epoch: 20 step: 1450, loss is 0.9053077697753906\n",
            "epoch: 20 step: 1451, loss is 0.8933972716331482\n",
            "epoch: 20 step: 1452, loss is 0.6883378028869629\n",
            "epoch: 20 step: 1453, loss is 0.8522599339485168\n",
            "epoch: 20 step: 1454, loss is 0.9319119453430176\n",
            "epoch: 20 step: 1455, loss is 0.8298026919364929\n",
            "epoch: 20 step: 1456, loss is 0.8915888071060181\n",
            "epoch: 20 step: 1457, loss is 0.9209829568862915\n",
            "epoch: 20 step: 1458, loss is 0.7467071413993835\n",
            "epoch: 20 step: 1459, loss is 1.031428337097168\n",
            "epoch: 20 step: 1460, loss is 1.0700045824050903\n",
            "epoch: 20 step: 1461, loss is 0.8168917894363403\n",
            "epoch: 20 step: 1462, loss is 0.8549565672874451\n",
            "epoch: 20 step: 1463, loss is 0.731113076210022\n",
            "epoch: 20 step: 1464, loss is 0.802020251750946\n",
            "epoch: 20 step: 1465, loss is 0.8462033867835999\n",
            "epoch: 20 step: 1466, loss is 0.7132582068443298\n",
            "epoch: 20 step: 1467, loss is 0.823012113571167\n",
            "epoch: 20 step: 1468, loss is 0.8925745487213135\n",
            "epoch: 20 step: 1469, loss is 1.1078616380691528\n",
            "epoch: 20 step: 1470, loss is 0.9598864316940308\n",
            "epoch: 20 step: 1471, loss is 0.8791732788085938\n",
            "epoch: 20 step: 1472, loss is 0.6203978061676025\n",
            "epoch: 20 step: 1473, loss is 0.9130529165267944\n",
            "epoch: 20 step: 1474, loss is 0.6992729306221008\n",
            "epoch: 20 step: 1475, loss is 0.7865710854530334\n",
            "epoch: 20 step: 1476, loss is 0.9576900601387024\n",
            "epoch: 20 step: 1477, loss is 0.810915470123291\n",
            "epoch: 20 step: 1478, loss is 0.7976177930831909\n",
            "epoch: 20 step: 1479, loss is 0.8368473052978516\n",
            "epoch: 20 step: 1480, loss is 0.8326147794723511\n",
            "epoch: 20 step: 1481, loss is 0.7892839908599854\n",
            "epoch: 20 step: 1482, loss is 0.8819094896316528\n",
            "epoch: 20 step: 1483, loss is 0.7882670164108276\n",
            "epoch: 20 step: 1484, loss is 0.9676225781440735\n",
            "epoch: 20 step: 1485, loss is 0.9606656432151794\n",
            "epoch: 20 step: 1486, loss is 0.8710184693336487\n",
            "epoch: 20 step: 1487, loss is 0.7277994155883789\n",
            "epoch: 20 step: 1488, loss is 0.9252152442932129\n",
            "epoch: 20 step: 1489, loss is 0.7840952277183533\n",
            "epoch: 20 step: 1490, loss is 0.8791648149490356\n",
            "epoch: 20 step: 1491, loss is 0.9348359704017639\n",
            "epoch: 20 step: 1492, loss is 1.0181090831756592\n",
            "epoch: 20 step: 1493, loss is 0.9117392897605896\n",
            "epoch: 20 step: 1494, loss is 0.7089244723320007\n",
            "epoch: 20 step: 1495, loss is 0.8739777207374573\n",
            "epoch: 20 step: 1496, loss is 0.7915548086166382\n",
            "epoch: 20 step: 1497, loss is 0.7479734420776367\n",
            "epoch: 20 step: 1498, loss is 0.8737452030181885\n",
            "epoch: 20 step: 1499, loss is 0.7421198487281799\n",
            "epoch: 20 step: 1500, loss is 0.911237359046936\n",
            "epoch: 20 step: 1501, loss is 0.968670666217804\n",
            "epoch: 20 step: 1502, loss is 0.7273728251457214\n",
            "epoch: 20 step: 1503, loss is 0.8974921107292175\n",
            "epoch: 20 step: 1504, loss is 0.9459616541862488\n",
            "epoch: 20 step: 1505, loss is 0.8696563243865967\n",
            "epoch: 20 step: 1506, loss is 0.8454349637031555\n",
            "epoch: 20 step: 1507, loss is 0.8129457831382751\n",
            "epoch: 20 step: 1508, loss is 1.0882225036621094\n",
            "epoch: 20 step: 1509, loss is 1.0763604640960693\n",
            "epoch: 20 step: 1510, loss is 0.9281439185142517\n",
            "epoch: 20 step: 1511, loss is 0.9312262535095215\n",
            "epoch: 20 step: 1512, loss is 0.6991373896598816\n",
            "epoch: 20 step: 1513, loss is 0.737790584564209\n",
            "epoch: 20 step: 1514, loss is 0.8642309904098511\n",
            "epoch: 20 step: 1515, loss is 0.8448670506477356\n",
            "epoch: 20 step: 1516, loss is 0.7941203713417053\n",
            "epoch: 20 step: 1517, loss is 0.9352207779884338\n",
            "epoch: 20 step: 1518, loss is 0.8266035914421082\n",
            "epoch: 20 step: 1519, loss is 1.0496294498443604\n",
            "epoch: 20 step: 1520, loss is 1.0721099376678467\n",
            "epoch: 20 step: 1521, loss is 0.7761561870574951\n",
            "epoch: 20 step: 1522, loss is 0.8656982779502869\n",
            "epoch: 20 step: 1523, loss is 0.8269125819206238\n",
            "epoch: 20 step: 1524, loss is 0.8411747217178345\n",
            "epoch: 20 step: 1525, loss is 0.8820524215698242\n",
            "epoch: 20 step: 1526, loss is 1.008120059967041\n",
            "epoch: 20 step: 1527, loss is 0.8603637218475342\n",
            "epoch: 20 step: 1528, loss is 0.9616114497184753\n",
            "epoch: 20 step: 1529, loss is 0.9456258416175842\n",
            "epoch: 20 step: 1530, loss is 0.8862106800079346\n",
            "epoch: 20 step: 1531, loss is 0.8421624302864075\n",
            "epoch: 20 step: 1532, loss is 0.9402485489845276\n",
            "epoch: 20 step: 1533, loss is 0.9226071834564209\n",
            "epoch: 20 step: 1534, loss is 0.9269288778305054\n",
            "epoch: 20 step: 1535, loss is 0.9251304864883423\n",
            "epoch: 20 step: 1536, loss is 0.9260315299034119\n",
            "epoch: 20 step: 1537, loss is 0.863765299320221\n",
            "epoch: 20 step: 1538, loss is 0.8195689916610718\n",
            "epoch: 20 step: 1539, loss is 0.8394801616668701\n",
            "epoch: 20 step: 1540, loss is 0.9246926307678223\n",
            "epoch: 20 step: 1541, loss is 0.7243235111236572\n",
            "epoch: 20 step: 1542, loss is 0.8874593377113342\n",
            "epoch: 20 step: 1543, loss is 0.8774817585945129\n",
            "epoch: 20 step: 1544, loss is 0.8975854516029358\n",
            "epoch: 20 step: 1545, loss is 0.9882479906082153\n",
            "epoch: 20 step: 1546, loss is 0.9923791885375977\n",
            "epoch: 20 step: 1547, loss is 0.834344208240509\n",
            "epoch: 20 step: 1548, loss is 0.8396378755569458\n",
            "epoch: 20 step: 1549, loss is 1.1414390802383423\n",
            "epoch: 20 step: 1550, loss is 0.8552067279815674\n",
            "epoch: 20 step: 1551, loss is 0.8594635725021362\n",
            "epoch: 20 step: 1552, loss is 0.819248616695404\n",
            "epoch: 20 step: 1553, loss is 0.9420796632766724\n",
            "epoch: 20 step: 1554, loss is 0.7123982310295105\n",
            "epoch: 20 step: 1555, loss is 0.8504707217216492\n",
            "epoch: 20 step: 1556, loss is 1.0454212427139282\n",
            "epoch: 20 step: 1557, loss is 0.8473919630050659\n",
            "epoch: 20 step: 1558, loss is 0.8311118483543396\n",
            "epoch: 20 step: 1559, loss is 0.9376102089881897\n",
            "epoch: 20 step: 1560, loss is 0.9149888157844543\n",
            "epoch: 20 step: 1561, loss is 0.8426949381828308\n",
            "epoch: 20 step: 1562, loss is 1.162009358406067\n",
            "epoch: 20 step: 1563, loss is 0.8800557255744934\n",
            "epoch: 20 step: 1564, loss is 0.9163658022880554\n",
            "epoch: 20 step: 1565, loss is 0.8456763625144958\n",
            "epoch: 20 step: 1566, loss is 0.7761168479919434\n",
            "epoch: 20 step: 1567, loss is 0.8478732109069824\n",
            "epoch: 20 step: 1568, loss is 0.8869861960411072\n",
            "epoch: 20 step: 1569, loss is 0.7812184691429138\n",
            "epoch: 20 step: 1570, loss is 0.9989017844200134\n",
            "epoch: 20 step: 1571, loss is 0.7986360192298889\n",
            "epoch: 20 step: 1572, loss is 0.7796956300735474\n",
            "epoch: 20 step: 1573, loss is 0.8475112915039062\n",
            "epoch: 20 step: 1574, loss is 0.7820706367492676\n",
            "epoch: 20 step: 1575, loss is 0.9770927429199219\n",
            "epoch: 20 step: 1576, loss is 0.8565931916236877\n",
            "epoch: 20 step: 1577, loss is 0.7731834053993225\n",
            "epoch: 20 step: 1578, loss is 0.9958988428115845\n",
            "epoch: 20 step: 1579, loss is 0.7097775340080261\n",
            "epoch: 20 step: 1580, loss is 1.0245938301086426\n",
            "epoch: 20 step: 1581, loss is 0.9120969176292419\n",
            "epoch: 20 step: 1582, loss is 0.8056934475898743\n",
            "epoch: 20 step: 1583, loss is 1.0336004495620728\n",
            "epoch: 20 step: 1584, loss is 0.8177274465560913\n",
            "epoch: 20 step: 1585, loss is 0.8972870111465454\n",
            "epoch: 20 step: 1586, loss is 0.844049870967865\n",
            "epoch: 20 step: 1587, loss is 1.0005357265472412\n",
            "epoch: 20 step: 1588, loss is 0.9008274078369141\n",
            "epoch: 20 step: 1589, loss is 0.9927581548690796\n",
            "epoch: 20 step: 1590, loss is 0.8977724313735962\n",
            "epoch: 20 step: 1591, loss is 0.8718814253807068\n",
            "epoch: 20 step: 1592, loss is 1.033939242362976\n",
            "epoch: 20 step: 1593, loss is 0.9440141320228577\n",
            "epoch: 20 step: 1594, loss is 0.9007470607757568\n",
            "epoch: 20 step: 1595, loss is 0.8525810241699219\n",
            "epoch: 20 step: 1596, loss is 0.9203048944473267\n",
            "epoch: 20 step: 1597, loss is 0.850038468837738\n",
            "epoch: 20 step: 1598, loss is 0.7525398135185242\n",
            "epoch: 20 step: 1599, loss is 0.8955821394920349\n",
            "epoch: 20 step: 1600, loss is 1.0148645639419556\n",
            "epoch: 20 step: 1601, loss is 1.1918277740478516\n",
            "epoch: 20 step: 1602, loss is 1.02265202999115\n",
            "epoch: 20 step: 1603, loss is 0.8227789402008057\n",
            "epoch: 20 step: 1604, loss is 0.9637362360954285\n",
            "epoch: 20 step: 1605, loss is 0.9711022973060608\n",
            "epoch: 20 step: 1606, loss is 0.9266472458839417\n",
            "epoch: 20 step: 1607, loss is 0.7650218605995178\n",
            "epoch: 20 step: 1608, loss is 1.033944010734558\n",
            "epoch: 20 step: 1609, loss is 0.8141953349113464\n",
            "epoch: 20 step: 1610, loss is 0.8607369661331177\n",
            "epoch: 20 step: 1611, loss is 0.9693908095359802\n",
            "epoch: 20 step: 1612, loss is 0.8068683743476868\n",
            "epoch: 20 step: 1613, loss is 0.8322781920433044\n",
            "epoch: 20 step: 1614, loss is 0.7300859689712524\n",
            "epoch: 20 step: 1615, loss is 0.989315390586853\n",
            "epoch: 20 step: 1616, loss is 0.866303563117981\n",
            "epoch: 20 step: 1617, loss is 0.6953670978546143\n",
            "epoch: 20 step: 1618, loss is 0.9783235788345337\n",
            "epoch: 20 step: 1619, loss is 0.875795304775238\n",
            "epoch: 20 step: 1620, loss is 0.8785580992698669\n",
            "epoch: 20 step: 1621, loss is 0.9489566087722778\n",
            "epoch: 20 step: 1622, loss is 0.8201975226402283\n",
            "epoch: 20 step: 1623, loss is 1.0519942045211792\n",
            "epoch: 20 step: 1624, loss is 0.873080313205719\n",
            "epoch: 20 step: 1625, loss is 1.0119295120239258\n",
            "epoch: 20 step: 1626, loss is 0.9500791430473328\n",
            "epoch: 20 step: 1627, loss is 0.9357341527938843\n",
            "epoch: 20 step: 1628, loss is 0.8991700410842896\n",
            "epoch: 20 step: 1629, loss is 0.9566243886947632\n",
            "epoch: 20 step: 1630, loss is 0.9397227764129639\n",
            "epoch: 20 step: 1631, loss is 0.9046555757522583\n",
            "epoch: 20 step: 1632, loss is 0.9407513737678528\n",
            "epoch: 20 step: 1633, loss is 0.8386124968528748\n",
            "epoch: 20 step: 1634, loss is 1.051269769668579\n",
            "epoch: 20 step: 1635, loss is 0.9305960536003113\n",
            "epoch: 20 step: 1636, loss is 1.0202311277389526\n",
            "epoch: 20 step: 1637, loss is 1.18837308883667\n",
            "epoch: 20 step: 1638, loss is 0.7934040427207947\n",
            "epoch: 20 step: 1639, loss is 0.996391773223877\n",
            "epoch: 20 step: 1640, loss is 0.9980303049087524\n",
            "epoch: 20 step: 1641, loss is 0.8998693227767944\n",
            "epoch: 20 step: 1642, loss is 0.9594343304634094\n",
            "epoch: 20 step: 1643, loss is 0.9266277551651001\n",
            "epoch: 20 step: 1644, loss is 0.8229313492774963\n",
            "epoch: 20 step: 1645, loss is 1.0797767639160156\n",
            "epoch: 20 step: 1646, loss is 0.9355061054229736\n",
            "epoch: 20 step: 1647, loss is 0.8234604001045227\n",
            "epoch: 20 step: 1648, loss is 1.0834646224975586\n",
            "epoch: 20 step: 1649, loss is 0.9172585010528564\n",
            "epoch: 20 step: 1650, loss is 0.8463714718818665\n",
            "epoch: 20 step: 1651, loss is 0.8644146919250488\n",
            "epoch: 20 step: 1652, loss is 0.9999387264251709\n",
            "epoch: 20 step: 1653, loss is 0.8213814496994019\n",
            "epoch: 20 step: 1654, loss is 0.8880652189254761\n",
            "epoch: 20 step: 1655, loss is 0.8635531067848206\n",
            "epoch: 20 step: 1656, loss is 0.9277941584587097\n",
            "epoch: 20 step: 1657, loss is 0.9304532408714294\n",
            "epoch: 20 step: 1658, loss is 0.9053290486335754\n",
            "epoch: 20 step: 1659, loss is 0.8763465881347656\n",
            "epoch: 20 step: 1660, loss is 0.8004015684127808\n",
            "epoch: 20 step: 1661, loss is 0.8243423700332642\n",
            "epoch: 20 step: 1662, loss is 0.766718864440918\n",
            "epoch: 20 step: 1663, loss is 0.8108301162719727\n",
            "epoch: 20 step: 1664, loss is 0.8265476226806641\n",
            "epoch: 20 step: 1665, loss is 0.8289459347724915\n",
            "epoch: 20 step: 1666, loss is 1.1007840633392334\n",
            "epoch: 20 step: 1667, loss is 0.8128116726875305\n",
            "epoch: 20 step: 1668, loss is 0.9363929033279419\n",
            "epoch: 20 step: 1669, loss is 0.7773303389549255\n",
            "epoch: 20 step: 1670, loss is 0.9928804636001587\n",
            "epoch: 20 step: 1671, loss is 0.879599928855896\n",
            "epoch: 20 step: 1672, loss is 0.873059868812561\n",
            "epoch: 20 step: 1673, loss is 0.7905257940292358\n",
            "epoch: 20 step: 1674, loss is 1.1585277318954468\n",
            "epoch: 20 step: 1675, loss is 0.7937530875205994\n",
            "epoch: 20 step: 1676, loss is 0.8946682810783386\n",
            "epoch: 20 step: 1677, loss is 0.9158304929733276\n",
            "epoch: 20 step: 1678, loss is 0.9014989733695984\n",
            "epoch: 20 step: 1679, loss is 1.0024316310882568\n",
            "epoch: 20 step: 1680, loss is 0.9859797358512878\n",
            "epoch: 20 step: 1681, loss is 0.744292140007019\n",
            "epoch: 20 step: 1682, loss is 0.9455143809318542\n",
            "epoch: 20 step: 1683, loss is 0.9110512733459473\n",
            "epoch: 20 step: 1684, loss is 0.8233605623245239\n",
            "epoch: 20 step: 1685, loss is 0.9255604147911072\n",
            "epoch: 20 step: 1686, loss is 0.8508937358856201\n",
            "epoch: 20 step: 1687, loss is 0.9278751611709595\n",
            "epoch: 20 step: 1688, loss is 0.8934439420700073\n",
            "epoch: 20 step: 1689, loss is 0.964886486530304\n",
            "epoch: 20 step: 1690, loss is 0.9799953699111938\n",
            "epoch: 20 step: 1691, loss is 1.0011683702468872\n",
            "epoch: 20 step: 1692, loss is 1.032031536102295\n",
            "epoch: 20 step: 1693, loss is 0.913195013999939\n",
            "epoch: 20 step: 1694, loss is 1.1275171041488647\n",
            "epoch: 20 step: 1695, loss is 0.9776186943054199\n",
            "epoch: 20 step: 1696, loss is 0.874111533164978\n",
            "epoch: 20 step: 1697, loss is 0.9107975959777832\n",
            "epoch: 20 step: 1698, loss is 0.9360750317573547\n",
            "epoch: 20 step: 1699, loss is 0.8255609273910522\n",
            "epoch: 20 step: 1700, loss is 1.0590764284133911\n",
            "epoch: 20 step: 1701, loss is 0.752932071685791\n",
            "epoch: 20 step: 1702, loss is 0.8815204501152039\n",
            "epoch: 20 step: 1703, loss is 0.9759483337402344\n",
            "epoch: 20 step: 1704, loss is 0.901733934879303\n",
            "epoch: 20 step: 1705, loss is 0.8448258638381958\n",
            "epoch: 20 step: 1706, loss is 0.8441898822784424\n",
            "epoch: 20 step: 1707, loss is 0.8725193738937378\n",
            "epoch: 20 step: 1708, loss is 0.8613014221191406\n",
            "epoch: 20 step: 1709, loss is 0.7825459241867065\n",
            "epoch: 20 step: 1710, loss is 1.0101767778396606\n",
            "epoch: 20 step: 1711, loss is 0.8293911814689636\n",
            "epoch: 20 step: 1712, loss is 0.7949713468551636\n",
            "epoch: 20 step: 1713, loss is 0.8434960842132568\n",
            "epoch: 20 step: 1714, loss is 1.0314353704452515\n",
            "epoch: 20 step: 1715, loss is 0.9077109098434448\n",
            "epoch: 20 step: 1716, loss is 0.9023964405059814\n",
            "epoch: 20 step: 1717, loss is 0.8473005890846252\n",
            "epoch: 20 step: 1718, loss is 0.8964477777481079\n",
            "epoch: 20 step: 1719, loss is 0.7239634394645691\n",
            "epoch: 20 step: 1720, loss is 1.0015370845794678\n",
            "epoch: 20 step: 1721, loss is 0.8397778868675232\n",
            "epoch: 20 step: 1722, loss is 0.9744437336921692\n",
            "epoch: 20 step: 1723, loss is 0.7826219201087952\n",
            "epoch: 20 step: 1724, loss is 0.9288635849952698\n",
            "epoch: 20 step: 1725, loss is 0.6900349259376526\n",
            "epoch: 20 step: 1726, loss is 0.925037145614624\n",
            "epoch: 20 step: 1727, loss is 1.1035656929016113\n",
            "epoch: 20 step: 1728, loss is 0.8902457356452942\n",
            "epoch: 20 step: 1729, loss is 0.9817897081375122\n",
            "epoch: 20 step: 1730, loss is 1.006358027458191\n",
            "epoch: 20 step: 1731, loss is 0.9990594387054443\n",
            "epoch: 20 step: 1732, loss is 0.9880242347717285\n",
            "epoch: 20 step: 1733, loss is 0.8116728663444519\n",
            "epoch: 20 step: 1734, loss is 0.9090802669525146\n",
            "epoch: 20 step: 1735, loss is 0.8325180411338806\n",
            "epoch: 20 step: 1736, loss is 0.9075382947921753\n",
            "epoch: 20 step: 1737, loss is 0.641323983669281\n",
            "epoch: 20 step: 1738, loss is 0.9058352708816528\n",
            "epoch: 20 step: 1739, loss is 0.8189285397529602\n",
            "epoch: 20 step: 1740, loss is 0.8292369842529297\n",
            "epoch: 20 step: 1741, loss is 0.9205822348594666\n",
            "epoch: 20 step: 1742, loss is 1.017013669013977\n",
            "epoch: 20 step: 1743, loss is 0.9395607709884644\n",
            "epoch: 20 step: 1744, loss is 0.9880971312522888\n",
            "epoch: 20 step: 1745, loss is 0.8168510794639587\n",
            "epoch: 20 step: 1746, loss is 0.8782838582992554\n",
            "epoch: 20 step: 1747, loss is 0.9725834727287292\n",
            "epoch: 20 step: 1748, loss is 0.9631032943725586\n",
            "epoch: 20 step: 1749, loss is 0.8051547408103943\n",
            "epoch: 20 step: 1750, loss is 0.9076961278915405\n",
            "epoch: 20 step: 1751, loss is 0.8867477774620056\n",
            "epoch: 20 step: 1752, loss is 1.0510720014572144\n",
            "epoch: 20 step: 1753, loss is 1.2374167442321777\n",
            "epoch: 20 step: 1754, loss is 0.9682250022888184\n",
            "epoch: 20 step: 1755, loss is 0.7874957919120789\n",
            "epoch: 20 step: 1756, loss is 1.0610800981521606\n",
            "epoch: 20 step: 1757, loss is 0.9662498831748962\n",
            "epoch: 20 step: 1758, loss is 0.951758861541748\n",
            "epoch: 20 step: 1759, loss is 0.9255006313323975\n",
            "epoch: 20 step: 1760, loss is 0.8512604832649231\n",
            "epoch: 20 step: 1761, loss is 0.7777759432792664\n",
            "epoch: 20 step: 1762, loss is 0.9638309478759766\n",
            "epoch: 20 step: 1763, loss is 0.8174916505813599\n",
            "epoch: 20 step: 1764, loss is 0.8867546916007996\n",
            "epoch: 20 step: 1765, loss is 0.7431975603103638\n",
            "epoch: 20 step: 1766, loss is 0.9675525426864624\n",
            "epoch: 20 step: 1767, loss is 0.7866952419281006\n",
            "epoch: 20 step: 1768, loss is 0.8420519232749939\n",
            "epoch: 20 step: 1769, loss is 1.0402177572250366\n",
            "epoch: 20 step: 1770, loss is 0.8935992121696472\n",
            "epoch: 20 step: 1771, loss is 0.780214786529541\n",
            "epoch: 20 step: 1772, loss is 1.0006394386291504\n",
            "epoch: 20 step: 1773, loss is 0.9260939955711365\n",
            "epoch: 20 step: 1774, loss is 0.9688332676887512\n",
            "epoch: 20 step: 1775, loss is 0.8537806868553162\n",
            "epoch: 20 step: 1776, loss is 1.0136092901229858\n",
            "epoch: 20 step: 1777, loss is 0.9471157193183899\n",
            "epoch: 20 step: 1778, loss is 0.8225408792495728\n",
            "epoch: 20 step: 1779, loss is 0.6969730257987976\n",
            "epoch: 20 step: 1780, loss is 1.1186256408691406\n",
            "epoch: 20 step: 1781, loss is 0.8446789979934692\n",
            "epoch: 20 step: 1782, loss is 0.746367335319519\n",
            "epoch: 20 step: 1783, loss is 0.7606161832809448\n",
            "epoch: 20 step: 1784, loss is 0.809596598148346\n",
            "epoch: 20 step: 1785, loss is 0.8950480818748474\n",
            "epoch: 20 step: 1786, loss is 0.6598393321037292\n",
            "epoch: 20 step: 1787, loss is 0.8376401662826538\n",
            "epoch: 20 step: 1788, loss is 0.9470455646514893\n",
            "epoch: 20 step: 1789, loss is 0.9540975689888\n",
            "epoch: 20 step: 1790, loss is 0.980901837348938\n",
            "epoch: 20 step: 1791, loss is 0.7716483473777771\n",
            "epoch: 20 step: 1792, loss is 0.8093323707580566\n",
            "epoch: 20 step: 1793, loss is 0.9384717345237732\n",
            "epoch: 20 step: 1794, loss is 0.8501327633857727\n",
            "epoch: 20 step: 1795, loss is 0.8214716911315918\n",
            "epoch: 20 step: 1796, loss is 0.9857033491134644\n",
            "epoch: 20 step: 1797, loss is 0.8701950311660767\n",
            "epoch: 20 step: 1798, loss is 0.8562318682670593\n",
            "epoch: 20 step: 1799, loss is 0.8751058578491211\n",
            "epoch: 20 step: 1800, loss is 0.8049005270004272\n",
            "epoch: 20 step: 1801, loss is 0.8164274096488953\n",
            "epoch: 20 step: 1802, loss is 0.9051545858383179\n",
            "epoch: 20 step: 1803, loss is 0.9778733849525452\n",
            "epoch: 20 step: 1804, loss is 0.8874675035476685\n",
            "epoch: 20 step: 1805, loss is 0.7814913988113403\n",
            "epoch: 20 step: 1806, loss is 1.0250719785690308\n",
            "epoch: 20 step: 1807, loss is 0.9581798911094666\n",
            "epoch: 20 step: 1808, loss is 1.0094236135482788\n",
            "epoch: 20 step: 1809, loss is 0.8944063782691956\n",
            "epoch: 20 step: 1810, loss is 0.6880170106887817\n",
            "epoch: 20 step: 1811, loss is 0.978018581867218\n",
            "epoch: 20 step: 1812, loss is 0.9009544253349304\n",
            "epoch: 20 step: 1813, loss is 1.0796527862548828\n",
            "epoch: 20 step: 1814, loss is 0.9230618476867676\n",
            "epoch: 20 step: 1815, loss is 0.8771716356277466\n",
            "epoch: 20 step: 1816, loss is 0.836724042892456\n",
            "epoch: 20 step: 1817, loss is 1.0003671646118164\n",
            "epoch: 20 step: 1818, loss is 0.8813441395759583\n",
            "epoch: 20 step: 1819, loss is 0.9011059999465942\n",
            "epoch: 20 step: 1820, loss is 0.9749459028244019\n",
            "epoch: 20 step: 1821, loss is 1.0573582649230957\n",
            "epoch: 20 step: 1822, loss is 0.7752951979637146\n",
            "epoch: 20 step: 1823, loss is 0.9066834449768066\n",
            "epoch: 20 step: 1824, loss is 0.8128136992454529\n",
            "epoch: 20 step: 1825, loss is 0.9217197895050049\n",
            "epoch: 20 step: 1826, loss is 0.8378582000732422\n",
            "epoch: 20 step: 1827, loss is 1.0378391742706299\n",
            "epoch: 20 step: 1828, loss is 0.8543841242790222\n",
            "epoch: 20 step: 1829, loss is 0.8634521961212158\n",
            "epoch: 20 step: 1830, loss is 0.7892722487449646\n",
            "epoch: 20 step: 1831, loss is 0.8421148061752319\n",
            "epoch: 20 step: 1832, loss is 0.69783616065979\n",
            "epoch: 20 step: 1833, loss is 0.9693293571472168\n",
            "epoch: 20 step: 1834, loss is 0.8555659651756287\n",
            "epoch: 20 step: 1835, loss is 0.9473426342010498\n",
            "epoch: 20 step: 1836, loss is 0.9221802949905396\n",
            "epoch: 20 step: 1837, loss is 0.8863701820373535\n",
            "epoch: 20 step: 1838, loss is 0.8622617721557617\n",
            "epoch: 20 step: 1839, loss is 0.9184552431106567\n",
            "epoch: 20 step: 1840, loss is 0.8492094874382019\n",
            "epoch: 20 step: 1841, loss is 0.9046651721000671\n",
            "epoch: 20 step: 1842, loss is 0.8815367221832275\n",
            "epoch: 20 step: 1843, loss is 0.8096285462379456\n",
            "epoch: 20 step: 1844, loss is 0.901906430721283\n",
            "epoch: 20 step: 1845, loss is 0.9095891714096069\n",
            "epoch: 20 step: 1846, loss is 0.9576780796051025\n",
            "epoch: 20 step: 1847, loss is 0.9844663143157959\n",
            "epoch: 20 step: 1848, loss is 0.9077057838439941\n",
            "epoch: 20 step: 1849, loss is 0.8761155605316162\n",
            "epoch: 20 step: 1850, loss is 0.836945116519928\n",
            "epoch: 20 step: 1851, loss is 0.846741795539856\n",
            "epoch: 20 step: 1852, loss is 0.8841715455055237\n",
            "epoch: 20 step: 1853, loss is 0.9178151488304138\n",
            "epoch: 20 step: 1854, loss is 0.995612621307373\n",
            "epoch: 20 step: 1855, loss is 1.1441775560379028\n",
            "epoch: 20 step: 1856, loss is 0.970618486404419\n",
            "epoch: 20 step: 1857, loss is 0.9010171890258789\n",
            "epoch: 20 step: 1858, loss is 0.8351281881332397\n",
            "epoch: 20 step: 1859, loss is 0.9643647074699402\n",
            "epoch: 20 step: 1860, loss is 0.9521090984344482\n",
            "epoch: 20 step: 1861, loss is 0.9143071174621582\n",
            "epoch: 20 step: 1862, loss is 0.7996350526809692\n",
            "epoch: 20 step: 1863, loss is 0.9284764528274536\n",
            "epoch: 20 step: 1864, loss is 0.8600813150405884\n",
            "epoch: 20 step: 1865, loss is 0.9777612090110779\n",
            "epoch: 20 step: 1866, loss is 0.8365053534507751\n",
            "epoch: 20 step: 1867, loss is 1.0465669631958008\n",
            "epoch: 20 step: 1868, loss is 1.0806432962417603\n",
            "epoch: 20 step: 1869, loss is 1.0140148401260376\n",
            "epoch: 20 step: 1870, loss is 0.884711503982544\n",
            "epoch: 20 step: 1871, loss is 0.8813813328742981\n",
            "epoch: 20 step: 1872, loss is 0.9651172161102295\n",
            "epoch: 20 step: 1873, loss is 1.1352463960647583\n",
            "epoch: 20 step: 1874, loss is 0.8683275580406189\n",
            "epoch: 20 step: 1875, loss is 0.9308838248252869\n",
            "epoch: 20 step: 1876, loss is 0.790377676486969\n",
            "epoch: 20 step: 1877, loss is 0.915115237236023\n",
            "epoch: 20 step: 1878, loss is 1.0192114114761353\n",
            "epoch: 20 step: 1879, loss is 0.9044241309165955\n",
            "epoch: 20 step: 1880, loss is 0.7834579944610596\n",
            "epoch: 20 step: 1881, loss is 0.9793208837509155\n",
            "epoch: 20 step: 1882, loss is 0.8766248226165771\n",
            "epoch: 20 step: 1883, loss is 0.8671000599861145\n",
            "epoch: 20 step: 1884, loss is 0.9612360000610352\n",
            "epoch: 20 step: 1885, loss is 0.9233718514442444\n",
            "epoch: 20 step: 1886, loss is 0.7828893661499023\n",
            "epoch: 20 step: 1887, loss is 0.9252013564109802\n",
            "epoch: 20 step: 1888, loss is 0.9035810828208923\n",
            "epoch: 20 step: 1889, loss is 0.8878709077835083\n",
            "epoch: 20 step: 1890, loss is 0.8897840976715088\n",
            "epoch: 20 step: 1891, loss is 0.9529300928115845\n",
            "epoch: 20 step: 1892, loss is 1.0002795457839966\n",
            "epoch: 20 step: 1893, loss is 1.1160703897476196\n",
            "epoch: 20 step: 1894, loss is 0.9420132040977478\n",
            "epoch: 20 step: 1895, loss is 0.7655820250511169\n",
            "epoch: 20 step: 1896, loss is 0.9048504829406738\n",
            "epoch: 20 step: 1897, loss is 0.8576064705848694\n",
            "epoch: 20 step: 1898, loss is 0.8416098356246948\n",
            "epoch: 20 step: 1899, loss is 0.7834504842758179\n",
            "epoch: 20 step: 1900, loss is 0.7514394521713257\n",
            "epoch: 20 step: 1901, loss is 0.8912734985351562\n",
            "epoch: 20 step: 1902, loss is 0.8886876702308655\n",
            "epoch: 20 step: 1903, loss is 1.0062384605407715\n",
            "epoch: 20 step: 1904, loss is 0.950437605381012\n",
            "epoch: 20 step: 1905, loss is 0.9911791086196899\n",
            "epoch: 20 step: 1906, loss is 0.9821896553039551\n",
            "epoch: 20 step: 1907, loss is 0.807698667049408\n",
            "epoch: 20 step: 1908, loss is 0.9245165586471558\n",
            "epoch: 20 step: 1909, loss is 0.731843888759613\n",
            "epoch: 20 step: 1910, loss is 0.815686821937561\n",
            "epoch: 20 step: 1911, loss is 0.7559022307395935\n",
            "epoch: 20 step: 1912, loss is 0.9089181423187256\n",
            "epoch: 20 step: 1913, loss is 1.0314650535583496\n",
            "epoch: 20 step: 1914, loss is 0.7655457854270935\n",
            "epoch: 20 step: 1915, loss is 0.822669267654419\n",
            "epoch: 20 step: 1916, loss is 0.7966066002845764\n",
            "epoch: 20 step: 1917, loss is 0.847331702709198\n",
            "epoch: 20 step: 1918, loss is 0.8921215534210205\n",
            "epoch: 20 step: 1919, loss is 0.9243131875991821\n",
            "epoch: 20 step: 1920, loss is 0.8944557309150696\n",
            "epoch: 20 step: 1921, loss is 0.9365984797477722\n",
            "epoch: 20 step: 1922, loss is 0.8618463277816772\n",
            "epoch: 20 step: 1923, loss is 0.9039133787155151\n",
            "epoch: 20 step: 1924, loss is 0.956693708896637\n",
            "epoch: 20 step: 1925, loss is 0.8697282075881958\n",
            "epoch: 20 step: 1926, loss is 0.8324998021125793\n",
            "epoch: 20 step: 1927, loss is 0.8872717022895813\n",
            "epoch: 20 step: 1928, loss is 1.0364470481872559\n",
            "epoch: 20 step: 1929, loss is 0.9453747272491455\n",
            "epoch: 20 step: 1930, loss is 0.725775420665741\n",
            "epoch: 20 step: 1931, loss is 0.8892766833305359\n",
            "epoch: 20 step: 1932, loss is 0.6031832098960876\n",
            "epoch: 20 step: 1933, loss is 0.9941200017929077\n",
            "epoch: 20 step: 1934, loss is 0.866887629032135\n",
            "epoch: 20 step: 1935, loss is 0.8504505157470703\n",
            "epoch: 20 step: 1936, loss is 1.044142723083496\n",
            "epoch: 20 step: 1937, loss is 0.7948651909828186\n",
            "epoch: 20 step: 1938, loss is 0.997253954410553\n",
            "epoch: 20 step: 1939, loss is 1.037203311920166\n",
            "epoch: 20 step: 1940, loss is 0.9758766293525696\n",
            "epoch: 20 step: 1941, loss is 1.0688773393630981\n",
            "epoch: 20 step: 1942, loss is 0.9311818480491638\n",
            "epoch: 20 step: 1943, loss is 0.7607836723327637\n",
            "epoch: 20 step: 1944, loss is 0.8260434865951538\n",
            "epoch: 20 step: 1945, loss is 0.8556457757949829\n",
            "epoch: 20 step: 1946, loss is 0.9730033874511719\n",
            "epoch: 20 step: 1947, loss is 0.8597052693367004\n",
            "epoch: 20 step: 1948, loss is 0.8316649794578552\n",
            "epoch: 20 step: 1949, loss is 0.763825535774231\n",
            "epoch: 20 step: 1950, loss is 0.8989746570587158\n",
            "epoch: 20 step: 1951, loss is 0.7143588662147522\n",
            "epoch: 20 step: 1952, loss is 0.8013685941696167\n",
            "epoch: 20 step: 1953, loss is 0.9835641384124756\n",
            "epoch: 20 step: 1954, loss is 0.9595426321029663\n",
            "epoch: 20 step: 1955, loss is 0.855896532535553\n",
            "epoch: 20 step: 1956, loss is 0.842883288860321\n",
            "epoch: 20 step: 1957, loss is 0.8420378565788269\n",
            "epoch: 20 step: 1958, loss is 0.9343971610069275\n",
            "epoch: 20 step: 1959, loss is 0.7447887063026428\n",
            "epoch: 20 step: 1960, loss is 0.7994964122772217\n",
            "epoch: 20 step: 1961, loss is 1.211550235748291\n",
            "epoch: 20 step: 1962, loss is 0.9783025979995728\n",
            "epoch: 20 step: 1963, loss is 0.9654181599617004\n",
            "epoch: 20 step: 1964, loss is 0.8497071862220764\n",
            "epoch: 20 step: 1965, loss is 0.6860857009887695\n",
            "epoch: 20 step: 1966, loss is 0.8960506916046143\n",
            "epoch: 20 step: 1967, loss is 0.798214852809906\n",
            "epoch: 20 step: 1968, loss is 0.8526683449745178\n",
            "epoch: 20 step: 1969, loss is 0.7351706027984619\n",
            "epoch: 20 step: 1970, loss is 0.9305421710014343\n",
            "epoch: 20 step: 1971, loss is 0.7972484230995178\n",
            "epoch: 20 step: 1972, loss is 1.0054937601089478\n",
            "epoch: 20 step: 1973, loss is 0.9406548738479614\n",
            "epoch: 20 step: 1974, loss is 0.9230974316596985\n",
            "epoch: 20 step: 1975, loss is 1.0090150833129883\n",
            "epoch: 20 step: 1976, loss is 0.961858868598938\n",
            "epoch: 20 step: 1977, loss is 0.8646836280822754\n",
            "epoch: 20 step: 1978, loss is 0.8886846303939819\n",
            "epoch: 20 step: 1979, loss is 0.8368898034095764\n",
            "epoch: 20 step: 1980, loss is 0.8543639779090881\n",
            "epoch: 20 step: 1981, loss is 1.092500925064087\n",
            "epoch: 20 step: 1982, loss is 0.9599223732948303\n",
            "epoch: 20 step: 1983, loss is 0.6997082233428955\n",
            "epoch: 20 step: 1984, loss is 0.9976181983947754\n",
            "epoch: 20 step: 1985, loss is 0.8500859141349792\n",
            "epoch: 20 step: 1986, loss is 0.9814980626106262\n",
            "epoch: 20 step: 1987, loss is 0.922479510307312\n",
            "epoch: 20 step: 1988, loss is 1.051543951034546\n",
            "epoch: 20 step: 1989, loss is 0.891893744468689\n",
            "epoch: 20 step: 1990, loss is 1.0612702369689941\n",
            "epoch: 20 step: 1991, loss is 0.8566969633102417\n",
            "epoch: 20 step: 1992, loss is 0.8578501343727112\n",
            "epoch: 20 step: 1993, loss is 0.748458981513977\n",
            "epoch: 20 step: 1994, loss is 0.9113515615463257\n",
            "epoch: 20 step: 1995, loss is 0.9586966037750244\n",
            "epoch: 20 step: 1996, loss is 0.7724527716636658\n",
            "epoch: 20 step: 1997, loss is 0.9569782018661499\n",
            "epoch: 20 step: 1998, loss is 0.9425183534622192\n",
            "epoch: 20 step: 1999, loss is 1.0429329872131348\n",
            "epoch: 20 step: 2000, loss is 0.7715687155723572\n",
            "epoch: 20 step: 2001, loss is 0.9427754878997803\n",
            "epoch: 20 step: 2002, loss is 0.7491492629051208\n",
            "epoch: 20 step: 2003, loss is 0.9078693389892578\n",
            "epoch: 20 step: 2004, loss is 0.982843279838562\n",
            "epoch: 20 step: 2005, loss is 0.8368663191795349\n",
            "epoch: 20 step: 2006, loss is 0.9475783705711365\n",
            "epoch: 20 step: 2007, loss is 1.0292457342147827\n",
            "epoch: 20 step: 2008, loss is 0.9191736578941345\n",
            "epoch: 20 step: 2009, loss is 0.9387679696083069\n",
            "epoch: 20 step: 2010, loss is 0.8867659568786621\n",
            "epoch: 20 step: 2011, loss is 0.838932454586029\n",
            "epoch: 20 step: 2012, loss is 0.8223419189453125\n",
            "epoch: 20 step: 2013, loss is 0.883640468120575\n",
            "epoch: 20 step: 2014, loss is 0.9741880893707275\n",
            "epoch: 20 step: 2015, loss is 0.9318509697914124\n",
            "epoch: 20 step: 2016, loss is 0.7493019104003906\n",
            "epoch: 20 step: 2017, loss is 1.0207325220108032\n",
            "epoch: 20 step: 2018, loss is 0.9590067863464355\n",
            "epoch: 20 step: 2019, loss is 0.7987698316574097\n",
            "epoch: 20 step: 2020, loss is 0.7133092880249023\n",
            "epoch: 20 step: 2021, loss is 0.9146997332572937\n",
            "epoch: 20 step: 2022, loss is 0.8599793314933777\n",
            "epoch: 20 step: 2023, loss is 0.7493936419487\n",
            "epoch: 20 step: 2024, loss is 0.9743741750717163\n",
            "epoch: 20 step: 2025, loss is 1.014040231704712\n",
            "epoch: 20 step: 2026, loss is 0.8307678699493408\n",
            "epoch: 20 step: 2027, loss is 0.933706521987915\n",
            "epoch: 20 step: 2028, loss is 0.9431248903274536\n",
            "epoch: 20 step: 2029, loss is 0.7656879425048828\n",
            "epoch: 20 step: 2030, loss is 0.8499352931976318\n",
            "epoch: 20 step: 2031, loss is 0.7501909732818604\n",
            "epoch: 20 step: 2032, loss is 0.8970816135406494\n",
            "epoch: 20 step: 2033, loss is 0.7271345257759094\n",
            "epoch: 20 step: 2034, loss is 0.8939239382743835\n",
            "epoch: 20 step: 2035, loss is 0.8677257895469666\n",
            "epoch: 20 step: 2036, loss is 0.8988749384880066\n",
            "epoch: 20 step: 2037, loss is 0.8376154899597168\n",
            "epoch: 20 step: 2038, loss is 0.8833032250404358\n",
            "epoch: 20 step: 2039, loss is 0.8430860638618469\n",
            "epoch: 20 step: 2040, loss is 0.9025468826293945\n",
            "epoch: 20 step: 2041, loss is 0.8171935081481934\n",
            "epoch: 20 step: 2042, loss is 0.8911716938018799\n",
            "epoch: 20 step: 2043, loss is 0.7994773983955383\n",
            "epoch: 20 step: 2044, loss is 0.7457510828971863\n",
            "epoch: 20 step: 2045, loss is 0.8120992183685303\n",
            "epoch: 20 step: 2046, loss is 1.0951292514801025\n",
            "epoch: 20 step: 2047, loss is 0.8784651160240173\n",
            "epoch: 20 step: 2048, loss is 1.0565863847732544\n",
            "epoch: 20 step: 2049, loss is 0.9481741786003113\n",
            "epoch: 20 step: 2050, loss is 0.8526583909988403\n",
            "epoch: 20 step: 2051, loss is 1.0291833877563477\n",
            "epoch: 20 step: 2052, loss is 0.9558922052383423\n",
            "epoch: 20 step: 2053, loss is 0.946736752986908\n",
            "epoch: 20 step: 2054, loss is 0.8406999707221985\n",
            "epoch: 20 step: 2055, loss is 1.0767273902893066\n",
            "epoch: 20 step: 2056, loss is 0.9150326251983643\n",
            "epoch: 20 step: 2057, loss is 0.7654833197593689\n",
            "epoch: 20 step: 2058, loss is 0.9347139000892639\n",
            "epoch: 20 step: 2059, loss is 0.801443338394165\n",
            "epoch: 20 step: 2060, loss is 0.8889446258544922\n",
            "epoch: 20 step: 2061, loss is 1.010238528251648\n",
            "epoch: 20 step: 2062, loss is 0.8691476583480835\n",
            "epoch: 20 step: 2063, loss is 0.7619975209236145\n",
            "epoch: 20 step: 2064, loss is 0.9319061040878296\n",
            "epoch: 20 step: 2065, loss is 0.8384568691253662\n",
            "epoch: 20 step: 2066, loss is 0.9729058742523193\n",
            "epoch: 20 step: 2067, loss is 0.8605717420578003\n",
            "epoch: 20 step: 2068, loss is 0.7920041680335999\n",
            "epoch: 20 step: 2069, loss is 1.0431389808654785\n",
            "epoch: 20 step: 2070, loss is 0.9015169739723206\n",
            "epoch: 20 step: 2071, loss is 0.8883669972419739\n",
            "epoch: 20 step: 2072, loss is 0.8886451721191406\n",
            "epoch: 20 step: 2073, loss is 1.0836148262023926\n",
            "epoch: 20 step: 2074, loss is 0.922538697719574\n",
            "epoch: 20 step: 2075, loss is 0.8325992822647095\n",
            "epoch: 20 step: 2076, loss is 0.8052033185958862\n",
            "epoch: 20 step: 2077, loss is 0.9481478929519653\n",
            "epoch: 20 step: 2078, loss is 0.7935215830802917\n",
            "epoch: 20 step: 2079, loss is 0.8701354265213013\n",
            "epoch: 20 step: 2080, loss is 0.864612340927124\n",
            "epoch: 20 step: 2081, loss is 0.8151869773864746\n",
            "epoch: 20 step: 2082, loss is 0.7509641051292419\n",
            "epoch: 20 step: 2083, loss is 0.8133453726768494\n",
            "epoch: 20 step: 2084, loss is 0.8355054259300232\n",
            "epoch: 20 step: 2085, loss is 0.9198155403137207\n",
            "epoch: 20 step: 2086, loss is 0.7566765546798706\n",
            "epoch: 20 step: 2087, loss is 0.9935117363929749\n",
            "epoch: 20 step: 2088, loss is 0.9159311652183533\n",
            "epoch: 20 step: 2089, loss is 0.8425768613815308\n",
            "epoch: 20 step: 2090, loss is 0.9077759981155396\n",
            "epoch: 20 step: 2091, loss is 0.9750855565071106\n",
            "epoch: 20 step: 2092, loss is 0.976465106010437\n",
            "epoch: 20 step: 2093, loss is 0.8013924360275269\n",
            "epoch: 20 step: 2094, loss is 0.9167510271072388\n",
            "epoch: 20 step: 2095, loss is 0.776667058467865\n",
            "epoch: 20 step: 2096, loss is 0.6993902921676636\n",
            "epoch: 20 step: 2097, loss is 0.8653446435928345\n",
            "epoch: 20 step: 2098, loss is 0.8485821485519409\n",
            "epoch: 20 step: 2099, loss is 0.8275153040885925\n",
            "epoch: 20 step: 2100, loss is 0.8544394969940186\n",
            "epoch: 20 step: 2101, loss is 0.7893291711807251\n",
            "epoch: 20 step: 2102, loss is 1.2664942741394043\n",
            "epoch: 20 step: 2103, loss is 0.7004926204681396\n",
            "epoch: 20 step: 2104, loss is 1.0195672512054443\n",
            "epoch: 20 step: 2105, loss is 0.7796924710273743\n",
            "epoch: 20 step: 2106, loss is 0.9489892721176147\n",
            "epoch: 20 step: 2107, loss is 0.6633886098861694\n",
            "epoch: 20 step: 2108, loss is 0.9878571033477783\n",
            "epoch: 20 step: 2109, loss is 1.0048534870147705\n",
            "epoch: 20 step: 2110, loss is 0.6711251735687256\n",
            "epoch: 20 step: 2111, loss is 0.9591029286384583\n",
            "epoch: 20 step: 2112, loss is 0.8992124199867249\n",
            "epoch: 20 step: 2113, loss is 1.0295974016189575\n",
            "epoch: 20 step: 2114, loss is 0.8787601590156555\n",
            "epoch: 20 step: 2115, loss is 0.6778128743171692\n",
            "epoch: 20 step: 2116, loss is 0.9085767865180969\n",
            "epoch: 20 step: 2117, loss is 0.9124577641487122\n",
            "epoch: 20 step: 2118, loss is 0.9522972702980042\n",
            "epoch: 20 step: 2119, loss is 0.9622378945350647\n",
            "epoch: 20 step: 2120, loss is 1.006038784980774\n",
            "epoch: 20 step: 2121, loss is 1.0280994176864624\n",
            "epoch: 20 step: 2122, loss is 0.7437151670455933\n",
            "epoch: 20 step: 2123, loss is 0.8453652858734131\n",
            "epoch: 20 step: 2124, loss is 1.0317487716674805\n",
            "epoch: 20 step: 2125, loss is 0.9616700410842896\n",
            "epoch: 20 step: 2126, loss is 0.8490511178970337\n",
            "epoch: 20 step: 2127, loss is 0.9954473972320557\n",
            "epoch: 20 step: 2128, loss is 0.9465908408164978\n",
            "epoch: 20 step: 2129, loss is 0.9295741319656372\n",
            "epoch: 20 step: 2130, loss is 0.7578660845756531\n",
            "epoch: 20 step: 2131, loss is 0.9308750033378601\n",
            "epoch: 20 step: 2132, loss is 0.7857494950294495\n",
            "epoch: 20 step: 2133, loss is 0.995893657207489\n",
            "epoch: 20 step: 2134, loss is 0.8464398980140686\n",
            "epoch: 20 step: 2135, loss is 0.7976230382919312\n",
            "epoch: 20 step: 2136, loss is 0.9589290022850037\n",
            "epoch: 20 step: 2137, loss is 0.9291810989379883\n",
            "epoch: 20 step: 2138, loss is 0.8886557817459106\n",
            "epoch: 20 step: 2139, loss is 1.059560775756836\n",
            "epoch: 20 step: 2140, loss is 0.9986844658851624\n",
            "epoch: 20 step: 2141, loss is 0.9649050235748291\n",
            "epoch: 20 step: 2142, loss is 0.9115716218948364\n",
            "epoch: 20 step: 2143, loss is 0.8505722284317017\n",
            "epoch: 20 step: 2144, loss is 0.8666566014289856\n",
            "epoch: 20 step: 2145, loss is 1.0628302097320557\n",
            "epoch: 20 step: 2146, loss is 0.7028178572654724\n",
            "epoch: 20 step: 2147, loss is 0.829024612903595\n",
            "epoch: 20 step: 2148, loss is 1.027016520500183\n",
            "epoch: 20 step: 2149, loss is 0.9778399467468262\n",
            "epoch: 20 step: 2150, loss is 0.9356606006622314\n",
            "epoch: 20 step: 2151, loss is 0.8605709075927734\n",
            "epoch: 20 step: 2152, loss is 0.8578124046325684\n",
            "epoch: 20 step: 2153, loss is 0.8987554907798767\n",
            "epoch: 20 step: 2154, loss is 0.8578351140022278\n",
            "epoch: 20 step: 2155, loss is 0.8805722594261169\n",
            "epoch: 20 step: 2156, loss is 0.8541765213012695\n",
            "epoch: 20 step: 2157, loss is 1.1158698797225952\n",
            "epoch: 20 step: 2158, loss is 0.8129667043685913\n",
            "epoch: 20 step: 2159, loss is 0.9098466038703918\n",
            "epoch: 20 step: 2160, loss is 0.8269052505493164\n",
            "epoch: 20 step: 2161, loss is 0.8689541220664978\n",
            "epoch: 20 step: 2162, loss is 0.7956812977790833\n",
            "epoch: 20 step: 2163, loss is 1.0052388906478882\n",
            "epoch: 20 step: 2164, loss is 0.8708579540252686\n",
            "epoch: 20 step: 2165, loss is 0.8843932151794434\n",
            "epoch: 20 step: 2166, loss is 0.8650823831558228\n",
            "epoch: 20 step: 2167, loss is 0.9499115943908691\n",
            "epoch: 20 step: 2168, loss is 0.9886656403541565\n",
            "epoch: 20 step: 2169, loss is 0.721087634563446\n",
            "epoch: 20 step: 2170, loss is 1.0007672309875488\n",
            "epoch: 20 step: 2171, loss is 0.8010786771774292\n",
            "epoch: 20 step: 2172, loss is 0.796291172504425\n",
            "epoch: 20 step: 2173, loss is 0.8285544514656067\n",
            "epoch: 20 step: 2174, loss is 0.9397286176681519\n",
            "epoch: 20 step: 2175, loss is 0.7428972125053406\n",
            "epoch: 20 step: 2176, loss is 0.9658950567245483\n",
            "epoch: 20 step: 2177, loss is 0.8193499445915222\n",
            "epoch: 20 step: 2178, loss is 0.8165745139122009\n",
            "epoch: 20 step: 2179, loss is 0.8666919469833374\n",
            "epoch: 20 step: 2180, loss is 0.8492725491523743\n",
            "epoch: 20 step: 2181, loss is 0.9119149446487427\n",
            "epoch: 20 step: 2182, loss is 0.8143318891525269\n",
            "epoch: 20 step: 2183, loss is 1.0137245655059814\n",
            "epoch: 20 step: 2184, loss is 0.9331538677215576\n",
            "epoch: 20 step: 2185, loss is 1.0790445804595947\n",
            "epoch: 20 step: 2186, loss is 0.879548966884613\n",
            "epoch: 20 step: 2187, loss is 0.8032997250556946\n",
            "epoch: 20 step: 2188, loss is 0.9338120222091675\n",
            "epoch: 20 step: 2189, loss is 0.9248413443565369\n",
            "epoch: 20 step: 2190, loss is 0.7203448414802551\n",
            "epoch: 20 step: 2191, loss is 0.8639906644821167\n",
            "epoch: 20 step: 2192, loss is 0.8992806673049927\n",
            "epoch: 20 step: 2193, loss is 0.7200067639350891\n",
            "epoch: 20 step: 2194, loss is 1.0261231660842896\n",
            "epoch: 20 step: 2195, loss is 0.8450518846511841\n",
            "epoch: 20 step: 2196, loss is 1.0552682876586914\n",
            "epoch: 20 step: 2197, loss is 0.7397855520248413\n",
            "epoch: 20 step: 2198, loss is 0.8802829384803772\n",
            "epoch: 20 step: 2199, loss is 0.8951325416564941\n",
            "epoch: 20 step: 2200, loss is 0.7845833897590637\n",
            "epoch: 20 step: 2201, loss is 0.9716200828552246\n",
            "epoch: 20 step: 2202, loss is 1.0209754705429077\n",
            "epoch: 20 step: 2203, loss is 0.8586218953132629\n",
            "epoch: 20 step: 2204, loss is 0.8615037798881531\n",
            "epoch: 20 step: 2205, loss is 0.8996233940124512\n",
            "epoch: 20 step: 2206, loss is 0.9440945982933044\n",
            "epoch: 20 step: 2207, loss is 0.8203034996986389\n",
            "epoch: 20 step: 2208, loss is 0.8528270125389099\n",
            "epoch: 20 step: 2209, loss is 1.0858696699142456\n",
            "epoch: 20 step: 2210, loss is 0.7903209328651428\n",
            "epoch: 20 step: 2211, loss is 0.9086970090866089\n",
            "epoch: 20 step: 2212, loss is 0.821648120880127\n",
            "epoch: 20 step: 2213, loss is 1.0907008647918701\n",
            "epoch: 20 step: 2214, loss is 0.9100349545478821\n",
            "epoch: 20 step: 2215, loss is 0.8420597314834595\n",
            "epoch: 20 step: 2216, loss is 0.6997045874595642\n",
            "epoch: 20 step: 2217, loss is 1.091009259223938\n",
            "epoch: 20 step: 2218, loss is 0.9630986452102661\n",
            "epoch: 20 step: 2219, loss is 0.8149631023406982\n",
            "epoch: 20 step: 2220, loss is 0.9005491137504578\n",
            "epoch: 20 step: 2221, loss is 0.9480645060539246\n",
            "epoch: 20 step: 2222, loss is 0.8626675605773926\n",
            "epoch: 20 step: 2223, loss is 0.9356633424758911\n",
            "epoch: 20 step: 2224, loss is 0.9194302558898926\n",
            "epoch: 20 step: 2225, loss is 0.9963453412055969\n",
            "epoch: 20 step: 2226, loss is 1.1134223937988281\n",
            "epoch: 20 step: 2227, loss is 0.9214595556259155\n",
            "epoch: 20 step: 2228, loss is 0.9210796356201172\n",
            "epoch: 20 step: 2229, loss is 0.9083060622215271\n",
            "epoch: 20 step: 2230, loss is 0.8220362067222595\n",
            "epoch: 20 step: 2231, loss is 0.8116362690925598\n",
            "epoch: 20 step: 2232, loss is 0.8680245876312256\n",
            "epoch: 20 step: 2233, loss is 1.0037764310836792\n",
            "epoch: 20 step: 2234, loss is 0.733356237411499\n",
            "epoch: 20 step: 2235, loss is 1.0603556632995605\n",
            "epoch: 20 step: 2236, loss is 0.9616710543632507\n",
            "epoch: 20 step: 2237, loss is 0.7624539136886597\n",
            "epoch: 20 step: 2238, loss is 0.8861708045005798\n",
            "epoch: 20 step: 2239, loss is 0.889782726764679\n",
            "epoch: 20 step: 2240, loss is 1.0135725736618042\n",
            "epoch: 20 step: 2241, loss is 0.8957808017730713\n",
            "epoch: 20 step: 2242, loss is 0.8414373397827148\n",
            "epoch: 20 step: 2243, loss is 0.7586078643798828\n",
            "epoch: 20 step: 2244, loss is 0.9947552680969238\n",
            "epoch: 20 step: 2245, loss is 0.9209861755371094\n",
            "epoch: 20 step: 2246, loss is 0.8380975127220154\n",
            "epoch: 20 step: 2247, loss is 0.855103611946106\n",
            "epoch: 20 step: 2248, loss is 0.7423596382141113\n",
            "epoch: 20 step: 2249, loss is 0.9690581560134888\n",
            "epoch: 20 step: 2250, loss is 0.9444871544837952\n",
            "epoch: 20 step: 2251, loss is 0.6486556529998779\n",
            "epoch: 20 step: 2252, loss is 0.9314787983894348\n",
            "epoch: 20 step: 2253, loss is 1.0351293087005615\n",
            "epoch: 20 step: 2254, loss is 0.9315410256385803\n",
            "epoch: 20 step: 2255, loss is 0.9015653133392334\n",
            "epoch: 20 step: 2256, loss is 0.8516615033149719\n",
            "epoch: 20 step: 2257, loss is 0.848134458065033\n",
            "epoch: 20 step: 2258, loss is 0.9801381230354309\n",
            "epoch: 20 step: 2259, loss is 0.8969250321388245\n",
            "epoch: 20 step: 2260, loss is 0.8459288477897644\n",
            "epoch: 20 step: 2261, loss is 0.8843648433685303\n",
            "epoch: 20 step: 2262, loss is 0.758313775062561\n",
            "epoch: 20 step: 2263, loss is 0.8859178423881531\n",
            "epoch: 20 step: 2264, loss is 0.911363959312439\n",
            "epoch: 20 step: 2265, loss is 0.7564747929573059\n",
            "epoch: 20 step: 2266, loss is 0.8611603379249573\n",
            "epoch: 20 step: 2267, loss is 0.862348198890686\n",
            "epoch: 20 step: 2268, loss is 0.7904409766197205\n",
            "epoch: 20 step: 2269, loss is 1.01530921459198\n",
            "epoch: 20 step: 2270, loss is 0.8728162050247192\n",
            "epoch: 20 step: 2271, loss is 0.9330728054046631\n",
            "epoch: 20 step: 2272, loss is 0.8758021593093872\n",
            "epoch: 20 step: 2273, loss is 0.9543622732162476\n",
            "epoch: 20 step: 2274, loss is 0.8340229392051697\n",
            "epoch: 20 step: 2275, loss is 0.9621921181678772\n",
            "epoch: 20 step: 2276, loss is 1.141018033027649\n",
            "epoch: 20 step: 2277, loss is 0.8346844911575317\n",
            "epoch: 20 step: 2278, loss is 0.8648796081542969\n",
            "epoch: 20 step: 2279, loss is 0.9926437735557556\n",
            "epoch: 20 step: 2280, loss is 0.9439185857772827\n",
            "epoch: 20 step: 2281, loss is 0.8893631100654602\n",
            "epoch: 20 step: 2282, loss is 0.8933068513870239\n",
            "epoch: 20 step: 2283, loss is 0.8584650158882141\n",
            "epoch: 20 step: 2284, loss is 0.7237067818641663\n",
            "epoch: 20 step: 2285, loss is 0.8727779984474182\n",
            "epoch: 20 step: 2286, loss is 0.810275673866272\n",
            "epoch: 20 step: 2287, loss is 0.7775703072547913\n",
            "epoch: 20 step: 2288, loss is 0.9516294598579407\n",
            "epoch: 20 step: 2289, loss is 0.7824758291244507\n",
            "epoch: 20 step: 2290, loss is 0.8477822542190552\n",
            "epoch: 20 step: 2291, loss is 0.8224478363990784\n",
            "epoch: 20 step: 2292, loss is 1.1653547286987305\n",
            "epoch: 20 step: 2293, loss is 1.1262333393096924\n",
            "epoch: 20 step: 2294, loss is 0.7543943524360657\n",
            "epoch: 20 step: 2295, loss is 0.8997280597686768\n",
            "epoch: 20 step: 2296, loss is 0.9930651187896729\n",
            "epoch: 20 step: 2297, loss is 0.929718554019928\n",
            "epoch: 20 step: 2298, loss is 1.1399459838867188\n",
            "epoch: 20 step: 2299, loss is 1.0739549398422241\n",
            "epoch: 20 step: 2300, loss is 1.0375471115112305\n",
            "epoch: 20 step: 2301, loss is 0.6881586909294128\n",
            "epoch: 20 step: 2302, loss is 0.738121747970581\n",
            "epoch: 20 step: 2303, loss is 0.8243841528892517\n",
            "epoch: 20 step: 2304, loss is 0.9426892399787903\n",
            "epoch: 20 step: 2305, loss is 0.7547920942306519\n",
            "epoch: 20 step: 2306, loss is 0.8988980650901794\n",
            "epoch: 20 step: 2307, loss is 0.7801657319068909\n",
            "epoch: 20 step: 2308, loss is 0.7768248319625854\n",
            "epoch: 20 step: 2309, loss is 0.8200775980949402\n",
            "epoch: 20 step: 2310, loss is 1.045798420906067\n",
            "epoch: 20 step: 2311, loss is 0.8290539383888245\n",
            "epoch: 20 step: 2312, loss is 0.8902639150619507\n",
            "epoch: 20 step: 2313, loss is 0.8668794631958008\n",
            "epoch: 20 step: 2314, loss is 0.9056479930877686\n",
            "epoch: 20 step: 2315, loss is 0.8424477577209473\n",
            "epoch: 20 step: 2316, loss is 1.0256067514419556\n",
            "epoch: 20 step: 2317, loss is 0.8648401498794556\n",
            "epoch: 20 step: 2318, loss is 0.7670102715492249\n",
            "epoch: 20 step: 2319, loss is 0.8391870260238647\n",
            "epoch: 20 step: 2320, loss is 0.9382627010345459\n",
            "epoch: 20 step: 2321, loss is 0.9482059478759766\n",
            "epoch: 20 step: 2322, loss is 0.9402512311935425\n",
            "epoch: 20 step: 2323, loss is 0.8106352686882019\n",
            "epoch: 20 step: 2324, loss is 0.9202538728713989\n",
            "epoch: 20 step: 2325, loss is 0.8209526538848877\n",
            "epoch: 20 step: 2326, loss is 0.9166123270988464\n",
            "epoch: 20 step: 2327, loss is 0.9595404267311096\n",
            "epoch: 20 step: 2328, loss is 0.915408730506897\n",
            "epoch: 20 step: 2329, loss is 0.9127392768859863\n",
            "epoch: 20 step: 2330, loss is 0.807192325592041\n",
            "epoch: 20 step: 2331, loss is 0.7875048518180847\n",
            "epoch: 20 step: 2332, loss is 0.9333442449569702\n",
            "epoch: 20 step: 2333, loss is 0.7683994174003601\n",
            "epoch: 20 step: 2334, loss is 1.0110357999801636\n",
            "epoch: 20 step: 2335, loss is 1.0272811651229858\n",
            "epoch: 20 step: 2336, loss is 0.8754433989524841\n",
            "epoch: 20 step: 2337, loss is 0.8076477646827698\n",
            "epoch: 20 step: 2338, loss is 0.9327894449234009\n",
            "epoch: 20 step: 2339, loss is 0.6354613900184631\n",
            "epoch: 20 step: 2340, loss is 0.9817641377449036\n",
            "epoch: 20 step: 2341, loss is 1.0248793363571167\n",
            "epoch: 20 step: 2342, loss is 0.9806264638900757\n",
            "epoch: 20 step: 2343, loss is 0.7032136917114258\n",
            "epoch: 20 step: 2344, loss is 0.849507749080658\n",
            "epoch: 20 step: 2345, loss is 0.8872909545898438\n",
            "epoch: 20 step: 2346, loss is 0.9157158732414246\n",
            "epoch: 20 step: 2347, loss is 1.1226173639297485\n",
            "epoch: 20 step: 2348, loss is 0.9844956398010254\n",
            "epoch: 20 step: 2349, loss is 0.9607895016670227\n",
            "epoch: 20 step: 2350, loss is 1.1098829507827759\n",
            "epoch: 20 step: 2351, loss is 0.7529420256614685\n",
            "epoch: 20 step: 2352, loss is 0.7716907262802124\n",
            "epoch: 20 step: 2353, loss is 0.9007623791694641\n",
            "epoch: 20 step: 2354, loss is 0.8641083836555481\n",
            "epoch: 20 step: 2355, loss is 0.9604673981666565\n",
            "epoch: 20 step: 2356, loss is 0.9091971516609192\n",
            "epoch: 20 step: 2357, loss is 0.8977010250091553\n",
            "epoch: 20 step: 2358, loss is 0.7595462203025818\n",
            "epoch: 20 step: 2359, loss is 0.9835187792778015\n",
            "epoch: 20 step: 2360, loss is 0.7842239141464233\n",
            "epoch: 20 step: 2361, loss is 1.02130925655365\n",
            "epoch: 20 step: 2362, loss is 0.9796338677406311\n",
            "epoch: 20 step: 2363, loss is 1.0453338623046875\n",
            "epoch: 20 step: 2364, loss is 0.8524272441864014\n",
            "epoch: 20 step: 2365, loss is 1.0432394742965698\n",
            "epoch: 20 step: 2366, loss is 0.8180273175239563\n",
            "epoch: 20 step: 2367, loss is 0.7626663446426392\n",
            "epoch: 20 step: 2368, loss is 1.0768033266067505\n",
            "epoch: 20 step: 2369, loss is 0.7759991884231567\n",
            "epoch: 20 step: 2370, loss is 1.051257848739624\n",
            "epoch: 20 step: 2371, loss is 0.6958391070365906\n",
            "epoch: 20 step: 2372, loss is 0.7963368892669678\n",
            "epoch: 20 step: 2373, loss is 0.9420018196105957\n",
            "epoch: 20 step: 2374, loss is 0.8822084069252014\n",
            "epoch: 20 step: 2375, loss is 0.8105872273445129\n",
            "epoch: 20 step: 2376, loss is 0.840167224407196\n",
            "epoch: 20 step: 2377, loss is 0.8692308664321899\n",
            "epoch: 20 step: 2378, loss is 0.6736295819282532\n",
            "epoch: 20 step: 2379, loss is 0.8604845404624939\n",
            "epoch: 20 step: 2380, loss is 0.8650943636894226\n",
            "epoch: 20 step: 2381, loss is 0.8656907677650452\n",
            "epoch: 20 step: 2382, loss is 0.8569024205207825\n",
            "epoch: 20 step: 2383, loss is 0.9744158387184143\n",
            "epoch: 20 step: 2384, loss is 1.0179508924484253\n",
            "epoch: 20 step: 2385, loss is 0.9800013899803162\n",
            "epoch: 20 step: 2386, loss is 0.9538211822509766\n",
            "epoch: 20 step: 2387, loss is 0.8380765914916992\n",
            "epoch: 20 step: 2388, loss is 0.7597799897193909\n",
            "epoch: 20 step: 2389, loss is 1.0004960298538208\n",
            "epoch: 20 step: 2390, loss is 0.7664738893508911\n",
            "epoch: 20 step: 2391, loss is 0.947622537612915\n",
            "epoch: 20 step: 2392, loss is 0.8110906481742859\n",
            "epoch: 20 step: 2393, loss is 0.9151147603988647\n",
            "epoch: 20 step: 2394, loss is 0.9346893429756165\n",
            "epoch: 20 step: 2395, loss is 0.8927879333496094\n",
            "epoch: 20 step: 2396, loss is 0.9750580787658691\n",
            "epoch: 20 step: 2397, loss is 0.7719403505325317\n",
            "epoch: 20 step: 2398, loss is 0.8136129975318909\n",
            "epoch: 20 step: 2399, loss is 0.7887450456619263\n",
            "epoch: 20 step: 2400, loss is 0.8999226689338684\n",
            "epoch: 20 step: 2401, loss is 0.8344637751579285\n",
            "epoch: 20 step: 2402, loss is 0.8165943026542664\n",
            "epoch: 20 step: 2403, loss is 0.9073723554611206\n",
            "epoch: 20 step: 2404, loss is 0.9287728667259216\n",
            "epoch: 20 step: 2405, loss is 0.8397420048713684\n",
            "epoch: 20 step: 2406, loss is 0.9358580708503723\n",
            "epoch: 20 step: 2407, loss is 1.0112138986587524\n",
            "epoch: 20 step: 2408, loss is 0.9039512872695923\n",
            "epoch: 20 step: 2409, loss is 0.9173220992088318\n",
            "epoch: 20 step: 2410, loss is 0.6107164621353149\n",
            "epoch: 20 step: 2411, loss is 0.9017623662948608\n",
            "epoch: 20 step: 2412, loss is 0.917493462562561\n",
            "epoch: 20 step: 2413, loss is 1.062657117843628\n",
            "epoch: 20 step: 2414, loss is 0.7752104997634888\n",
            "epoch: 20 step: 2415, loss is 0.9481829404830933\n",
            "epoch: 20 step: 2416, loss is 0.8957302570343018\n",
            "epoch: 20 step: 2417, loss is 0.8629581332206726\n",
            "epoch: 20 step: 2418, loss is 0.956468939781189\n",
            "epoch: 20 step: 2419, loss is 0.9498493075370789\n",
            "epoch: 20 step: 2420, loss is 0.9857870936393738\n",
            "epoch: 20 step: 2421, loss is 0.8824078440666199\n",
            "epoch: 20 step: 2422, loss is 1.1105707883834839\n",
            "epoch: 20 step: 2423, loss is 0.737794041633606\n",
            "epoch: 20 step: 2424, loss is 0.9340656399726868\n",
            "epoch: 20 step: 2425, loss is 0.9879612922668457\n",
            "epoch: 20 step: 2426, loss is 0.8883341550827026\n",
            "epoch: 20 step: 2427, loss is 1.0432018041610718\n",
            "epoch: 20 step: 2428, loss is 0.9167376160621643\n",
            "epoch: 20 step: 2429, loss is 1.047235131263733\n",
            "epoch: 20 step: 2430, loss is 0.7770811319351196\n",
            "epoch: 20 step: 2431, loss is 0.7028589248657227\n",
            "epoch: 20 step: 2432, loss is 0.8962875008583069\n",
            "epoch: 20 step: 2433, loss is 0.7823817133903503\n",
            "epoch: 20 step: 2434, loss is 1.1157718896865845\n",
            "epoch: 20 step: 2435, loss is 0.8828104138374329\n",
            "epoch: 20 step: 2436, loss is 0.9433354735374451\n",
            "epoch: 20 step: 2437, loss is 0.8787543177604675\n",
            "epoch: 20 step: 2438, loss is 0.771310567855835\n",
            "epoch: 20 step: 2439, loss is 0.987635612487793\n",
            "epoch: 20 step: 2440, loss is 0.9351648688316345\n",
            "epoch: 20 step: 2441, loss is 0.9292391538619995\n",
            "epoch: 20 step: 2442, loss is 0.8906197547912598\n",
            "epoch: 20 step: 2443, loss is 0.9320620894432068\n",
            "epoch: 20 step: 2444, loss is 0.9812316298484802\n",
            "epoch: 20 step: 2445, loss is 0.8736110925674438\n",
            "epoch: 20 step: 2446, loss is 0.8717157244682312\n",
            "epoch: 20 step: 2447, loss is 0.9353492856025696\n",
            "epoch: 20 step: 2448, loss is 0.9588286280632019\n",
            "epoch: 20 step: 2449, loss is 0.8924009799957275\n",
            "epoch: 20 step: 2450, loss is 0.9868146777153015\n",
            "epoch: 20 step: 2451, loss is 0.9884636998176575\n",
            "epoch: 20 step: 2452, loss is 0.9128552675247192\n",
            "epoch: 20 step: 2453, loss is 0.9212549924850464\n",
            "epoch: 20 step: 2454, loss is 1.0645601749420166\n",
            "epoch: 20 step: 2455, loss is 1.1995768547058105\n",
            "epoch: 20 step: 2456, loss is 0.74543297290802\n",
            "epoch: 20 step: 2457, loss is 0.9334548115730286\n",
            "epoch: 20 step: 2458, loss is 0.8156964182853699\n",
            "epoch: 20 step: 2459, loss is 0.8196279406547546\n",
            "epoch: 20 step: 2460, loss is 0.9280348420143127\n",
            "epoch: 20 step: 2461, loss is 0.8270719647407532\n",
            "epoch: 20 step: 2462, loss is 0.9495202302932739\n",
            "epoch: 20 step: 2463, loss is 0.7849394083023071\n",
            "epoch: 20 step: 2464, loss is 0.9722328782081604\n",
            "epoch: 20 step: 2465, loss is 0.9512694478034973\n",
            "epoch: 20 step: 2466, loss is 0.7162289023399353\n",
            "epoch: 20 step: 2467, loss is 0.8773075938224792\n",
            "epoch: 20 step: 2468, loss is 1.0647093057632446\n",
            "epoch: 20 step: 2469, loss is 1.0945950746536255\n",
            "epoch: 20 step: 2470, loss is 1.065523624420166\n",
            "epoch: 20 step: 2471, loss is 1.0529636144638062\n",
            "epoch: 20 step: 2472, loss is 0.7673775553703308\n",
            "epoch: 20 step: 2473, loss is 0.9402744174003601\n",
            "epoch: 20 step: 2474, loss is 0.9505458474159241\n",
            "epoch: 20 step: 2475, loss is 0.8219482898712158\n",
            "epoch: 20 step: 2476, loss is 1.0828341245651245\n",
            "epoch: 20 step: 2477, loss is 1.013282060623169\n",
            "epoch: 20 step: 2478, loss is 0.8713969588279724\n",
            "epoch: 20 step: 2479, loss is 1.1529982089996338\n",
            "epoch: 20 step: 2480, loss is 0.8658039569854736\n",
            "epoch: 20 step: 2481, loss is 0.7515220046043396\n",
            "epoch: 20 step: 2482, loss is 0.9488034844398499\n",
            "epoch: 20 step: 2483, loss is 0.9041650891304016\n",
            "epoch: 20 step: 2484, loss is 0.8228596448898315\n",
            "epoch: 20 step: 2485, loss is 0.7628973722457886\n",
            "epoch: 20 step: 2486, loss is 0.8634611964225769\n",
            "epoch: 20 step: 2487, loss is 0.8831396102905273\n",
            "epoch: 20 step: 2488, loss is 0.9716250896453857\n",
            "epoch: 20 step: 2489, loss is 0.973530650138855\n",
            "epoch: 20 step: 2490, loss is 1.0482540130615234\n",
            "epoch: 20 step: 2491, loss is 0.8126457333564758\n",
            "epoch: 20 step: 2492, loss is 1.0258710384368896\n",
            "epoch: 20 step: 2493, loss is 0.8247005343437195\n",
            "epoch: 20 step: 2494, loss is 0.8973786234855652\n",
            "epoch: 20 step: 2495, loss is 1.1629979610443115\n",
            "epoch: 20 step: 2496, loss is 1.062083125114441\n",
            "epoch: 20 step: 2497, loss is 1.0720006227493286\n",
            "epoch: 20 step: 2498, loss is 0.8786646127700806\n",
            "epoch: 20 step: 2499, loss is 0.9596462845802307\n",
            "epoch: 20 step: 2500, loss is 0.8216094374656677\n",
            "epoch: 20 step: 2501, loss is 0.8780145049095154\n",
            "epoch: 20 step: 2502, loss is 0.698194682598114\n",
            "epoch: 20 step: 2503, loss is 0.9827591776847839\n",
            "epoch: 20 step: 2504, loss is 0.9533334970474243\n",
            "epoch: 20 step: 2505, loss is 0.843015193939209\n",
            "epoch: 20 step: 2506, loss is 0.9393261075019836\n",
            "epoch: 20 step: 2507, loss is 1.15024733543396\n",
            "epoch: 20 step: 2508, loss is 0.6285452842712402\n",
            "epoch: 20 step: 2509, loss is 0.9179487824440002\n",
            "epoch: 20 step: 2510, loss is 0.9686667323112488\n",
            "epoch: 20 step: 2511, loss is 0.7693424820899963\n",
            "epoch: 20 step: 2512, loss is 0.8980240225791931\n",
            "epoch: 20 step: 2513, loss is 0.8148159980773926\n",
            "epoch: 20 step: 2514, loss is 0.9143247008323669\n",
            "epoch: 20 step: 2515, loss is 1.0197834968566895\n",
            "epoch: 20 step: 2516, loss is 0.9391472935676575\n",
            "epoch: 20 step: 2517, loss is 0.9317931532859802\n",
            "epoch: 20 step: 2518, loss is 0.7287229299545288\n",
            "epoch: 20 step: 2519, loss is 0.9655025005340576\n",
            "epoch: 20 step: 2520, loss is 0.8624854683876038\n",
            "epoch: 20 step: 2521, loss is 0.7045186161994934\n",
            "epoch: 20 step: 2522, loss is 0.8704277276992798\n",
            "epoch: 20 step: 2523, loss is 0.8942529559135437\n",
            "epoch: 20 step: 2524, loss is 0.8749276399612427\n",
            "epoch: 20 step: 2525, loss is 0.9216601848602295\n",
            "epoch: 20 step: 2526, loss is 0.8936925530433655\n",
            "epoch: 20 step: 2527, loss is 0.9559962153434753\n",
            "epoch: 20 step: 2528, loss is 0.7639285326004028\n",
            "epoch: 20 step: 2529, loss is 0.9703428745269775\n",
            "epoch: 20 step: 2530, loss is 0.8691820502281189\n",
            "epoch: 20 step: 2531, loss is 0.7917294502258301\n",
            "epoch: 20 step: 2532, loss is 0.8640302419662476\n",
            "epoch: 20 step: 2533, loss is 0.8401143550872803\n",
            "epoch: 20 step: 2534, loss is 0.7298534512519836\n",
            "epoch: 20 step: 2535, loss is 0.8907901644706726\n",
            "epoch: 20 step: 2536, loss is 0.8557090163230896\n",
            "epoch: 20 step: 2537, loss is 0.76109379529953\n",
            "epoch: 20 step: 2538, loss is 0.9750166535377502\n",
            "epoch: 20 step: 2539, loss is 0.7805492877960205\n",
            "epoch: 20 step: 2540, loss is 0.8987327218055725\n",
            "epoch: 20 step: 2541, loss is 0.8204125761985779\n",
            "epoch: 20 step: 2542, loss is 0.8090413808822632\n",
            "epoch: 20 step: 2543, loss is 0.9026964902877808\n",
            "epoch: 20 step: 2544, loss is 0.8404417634010315\n",
            "epoch: 20 step: 2545, loss is 0.9045738577842712\n",
            "epoch: 20 step: 2546, loss is 0.9463353157043457\n",
            "epoch: 20 step: 2547, loss is 1.0494964122772217\n",
            "epoch: 20 step: 2548, loss is 0.7618653774261475\n",
            "epoch: 20 step: 2549, loss is 0.9670838713645935\n",
            "epoch: 20 step: 2550, loss is 1.0201536417007446\n",
            "epoch: 20 step: 2551, loss is 0.7415409684181213\n",
            "epoch: 20 step: 2552, loss is 0.8377771377563477\n",
            "epoch: 20 step: 2553, loss is 0.767231285572052\n",
            "epoch: 20 step: 2554, loss is 0.8069145679473877\n",
            "epoch: 20 step: 2555, loss is 0.9427067637443542\n",
            "epoch: 20 step: 2556, loss is 0.8819634914398193\n",
            "epoch: 20 step: 2557, loss is 0.9158874750137329\n",
            "epoch: 20 step: 2558, loss is 0.8081250190734863\n",
            "epoch: 20 step: 2559, loss is 0.9312952756881714\n",
            "epoch: 20 step: 2560, loss is 1.0084925889968872\n",
            "epoch: 20 step: 2561, loss is 0.7566128373146057\n",
            "epoch: 20 step: 2562, loss is 1.1828339099884033\n",
            "epoch: 20 step: 2563, loss is 1.1324844360351562\n",
            "epoch: 20 step: 2564, loss is 0.9515289664268494\n",
            "epoch: 20 step: 2565, loss is 0.9642579555511475\n",
            "epoch: 20 step: 2566, loss is 0.9718554019927979\n",
            "epoch: 20 step: 2567, loss is 0.8593959808349609\n",
            "epoch: 20 step: 2568, loss is 0.9010127186775208\n",
            "epoch: 20 step: 2569, loss is 0.8707705140113831\n",
            "epoch: 20 step: 2570, loss is 1.2910082340240479\n",
            "epoch: 20 step: 2571, loss is 0.7480113506317139\n",
            "epoch: 20 step: 2572, loss is 0.7572311162948608\n",
            "epoch: 20 step: 2573, loss is 0.8256644010543823\n",
            "epoch: 20 step: 2574, loss is 0.9824599027633667\n",
            "epoch: 20 step: 2575, loss is 0.7256149649620056\n",
            "epoch: 20 step: 2576, loss is 0.8506515622138977\n",
            "epoch: 20 step: 2577, loss is 0.7749363780021667\n",
            "epoch: 20 step: 2578, loss is 0.9910759925842285\n",
            "epoch: 20 step: 2579, loss is 0.9161157011985779\n",
            "epoch: 20 step: 2580, loss is 0.8495826721191406\n",
            "epoch: 20 step: 2581, loss is 1.0737879276275635\n",
            "epoch: 20 step: 2582, loss is 0.9071304202079773\n",
            "epoch: 20 step: 2583, loss is 0.8804610371589661\n",
            "epoch: 20 step: 2584, loss is 0.7845460176467896\n",
            "epoch: 20 step: 2585, loss is 0.9866101145744324\n",
            "epoch: 20 step: 2586, loss is 0.9350240230560303\n",
            "epoch: 20 step: 2587, loss is 0.9253622889518738\n",
            "epoch: 20 step: 2588, loss is 0.8957772850990295\n",
            "epoch: 20 step: 2589, loss is 0.9959208965301514\n",
            "epoch: 20 step: 2590, loss is 1.0420633554458618\n",
            "epoch: 20 step: 2591, loss is 0.914928138256073\n",
            "epoch: 20 step: 2592, loss is 0.9885013103485107\n",
            "epoch: 20 step: 2593, loss is 0.9850114583969116\n",
            "epoch: 20 step: 2594, loss is 0.8199204206466675\n",
            "epoch: 20 step: 2595, loss is 0.8344609141349792\n",
            "epoch: 20 step: 2596, loss is 0.9966962933540344\n",
            "epoch: 20 step: 2597, loss is 0.8671616911888123\n",
            "epoch: 20 step: 2598, loss is 0.9102501273155212\n",
            "epoch: 20 step: 2599, loss is 0.8833062648773193\n",
            "epoch: 20 step: 2600, loss is 1.0252671241760254\n",
            "epoch: 20 step: 2601, loss is 1.0145944356918335\n",
            "epoch: 20 step: 2602, loss is 0.8883817791938782\n",
            "epoch: 20 step: 2603, loss is 0.8898051977157593\n",
            "epoch: 20 step: 2604, loss is 0.6425853371620178\n",
            "epoch: 20 step: 2605, loss is 0.9726487994194031\n",
            "epoch: 20 step: 2606, loss is 0.9180890917778015\n",
            "epoch: 20 step: 2607, loss is 0.932707667350769\n",
            "epoch: 20 step: 2608, loss is 0.8961067795753479\n",
            "epoch: 20 step: 2609, loss is 0.8798216581344604\n",
            "epoch: 20 step: 2610, loss is 0.7610103487968445\n",
            "epoch: 20 step: 2611, loss is 0.7967438101768494\n",
            "epoch: 20 step: 2612, loss is 0.7389474511146545\n",
            "epoch: 20 step: 2613, loss is 0.9248527884483337\n",
            "epoch: 20 step: 2614, loss is 0.7212833166122437\n",
            "epoch: 20 step: 2615, loss is 0.9471930265426636\n",
            "epoch: 20 step: 2616, loss is 0.8737668395042419\n",
            "epoch: 20 step: 2617, loss is 0.947680652141571\n",
            "epoch: 20 step: 2618, loss is 0.8748273849487305\n",
            "epoch: 20 step: 2619, loss is 0.9597529172897339\n",
            "epoch: 20 step: 2620, loss is 0.9914746284484863\n",
            "epoch: 20 step: 2621, loss is 0.9183774590492249\n",
            "epoch: 20 step: 2622, loss is 0.7794733643531799\n",
            "epoch: 20 step: 2623, loss is 0.9080519676208496\n",
            "epoch: 20 step: 2624, loss is 0.8651500940322876\n",
            "epoch: 20 step: 2625, loss is 0.8866322636604309\n",
            "epoch: 20 step: 2626, loss is 1.0445654392242432\n",
            "epoch: 20 step: 2627, loss is 0.9393072724342346\n",
            "epoch: 20 step: 2628, loss is 0.9873524308204651\n",
            "epoch: 20 step: 2629, loss is 0.8252319693565369\n",
            "epoch: 20 step: 2630, loss is 1.0032554864883423\n",
            "epoch: 20 step: 2631, loss is 0.8594599366188049\n",
            "epoch: 20 step: 2632, loss is 0.9732081294059753\n",
            "epoch: 20 step: 2633, loss is 0.887681245803833\n",
            "epoch: 20 step: 2634, loss is 0.9263059496879578\n",
            "epoch: 20 step: 2635, loss is 0.9953628182411194\n",
            "epoch: 20 step: 2636, loss is 0.9009862542152405\n",
            "epoch: 20 step: 2637, loss is 0.9915726184844971\n",
            "epoch: 20 step: 2638, loss is 0.7504931688308716\n",
            "epoch: 20 step: 2639, loss is 0.7450645565986633\n",
            "epoch: 20 step: 2640, loss is 0.9456704258918762\n",
            "epoch: 20 step: 2641, loss is 1.0005466938018799\n",
            "epoch: 20 step: 2642, loss is 0.7647178173065186\n",
            "epoch: 20 step: 2643, loss is 0.7713810801506042\n",
            "epoch: 20 step: 2644, loss is 0.9826881289482117\n",
            "epoch: 20 step: 2645, loss is 0.7827478647232056\n",
            "epoch: 20 step: 2646, loss is 0.8154736757278442\n",
            "epoch: 20 step: 2647, loss is 0.8494583368301392\n",
            "epoch: 20 step: 2648, loss is 0.8750356435775757\n",
            "epoch: 20 step: 2649, loss is 0.9963352084159851\n",
            "epoch: 20 step: 2650, loss is 0.8304150700569153\n",
            "epoch: 20 step: 2651, loss is 1.116702914237976\n",
            "epoch: 20 step: 2652, loss is 0.9317917823791504\n",
            "epoch: 20 step: 2653, loss is 0.8505927920341492\n",
            "epoch: 20 step: 2654, loss is 1.004386305809021\n",
            "epoch: 20 step: 2655, loss is 0.7732977867126465\n",
            "epoch: 20 step: 2656, loss is 0.8752633929252625\n",
            "epoch: 20 step: 2657, loss is 0.7956808805465698\n",
            "epoch: 20 step: 2658, loss is 0.7957231402397156\n",
            "epoch: 20 step: 2659, loss is 0.7775677442550659\n",
            "epoch: 20 step: 2660, loss is 0.9688258767127991\n",
            "epoch: 20 step: 2661, loss is 0.8611449599266052\n",
            "epoch: 20 step: 2662, loss is 1.0484710931777954\n",
            "epoch: 20 step: 2663, loss is 0.990272581577301\n",
            "epoch: 20 step: 2664, loss is 1.0661742687225342\n",
            "epoch: 20 step: 2665, loss is 0.7708081007003784\n",
            "epoch: 20 step: 2666, loss is 0.795915961265564\n",
            "epoch: 20 step: 2667, loss is 0.8295537829399109\n",
            "epoch: 20 step: 2668, loss is 1.0284196138381958\n",
            "epoch: 20 step: 2669, loss is 0.9191408157348633\n",
            "epoch: 20 step: 2670, loss is 0.8298777341842651\n",
            "epoch: 20 step: 2671, loss is 0.9386153817176819\n",
            "epoch: 20 step: 2672, loss is 0.9088455438613892\n",
            "epoch: 20 step: 2673, loss is 0.882102906703949\n",
            "epoch: 20 step: 2674, loss is 0.9651980996131897\n",
            "epoch: 20 step: 2675, loss is 0.8126438856124878\n",
            "epoch: 20 step: 2676, loss is 0.7773378491401672\n",
            "epoch: 20 step: 2677, loss is 0.9567298889160156\n",
            "epoch: 20 step: 2678, loss is 0.7345225811004639\n",
            "epoch: 20 step: 2679, loss is 0.9279024600982666\n",
            "epoch: 20 step: 2680, loss is 0.8664056062698364\n",
            "epoch: 20 step: 2681, loss is 0.8043270111083984\n",
            "epoch: 20 step: 2682, loss is 0.8314337730407715\n",
            "epoch: 20 step: 2683, loss is 0.8798995614051819\n",
            "epoch: 20 step: 2684, loss is 0.8356315493583679\n",
            "epoch: 20 step: 2685, loss is 0.8545099496841431\n",
            "epoch: 20 step: 2686, loss is 0.860175371170044\n",
            "epoch: 20 step: 2687, loss is 0.7673621773719788\n",
            "epoch: 20 step: 2688, loss is 0.8469716906547546\n",
            "epoch: 20 step: 2689, loss is 1.0383936166763306\n",
            "epoch: 20 step: 2690, loss is 0.7529563307762146\n",
            "epoch: 20 step: 2691, loss is 0.9082677960395813\n",
            "epoch: 20 step: 2692, loss is 0.8673750162124634\n",
            "epoch: 20 step: 2693, loss is 1.055131435394287\n",
            "epoch: 20 step: 2694, loss is 0.7740892171859741\n",
            "epoch: 20 step: 2695, loss is 1.0057536363601685\n",
            "epoch: 20 step: 2696, loss is 1.0782737731933594\n",
            "epoch: 20 step: 2697, loss is 0.9080721139907837\n",
            "epoch: 20 step: 2698, loss is 0.9387578964233398\n",
            "epoch: 20 step: 2699, loss is 0.8528082370758057\n",
            "epoch: 20 step: 2700, loss is 0.8338518738746643\n",
            "epoch: 20 step: 2701, loss is 0.9914348721504211\n",
            "epoch: 20 step: 2702, loss is 0.9074852466583252\n",
            "epoch: 20 step: 2703, loss is 0.8891387581825256\n",
            "epoch: 20 step: 2704, loss is 0.992963969707489\n",
            "epoch: 20 step: 2705, loss is 0.7448630928993225\n",
            "epoch: 20 step: 2706, loss is 1.165846824645996\n",
            "epoch: 20 step: 2707, loss is 0.9217955470085144\n",
            "epoch: 20 step: 2708, loss is 0.827882707118988\n",
            "epoch: 20 step: 2709, loss is 0.804070770740509\n",
            "epoch: 20 step: 2710, loss is 0.7171937227249146\n",
            "epoch: 20 step: 2711, loss is 0.6687542200088501\n",
            "epoch: 20 step: 2712, loss is 1.0793417692184448\n",
            "epoch: 20 step: 2713, loss is 0.7483425736427307\n",
            "epoch: 20 step: 2714, loss is 0.7874733805656433\n",
            "epoch: 20 step: 2715, loss is 0.8047263622283936\n",
            "epoch: 20 step: 2716, loss is 1.101776123046875\n",
            "epoch: 20 step: 2717, loss is 0.7532217502593994\n",
            "epoch: 20 step: 2718, loss is 0.9743447303771973\n",
            "epoch: 20 step: 2719, loss is 0.8067179322242737\n",
            "epoch: 20 step: 2720, loss is 0.9468927383422852\n",
            "epoch: 20 step: 2721, loss is 0.8765537738800049\n",
            "epoch: 20 step: 2722, loss is 0.8034005761146545\n",
            "epoch: 20 step: 2723, loss is 0.8461742401123047\n",
            "epoch: 20 step: 2724, loss is 0.8674389123916626\n",
            "epoch: 20 step: 2725, loss is 0.8112766742706299\n",
            "epoch: 20 step: 2726, loss is 0.8904169797897339\n",
            "epoch: 20 step: 2727, loss is 0.9174994230270386\n",
            "epoch: 20 step: 2728, loss is 0.9585585594177246\n",
            "epoch: 20 step: 2729, loss is 1.011223316192627\n",
            "epoch: 20 step: 2730, loss is 0.8456476926803589\n",
            "epoch: 20 step: 2731, loss is 0.8440892696380615\n",
            "epoch: 20 step: 2732, loss is 0.8140019774436951\n",
            "epoch: 20 step: 2733, loss is 0.9179023504257202\n",
            "epoch: 20 step: 2734, loss is 1.0112090110778809\n",
            "epoch: 20 step: 2735, loss is 0.6452056765556335\n",
            "epoch: 20 step: 2736, loss is 1.0799249410629272\n",
            "epoch: 20 step: 2737, loss is 0.8835105299949646\n",
            "epoch: 20 step: 2738, loss is 0.875726580619812\n",
            "epoch: 20 step: 2739, loss is 0.7459585666656494\n",
            "epoch: 20 step: 2740, loss is 0.785437285900116\n",
            "epoch: 20 step: 2741, loss is 1.0793286561965942\n",
            "epoch: 20 step: 2742, loss is 0.7920286655426025\n",
            "epoch: 20 step: 2743, loss is 0.9154327511787415\n",
            "epoch: 20 step: 2744, loss is 0.9676918983459473\n",
            "epoch: 20 step: 2745, loss is 0.6434143781661987\n",
            "epoch: 20 step: 2746, loss is 0.8767423033714294\n",
            "epoch: 20 step: 2747, loss is 0.8341602683067322\n",
            "epoch: 20 step: 2748, loss is 0.979914665222168\n",
            "epoch: 20 step: 2749, loss is 0.9569699168205261\n",
            "epoch: 20 step: 2750, loss is 0.6771040558815002\n",
            "epoch: 20 step: 2751, loss is 0.8958255648612976\n",
            "epoch: 20 step: 2752, loss is 0.8265826106071472\n",
            "epoch: 20 step: 2753, loss is 0.7946391105651855\n",
            "epoch: 20 step: 2754, loss is 0.8833603858947754\n",
            "epoch: 20 step: 2755, loss is 0.9148009419441223\n",
            "epoch: 20 step: 2756, loss is 0.8457191586494446\n",
            "epoch: 20 step: 2757, loss is 1.062192678451538\n",
            "epoch: 20 step: 2758, loss is 0.9313218593597412\n",
            "epoch: 20 step: 2759, loss is 0.7724797129631042\n",
            "epoch: 20 step: 2760, loss is 0.9240295886993408\n",
            "epoch: 20 step: 2761, loss is 1.0084501504898071\n",
            "epoch: 20 step: 2762, loss is 0.8076999187469482\n",
            "epoch: 20 step: 2763, loss is 0.8885694742202759\n",
            "epoch: 20 step: 2764, loss is 0.8775989413261414\n",
            "epoch: 20 step: 2765, loss is 1.06647527217865\n",
            "epoch: 20 step: 2766, loss is 0.9896112084388733\n",
            "epoch: 20 step: 2767, loss is 0.9507322907447815\n",
            "epoch: 20 step: 2768, loss is 0.9220289587974548\n",
            "epoch: 20 step: 2769, loss is 0.9323367476463318\n",
            "epoch: 20 step: 2770, loss is 1.0146796703338623\n",
            "epoch: 20 step: 2771, loss is 0.8430268168449402\n",
            "epoch: 20 step: 2772, loss is 0.7263046503067017\n",
            "epoch: 20 step: 2773, loss is 0.928743302822113\n",
            "epoch: 20 step: 2774, loss is 1.0107744932174683\n",
            "epoch: 20 step: 2775, loss is 1.016794204711914\n",
            "epoch: 20 step: 2776, loss is 0.8540001511573792\n",
            "epoch: 20 step: 2777, loss is 0.9093878269195557\n",
            "epoch: 20 step: 2778, loss is 0.8192322254180908\n",
            "epoch: 20 step: 2779, loss is 0.8584989309310913\n",
            "epoch: 20 step: 2780, loss is 0.843024492263794\n",
            "epoch: 20 step: 2781, loss is 0.9862611889839172\n",
            "epoch: 20 step: 2782, loss is 0.9144384264945984\n",
            "epoch: 20 step: 2783, loss is 0.8949485421180725\n",
            "epoch: 20 step: 2784, loss is 0.9314165115356445\n",
            "epoch: 20 step: 2785, loss is 1.0255879163742065\n",
            "epoch: 20 step: 2786, loss is 1.1111611127853394\n",
            "epoch: 20 step: 2787, loss is 0.8066785931587219\n",
            "epoch: 20 step: 2788, loss is 0.8647043108940125\n",
            "epoch: 20 step: 2789, loss is 0.7967361807823181\n",
            "epoch: 20 step: 2790, loss is 0.864726722240448\n",
            "epoch: 20 step: 2791, loss is 0.9437450766563416\n",
            "epoch: 20 step: 2792, loss is 1.0034905672073364\n",
            "epoch: 20 step: 2793, loss is 0.8906626105308533\n",
            "epoch: 20 step: 2794, loss is 0.9732012748718262\n",
            "epoch: 20 step: 2795, loss is 0.7946317195892334\n",
            "epoch: 20 step: 2796, loss is 1.0179675817489624\n",
            "epoch: 20 step: 2797, loss is 0.9826751947402954\n",
            "epoch: 20 step: 2798, loss is 0.9711055755615234\n",
            "epoch: 20 step: 2799, loss is 0.8029499650001526\n",
            "epoch: 20 step: 2800, loss is 0.9775650501251221\n",
            "epoch: 20 step: 2801, loss is 1.1655564308166504\n",
            "epoch: 20 step: 2802, loss is 0.823657751083374\n",
            "epoch: 20 step: 2803, loss is 0.8084463477134705\n",
            "epoch: 20 step: 2804, loss is 0.9232057332992554\n",
            "epoch: 20 step: 2805, loss is 0.9626049399375916\n",
            "epoch: 20 step: 2806, loss is 0.8669626712799072\n",
            "epoch: 20 step: 2807, loss is 0.9735556840896606\n",
            "epoch: 20 step: 2808, loss is 0.9686022400856018\n",
            "epoch: 20 step: 2809, loss is 0.9679834842681885\n",
            "epoch: 20 step: 2810, loss is 0.8890965580940247\n",
            "epoch: 20 step: 2811, loss is 0.8446992039680481\n",
            "epoch: 20 step: 2812, loss is 0.9428760409355164\n",
            "epoch: 20 step: 2813, loss is 0.7753943204879761\n",
            "epoch: 20 step: 2814, loss is 0.9021674990653992\n",
            "epoch: 20 step: 2815, loss is 0.7308939099311829\n",
            "epoch: 20 step: 2816, loss is 0.9730165600776672\n",
            "epoch: 20 step: 2817, loss is 0.9730477929115295\n",
            "epoch: 20 step: 2818, loss is 1.1638436317443848\n",
            "epoch: 20 step: 2819, loss is 1.0921134948730469\n",
            "epoch: 20 step: 2820, loss is 0.8314639329910278\n",
            "epoch: 20 step: 2821, loss is 0.9878466725349426\n",
            "epoch: 20 step: 2822, loss is 0.8141875863075256\n",
            "epoch: 20 step: 2823, loss is 0.907406210899353\n",
            "epoch: 20 step: 2824, loss is 0.9260939955711365\n",
            "epoch: 20 step: 2825, loss is 0.9997791647911072\n",
            "epoch: 20 step: 2826, loss is 0.8768783211708069\n",
            "epoch: 20 step: 2827, loss is 1.018684983253479\n",
            "epoch: 20 step: 2828, loss is 0.8842249512672424\n",
            "epoch: 20 step: 2829, loss is 0.9680350422859192\n",
            "epoch: 20 step: 2830, loss is 0.8835300207138062\n",
            "epoch: 20 step: 2831, loss is 0.9960429072380066\n",
            "epoch: 20 step: 2832, loss is 0.9502814412117004\n",
            "epoch: 20 step: 2833, loss is 0.7549792528152466\n",
            "epoch: 20 step: 2834, loss is 0.8031874299049377\n",
            "epoch: 20 step: 2835, loss is 0.9067438840866089\n",
            "epoch: 20 step: 2836, loss is 0.9053143858909607\n",
            "epoch: 20 step: 2837, loss is 0.841107964515686\n",
            "epoch: 20 step: 2838, loss is 0.9221615195274353\n",
            "epoch: 20 step: 2839, loss is 0.944063663482666\n",
            "epoch: 20 step: 2840, loss is 0.7929159998893738\n",
            "epoch: 20 step: 2841, loss is 0.8665737509727478\n",
            "epoch: 20 step: 2842, loss is 0.7156118750572205\n",
            "epoch: 20 step: 2843, loss is 0.9243812561035156\n",
            "epoch: 20 step: 2844, loss is 0.824876606464386\n",
            "epoch: 20 step: 2845, loss is 0.9402509927749634\n",
            "epoch: 20 step: 2846, loss is 0.9103983044624329\n",
            "epoch: 20 step: 2847, loss is 1.0274652242660522\n",
            "epoch: 20 step: 2848, loss is 0.8127461075782776\n",
            "epoch: 20 step: 2849, loss is 0.9247236251831055\n",
            "epoch: 20 step: 2850, loss is 0.8754898905754089\n",
            "epoch: 20 step: 2851, loss is 1.051923394203186\n",
            "epoch: 20 step: 2852, loss is 1.1240592002868652\n",
            "epoch: 20 step: 2853, loss is 0.8982527852058411\n",
            "epoch: 20 step: 2854, loss is 0.865948498249054\n",
            "epoch: 20 step: 2855, loss is 0.8477039337158203\n",
            "epoch: 20 step: 2856, loss is 0.8155361413955688\n",
            "epoch: 20 step: 2857, loss is 1.0338335037231445\n",
            "epoch: 20 step: 2858, loss is 0.961630642414093\n",
            "epoch: 20 step: 2859, loss is 0.8129320740699768\n",
            "epoch: 20 step: 2860, loss is 0.8165771961212158\n",
            "epoch: 20 step: 2861, loss is 0.9888846278190613\n",
            "epoch: 20 step: 2862, loss is 0.7423527240753174\n",
            "epoch: 20 step: 2863, loss is 0.9962846040725708\n",
            "epoch: 20 step: 2864, loss is 0.9728370308876038\n",
            "epoch: 20 step: 2865, loss is 0.936471700668335\n",
            "epoch: 20 step: 2866, loss is 0.9927489161491394\n",
            "epoch: 20 step: 2867, loss is 0.8051425814628601\n",
            "epoch: 20 step: 2868, loss is 0.914080023765564\n",
            "epoch: 20 step: 2869, loss is 0.872801661491394\n",
            "epoch: 20 step: 2870, loss is 0.9328271150588989\n",
            "epoch: 20 step: 2871, loss is 0.883297324180603\n",
            "epoch: 20 step: 2872, loss is 0.9343221187591553\n",
            "epoch: 20 step: 2873, loss is 1.0393376350402832\n",
            "epoch: 20 step: 2874, loss is 1.0658957958221436\n",
            "epoch: 20 step: 2875, loss is 1.0103620290756226\n",
            "epoch: 20 step: 2876, loss is 0.8750115036964417\n",
            "epoch: 20 step: 2877, loss is 0.7492445707321167\n",
            "epoch: 20 step: 2878, loss is 0.7609477043151855\n",
            "epoch: 20 step: 2879, loss is 0.9509433507919312\n",
            "epoch: 20 step: 2880, loss is 0.9291622042655945\n",
            "epoch: 20 step: 2881, loss is 0.7254027724266052\n",
            "epoch: 20 step: 2882, loss is 0.6986454725265503\n",
            "epoch: 20 step: 2883, loss is 0.7383797764778137\n",
            "epoch: 20 step: 2884, loss is 0.8735626935958862\n",
            "epoch: 20 step: 2885, loss is 0.8312235474586487\n",
            "epoch: 20 step: 2886, loss is 0.9126500487327576\n",
            "epoch: 20 step: 2887, loss is 0.7613509893417358\n",
            "epoch: 20 step: 2888, loss is 0.8800264596939087\n",
            "epoch: 20 step: 2889, loss is 0.920216977596283\n",
            "epoch: 20 step: 2890, loss is 0.7892667055130005\n",
            "epoch: 20 step: 2891, loss is 0.9890183210372925\n",
            "epoch: 20 step: 2892, loss is 0.8783411383628845\n",
            "epoch: 20 step: 2893, loss is 0.7976078987121582\n",
            "epoch: 20 step: 2894, loss is 0.6986210942268372\n",
            "epoch: 20 step: 2895, loss is 0.8614784479141235\n",
            "epoch: 20 step: 2896, loss is 0.9099054932594299\n",
            "epoch: 20 step: 2897, loss is 0.8711702227592468\n",
            "epoch: 20 step: 2898, loss is 0.7008039951324463\n",
            "epoch: 20 step: 2899, loss is 0.7494322061538696\n",
            "epoch: 20 step: 2900, loss is 0.8635895848274231\n",
            "epoch: 20 step: 2901, loss is 0.8766985535621643\n",
            "epoch: 20 step: 2902, loss is 0.7930036187171936\n",
            "epoch: 20 step: 2903, loss is 0.8299042582511902\n",
            "epoch: 20 step: 2904, loss is 0.8926025629043579\n",
            "epoch: 20 step: 2905, loss is 0.9652950763702393\n",
            "epoch: 20 step: 2906, loss is 0.8078116774559021\n",
            "epoch: 20 step: 2907, loss is 0.9449504613876343\n",
            "epoch: 20 step: 2908, loss is 0.9106218218803406\n",
            "epoch: 20 step: 2909, loss is 0.9743375182151794\n",
            "epoch: 20 step: 2910, loss is 0.8461317420005798\n",
            "epoch: 20 step: 2911, loss is 1.0255887508392334\n",
            "epoch: 20 step: 2912, loss is 0.937116265296936\n",
            "epoch: 20 step: 2913, loss is 0.7972084283828735\n",
            "epoch: 20 step: 2914, loss is 0.9828209280967712\n",
            "epoch: 20 step: 2915, loss is 0.7838147878646851\n",
            "epoch: 20 step: 2916, loss is 1.0033124685287476\n",
            "epoch: 20 step: 2917, loss is 0.8250011801719666\n",
            "epoch: 20 step: 2918, loss is 0.8624515533447266\n",
            "epoch: 20 step: 2919, loss is 1.0114715099334717\n",
            "epoch: 20 step: 2920, loss is 0.8359330296516418\n",
            "epoch: 20 step: 2921, loss is 0.9624618291854858\n",
            "epoch: 20 step: 2922, loss is 0.8864688277244568\n",
            "epoch: 20 step: 2923, loss is 1.000255823135376\n",
            "epoch: 20 step: 2924, loss is 0.8927087783813477\n",
            "epoch: 20 step: 2925, loss is 0.8675106167793274\n",
            "epoch: 20 step: 2926, loss is 0.7346391081809998\n",
            "epoch: 20 step: 2927, loss is 0.8324209451675415\n",
            "epoch: 20 step: 2928, loss is 0.8829109072685242\n",
            "epoch: 20 step: 2929, loss is 0.92120361328125\n",
            "epoch: 20 step: 2930, loss is 0.8939438462257385\n",
            "epoch: 20 step: 2931, loss is 0.9758778810501099\n",
            "epoch: 20 step: 2932, loss is 1.0048757791519165\n",
            "epoch: 20 step: 2933, loss is 0.9409852623939514\n",
            "epoch: 20 step: 2934, loss is 0.9944531917572021\n",
            "epoch: 20 step: 2935, loss is 0.8043441772460938\n",
            "epoch: 20 step: 2936, loss is 1.189767837524414\n",
            "epoch: 20 step: 2937, loss is 1.0481253862380981\n",
            "epoch: 20 step: 2938, loss is 0.9284374117851257\n",
            "epoch: 20 step: 2939, loss is 0.8748925924301147\n",
            "epoch: 20 step: 2940, loss is 0.9531751871109009\n",
            "epoch: 20 step: 2941, loss is 0.9755184650421143\n",
            "epoch: 20 step: 2942, loss is 0.6954250931739807\n",
            "epoch: 20 step: 2943, loss is 0.8875834345817566\n",
            "epoch: 20 step: 2944, loss is 0.8956766128540039\n",
            "epoch: 20 step: 2945, loss is 0.7675946354866028\n",
            "epoch: 20 step: 2946, loss is 0.912059485912323\n",
            "epoch: 20 step: 2947, loss is 0.9034841060638428\n",
            "epoch: 20 step: 2948, loss is 0.7852794528007507\n",
            "epoch: 20 step: 2949, loss is 0.9580588340759277\n",
            "epoch: 20 step: 2950, loss is 0.8916648626327515\n",
            "epoch: 20 step: 2951, loss is 0.9122183322906494\n",
            "epoch: 20 step: 2952, loss is 0.9948683977127075\n",
            "epoch: 20 step: 2953, loss is 0.8135119080543518\n",
            "epoch: 20 step: 2954, loss is 0.9170292615890503\n",
            "epoch: 20 step: 2955, loss is 0.852729856967926\n",
            "epoch: 20 step: 2956, loss is 0.7429109215736389\n",
            "epoch: 20 step: 2957, loss is 0.8703420162200928\n",
            "epoch: 20 step: 2958, loss is 0.9477267861366272\n",
            "epoch: 20 step: 2959, loss is 0.8504469394683838\n",
            "epoch: 20 step: 2960, loss is 0.8296926021575928\n",
            "epoch: 20 step: 2961, loss is 0.8721402883529663\n",
            "epoch: 20 step: 2962, loss is 0.9197697043418884\n",
            "epoch: 20 step: 2963, loss is 0.7716606855392456\n",
            "epoch: 20 step: 2964, loss is 0.8411417007446289\n",
            "epoch: 20 step: 2965, loss is 0.8928546905517578\n",
            "epoch: 20 step: 2966, loss is 0.7364509105682373\n",
            "epoch: 20 step: 2967, loss is 1.1544257402420044\n",
            "epoch: 20 step: 2968, loss is 0.9006983041763306\n",
            "epoch: 20 step: 2969, loss is 0.810278058052063\n",
            "epoch: 20 step: 2970, loss is 0.8788002133369446\n",
            "epoch: 20 step: 2971, loss is 0.7746055126190186\n",
            "epoch: 20 step: 2972, loss is 0.9988741874694824\n",
            "epoch: 20 step: 2973, loss is 0.963930070400238\n",
            "epoch: 20 step: 2974, loss is 0.8935950398445129\n",
            "epoch: 20 step: 2975, loss is 0.9802149534225464\n",
            "epoch: 20 step: 2976, loss is 0.7880687713623047\n",
            "epoch: 20 step: 2977, loss is 0.9191165566444397\n",
            "epoch: 20 step: 2978, loss is 0.9016386866569519\n",
            "epoch: 20 step: 2979, loss is 0.7306202054023743\n",
            "epoch: 20 step: 2980, loss is 1.1007329225540161\n",
            "epoch: 20 step: 2981, loss is 0.9571434259414673\n",
            "epoch: 20 step: 2982, loss is 0.9101151823997498\n",
            "epoch: 20 step: 2983, loss is 0.9979588985443115\n",
            "epoch: 20 step: 2984, loss is 0.8736971020698547\n",
            "epoch: 20 step: 2985, loss is 1.0170183181762695\n",
            "epoch: 20 step: 2986, loss is 0.8972031474113464\n",
            "epoch: 20 step: 2987, loss is 0.8904774188995361\n",
            "epoch: 20 step: 2988, loss is 0.9594924449920654\n",
            "epoch: 20 step: 2989, loss is 0.7361787557601929\n",
            "epoch: 20 step: 2990, loss is 0.8800445199012756\n",
            "epoch: 20 step: 2991, loss is 0.8938663005828857\n",
            "epoch: 20 step: 2992, loss is 0.8086831569671631\n",
            "epoch: 20 step: 2993, loss is 0.7655976414680481\n",
            "epoch: 20 step: 2994, loss is 0.7099781036376953\n",
            "epoch: 20 step: 2995, loss is 0.971423864364624\n",
            "epoch: 20 step: 2996, loss is 0.8155373930931091\n",
            "epoch: 20 step: 2997, loss is 0.8725229501724243\n",
            "epoch: 20 step: 2998, loss is 0.6096295118331909\n",
            "epoch: 20 step: 2999, loss is 0.7495383620262146\n",
            "epoch: 20 step: 3000, loss is 0.9884074330329895\n",
            "epoch: 20 step: 3001, loss is 1.0583323240280151\n",
            "epoch: 20 step: 3002, loss is 1.016902208328247\n",
            "epoch: 20 step: 3003, loss is 0.8875055909156799\n",
            "epoch: 20 step: 3004, loss is 0.8899614810943604\n",
            "epoch: 20 step: 3005, loss is 0.938269317150116\n",
            "epoch: 20 step: 3006, loss is 0.7568754553794861\n",
            "epoch: 20 step: 3007, loss is 1.0269321203231812\n",
            "epoch: 20 step: 3008, loss is 0.8813791871070862\n",
            "epoch: 20 step: 3009, loss is 0.8053314685821533\n",
            "epoch: 20 step: 3010, loss is 1.0344481468200684\n",
            "epoch: 20 step: 3011, loss is 1.1705186367034912\n",
            "epoch: 20 step: 3012, loss is 0.8608654737472534\n",
            "epoch: 20 step: 3013, loss is 0.7929263710975647\n",
            "epoch: 20 step: 3014, loss is 0.7678797245025635\n",
            "epoch: 20 step: 3015, loss is 0.9543373584747314\n",
            "epoch: 20 step: 3016, loss is 0.8651784658432007\n",
            "epoch: 20 step: 3017, loss is 0.9505839347839355\n",
            "epoch: 20 step: 3018, loss is 0.8530852794647217\n",
            "epoch: 20 step: 3019, loss is 0.8238629698753357\n",
            "epoch: 20 step: 3020, loss is 0.8807920217514038\n",
            "epoch: 20 step: 3021, loss is 0.7270723581314087\n",
            "epoch: 20 step: 3022, loss is 0.8288017511367798\n",
            "epoch: 20 step: 3023, loss is 0.8008773326873779\n",
            "epoch: 20 step: 3024, loss is 0.8519734740257263\n",
            "epoch: 20 step: 3025, loss is 0.8674803972244263\n",
            "epoch: 20 step: 3026, loss is 0.7245385050773621\n",
            "epoch: 20 step: 3027, loss is 0.8851006031036377\n",
            "epoch: 20 step: 3028, loss is 0.876716673374176\n",
            "epoch: 20 step: 3029, loss is 1.0583552122116089\n",
            "epoch: 20 step: 3030, loss is 0.8004472255706787\n",
            "epoch: 20 step: 3031, loss is 0.7581447958946228\n",
            "epoch: 20 step: 3032, loss is 0.8546749949455261\n",
            "epoch: 20 step: 3033, loss is 0.964914858341217\n",
            "epoch: 20 step: 3034, loss is 0.8522965908050537\n",
            "epoch: 20 step: 3035, loss is 0.9516714215278625\n",
            "epoch: 20 step: 3036, loss is 0.7957403063774109\n",
            "epoch: 20 step: 3037, loss is 0.8325850963592529\n",
            "epoch: 20 step: 3038, loss is 0.7536107897758484\n",
            "epoch: 20 step: 3039, loss is 0.7960216403007507\n",
            "epoch: 20 step: 3040, loss is 0.9478790760040283\n",
            "epoch: 20 step: 3041, loss is 0.8052772879600525\n",
            "epoch: 20 step: 3042, loss is 1.0569895505905151\n",
            "epoch: 20 step: 3043, loss is 0.8786149024963379\n",
            "epoch: 20 step: 3044, loss is 0.9817525744438171\n",
            "epoch: 20 step: 3045, loss is 1.0262442827224731\n",
            "epoch: 20 step: 3046, loss is 0.7602663636207581\n",
            "epoch: 20 step: 3047, loss is 0.9391245245933533\n",
            "epoch: 20 step: 3048, loss is 0.767400860786438\n",
            "epoch: 20 step: 3049, loss is 0.6570037007331848\n",
            "epoch: 20 step: 3050, loss is 0.7384061813354492\n",
            "epoch: 20 step: 3051, loss is 0.7856155633926392\n",
            "epoch: 20 step: 3052, loss is 0.7807765603065491\n",
            "epoch: 20 step: 3053, loss is 0.9110426902770996\n",
            "epoch: 20 step: 3054, loss is 0.9974034428596497\n",
            "epoch: 20 step: 3055, loss is 0.8204634189605713\n",
            "epoch: 20 step: 3056, loss is 1.033421277999878\n",
            "epoch: 20 step: 3057, loss is 0.7598557472229004\n",
            "epoch: 20 step: 3058, loss is 1.119130253791809\n",
            "epoch: 20 step: 3059, loss is 0.8791453838348389\n",
            "epoch: 20 step: 3060, loss is 1.0211620330810547\n",
            "epoch: 20 step: 3061, loss is 0.8772279620170593\n",
            "epoch: 20 step: 3062, loss is 0.7809841632843018\n",
            "epoch: 20 step: 3063, loss is 0.7341087460517883\n",
            "epoch: 20 step: 3064, loss is 0.7760326862335205\n",
            "epoch: 20 step: 3065, loss is 0.924330472946167\n",
            "epoch: 20 step: 3066, loss is 0.8866599798202515\n",
            "epoch: 20 step: 3067, loss is 0.9690101742744446\n",
            "epoch: 20 step: 3068, loss is 0.759727954864502\n",
            "epoch: 20 step: 3069, loss is 0.8767498135566711\n",
            "epoch: 20 step: 3070, loss is 0.8828495144844055\n",
            "epoch: 20 step: 3071, loss is 0.7186723351478577\n",
            "epoch: 20 step: 3072, loss is 0.9144780039787292\n",
            "epoch: 20 step: 3073, loss is 0.8950286507606506\n",
            "epoch: 20 step: 3074, loss is 0.8983029127120972\n",
            "epoch: 20 step: 3075, loss is 0.836108922958374\n",
            "epoch: 20 step: 3076, loss is 0.9485526084899902\n",
            "epoch: 20 step: 3077, loss is 0.9063224792480469\n",
            "epoch: 20 step: 3078, loss is 0.8631083965301514\n",
            "epoch: 20 step: 3079, loss is 1.0666495561599731\n",
            "epoch: 20 step: 3080, loss is 1.0953794717788696\n",
            "epoch: 20 step: 3081, loss is 0.7809203267097473\n",
            "epoch: 20 step: 3082, loss is 1.1147894859313965\n",
            "epoch: 20 step: 3083, loss is 0.833987295627594\n",
            "epoch: 20 step: 3084, loss is 0.9074901342391968\n",
            "epoch: 20 step: 3085, loss is 0.8907198905944824\n",
            "epoch: 20 step: 3086, loss is 1.1266006231307983\n",
            "epoch: 20 step: 3087, loss is 0.9216110706329346\n",
            "epoch: 20 step: 3088, loss is 1.0200175046920776\n",
            "epoch: 20 step: 3089, loss is 0.8153560161590576\n",
            "epoch: 20 step: 3090, loss is 0.751150369644165\n",
            "epoch: 20 step: 3091, loss is 1.0425004959106445\n",
            "epoch: 20 step: 3092, loss is 0.9055606722831726\n",
            "epoch: 20 step: 3093, loss is 0.8344287872314453\n",
            "epoch: 20 step: 3094, loss is 0.9065641164779663\n",
            "epoch: 20 step: 3095, loss is 0.8128748536109924\n",
            "epoch: 20 step: 3096, loss is 0.7437683343887329\n",
            "epoch: 20 step: 3097, loss is 0.9575023651123047\n",
            "epoch: 20 step: 3098, loss is 1.037527322769165\n",
            "epoch: 20 step: 3099, loss is 0.8799417614936829\n",
            "epoch: 20 step: 3100, loss is 0.8023988008499146\n",
            "epoch: 20 step: 3101, loss is 1.075478434562683\n",
            "epoch: 20 step: 3102, loss is 0.9135274887084961\n",
            "epoch: 20 step: 3103, loss is 0.8459069132804871\n",
            "epoch: 20 step: 3104, loss is 0.9873012900352478\n",
            "epoch: 20 step: 3105, loss is 0.7653826475143433\n",
            "epoch: 20 step: 3106, loss is 0.9433838725090027\n",
            "epoch: 20 step: 3107, loss is 0.9931671023368835\n",
            "epoch: 20 step: 3108, loss is 0.8838966488838196\n",
            "epoch: 20 step: 3109, loss is 1.1234256029129028\n",
            "epoch: 20 step: 3110, loss is 0.9477221369743347\n",
            "epoch: 20 step: 3111, loss is 0.7956055402755737\n",
            "epoch: 20 step: 3112, loss is 0.8741838932037354\n",
            "epoch: 20 step: 3113, loss is 0.822647213935852\n",
            "epoch: 20 step: 3114, loss is 0.7934003472328186\n",
            "epoch: 20 step: 3115, loss is 0.8703009486198425\n",
            "epoch: 20 step: 3116, loss is 0.807910680770874\n",
            "epoch: 20 step: 3117, loss is 0.8011806011199951\n",
            "epoch: 20 step: 3118, loss is 1.0726436376571655\n",
            "epoch: 20 step: 3119, loss is 0.7924803495407104\n",
            "epoch: 20 step: 3120, loss is 0.7797160744667053\n",
            "epoch: 20 step: 3121, loss is 0.8049202561378479\n",
            "epoch: 20 step: 3122, loss is 0.9421677589416504\n",
            "epoch: 20 step: 3123, loss is 0.981689989566803\n",
            "epoch: 20 step: 3124, loss is 0.9275943040847778\n",
            "epoch: 20 step: 3125, loss is 1.0912491083145142\n",
            "epoch: 20 step: 3126, loss is 1.08168625831604\n",
            "epoch: 20 step: 3127, loss is 0.6528582572937012\n",
            "epoch: 20 step: 3128, loss is 0.9827027916908264\n",
            "epoch: 20 step: 3129, loss is 1.0614861249923706\n",
            "epoch: 20 step: 3130, loss is 0.9400935769081116\n",
            "epoch: 20 step: 3131, loss is 0.8796696662902832\n",
            "epoch: 20 step: 3132, loss is 0.8017107248306274\n",
            "epoch: 20 step: 3133, loss is 1.2442567348480225\n",
            "epoch: 20 step: 3134, loss is 0.8621690273284912\n",
            "epoch: 20 step: 3135, loss is 1.024213433265686\n",
            "epoch: 20 step: 3136, loss is 0.8938907980918884\n",
            "epoch: 20 step: 3137, loss is 0.9677190184593201\n",
            "epoch: 20 step: 3138, loss is 1.0258934497833252\n",
            "epoch: 20 step: 3139, loss is 0.8831331729888916\n",
            "epoch: 20 step: 3140, loss is 0.9225367307662964\n",
            "epoch: 20 step: 3141, loss is 0.8980812430381775\n",
            "epoch: 20 step: 3142, loss is 0.9100261330604553\n",
            "epoch: 20 step: 3143, loss is 1.0199203491210938\n",
            "epoch: 20 step: 3144, loss is 0.8391083478927612\n",
            "epoch: 20 step: 3145, loss is 0.957441508769989\n",
            "epoch: 20 step: 3146, loss is 0.9189614653587341\n",
            "epoch: 20 step: 3147, loss is 0.9003326296806335\n",
            "epoch: 20 step: 3148, loss is 1.0561904907226562\n",
            "epoch: 20 step: 3149, loss is 0.8873353004455566\n",
            "epoch: 20 step: 3150, loss is 0.9475923776626587\n",
            "epoch: 20 step: 3151, loss is 0.8083794116973877\n",
            "epoch: 20 step: 3152, loss is 0.8408389091491699\n",
            "epoch: 20 step: 3153, loss is 0.7674852013587952\n",
            "epoch: 20 step: 3154, loss is 0.870235800743103\n",
            "epoch: 20 step: 3155, loss is 0.7974801659584045\n",
            "epoch: 20 step: 3156, loss is 0.850064754486084\n",
            "epoch: 20 step: 3157, loss is 0.8710950613021851\n",
            "epoch: 20 step: 3158, loss is 0.864745020866394\n",
            "epoch: 20 step: 3159, loss is 0.9804424047470093\n",
            "epoch: 20 step: 3160, loss is 0.9586151242256165\n",
            "epoch: 20 step: 3161, loss is 0.8943472504615784\n",
            "epoch: 20 step: 3162, loss is 0.9693789482116699\n",
            "epoch: 20 step: 3163, loss is 1.005479335784912\n",
            "epoch: 20 step: 3164, loss is 0.9362651109695435\n",
            "epoch: 20 step: 3165, loss is 1.0778393745422363\n",
            "epoch: 20 step: 3166, loss is 1.006048560142517\n",
            "epoch: 20 step: 3167, loss is 0.8894793391227722\n",
            "epoch: 20 step: 3168, loss is 0.9026882648468018\n",
            "epoch: 20 step: 3169, loss is 0.7553721070289612\n",
            "epoch: 20 step: 3170, loss is 1.072930097579956\n",
            "epoch: 20 step: 3171, loss is 0.7753453850746155\n",
            "epoch: 20 step: 3172, loss is 0.8985975980758667\n",
            "epoch: 20 step: 3173, loss is 0.9102500081062317\n",
            "epoch: 20 step: 3174, loss is 0.903279185295105\n",
            "epoch: 20 step: 3175, loss is 0.9658886790275574\n",
            "epoch: 20 step: 3176, loss is 0.8435535430908203\n",
            "epoch: 20 step: 3177, loss is 0.966204047203064\n",
            "epoch: 20 step: 3178, loss is 0.9376010894775391\n",
            "epoch: 20 step: 3179, loss is 1.0359463691711426\n",
            "epoch: 20 step: 3180, loss is 0.8616898655891418\n",
            "epoch: 20 step: 3181, loss is 0.9808970093727112\n",
            "epoch: 20 step: 3182, loss is 0.7538785338401794\n",
            "epoch: 20 step: 3183, loss is 0.8031195402145386\n",
            "epoch: 20 step: 3184, loss is 0.7695005536079407\n",
            "epoch: 20 step: 3185, loss is 0.9174560904502869\n",
            "epoch: 20 step: 3186, loss is 0.9491839408874512\n",
            "epoch: 20 step: 3187, loss is 1.1246614456176758\n",
            "epoch: 20 step: 3188, loss is 0.7024247050285339\n",
            "epoch: 20 step: 3189, loss is 0.9472647905349731\n",
            "epoch: 20 step: 3190, loss is 0.7256590127944946\n",
            "epoch: 20 step: 3191, loss is 0.8507524132728577\n",
            "epoch: 20 step: 3192, loss is 0.7945833802223206\n",
            "epoch: 20 step: 3193, loss is 1.0194250345230103\n",
            "epoch: 20 step: 3194, loss is 0.6646760106086731\n",
            "epoch: 20 step: 3195, loss is 0.8758679628372192\n",
            "epoch: 20 step: 3196, loss is 0.8110829591751099\n",
            "epoch: 20 step: 3197, loss is 0.8564990758895874\n",
            "epoch: 20 step: 3198, loss is 0.7867692708969116\n",
            "epoch: 20 step: 3199, loss is 0.7779437899589539\n",
            "epoch: 20 step: 3200, loss is 0.7336089015007019\n",
            "epoch: 20 step: 3201, loss is 0.9687721729278564\n",
            "epoch: 20 step: 3202, loss is 1.0318422317504883\n",
            "epoch: 20 step: 3203, loss is 1.0267781019210815\n",
            "epoch: 20 step: 3204, loss is 0.838365375995636\n",
            "epoch: 20 step: 3205, loss is 0.7847244739532471\n",
            "epoch: 20 step: 3206, loss is 0.8276087641716003\n",
            "epoch: 20 step: 3207, loss is 0.887702465057373\n",
            "epoch: 20 step: 3208, loss is 0.9591649174690247\n",
            "epoch: 20 step: 3209, loss is 0.9328674674034119\n",
            "epoch: 20 step: 3210, loss is 0.9565656185150146\n",
            "epoch: 20 step: 3211, loss is 0.8958573341369629\n",
            "epoch: 20 step: 3212, loss is 0.756515622138977\n",
            "epoch: 20 step: 3213, loss is 1.1054577827453613\n",
            "epoch: 20 step: 3214, loss is 0.9138840436935425\n",
            "epoch: 20 step: 3215, loss is 1.1658406257629395\n",
            "epoch: 20 step: 3216, loss is 1.038387417793274\n",
            "epoch: 20 step: 3217, loss is 0.7509921789169312\n",
            "epoch: 20 step: 3218, loss is 0.8508960008621216\n",
            "epoch: 20 step: 3219, loss is 0.8296971321105957\n",
            "epoch: 20 step: 3220, loss is 1.0339014530181885\n",
            "epoch: 20 step: 3221, loss is 0.9858731627464294\n",
            "epoch: 20 step: 3222, loss is 0.9626712203025818\n",
            "epoch: 20 step: 3223, loss is 0.9289717078208923\n",
            "epoch: 20 step: 3224, loss is 0.9001339673995972\n",
            "epoch: 20 step: 3225, loss is 1.0374337434768677\n",
            "epoch: 20 step: 3226, loss is 0.7210912704467773\n",
            "epoch: 20 step: 3227, loss is 0.9744366407394409\n",
            "epoch: 20 step: 3228, loss is 0.9171317219734192\n",
            "epoch: 20 step: 3229, loss is 1.0231890678405762\n",
            "epoch: 20 step: 3230, loss is 0.8257389068603516\n",
            "epoch: 20 step: 3231, loss is 1.1512324810028076\n",
            "epoch: 20 step: 3232, loss is 0.8549044132232666\n",
            "epoch: 20 step: 3233, loss is 0.8314643502235413\n",
            "epoch: 20 step: 3234, loss is 0.8124887347221375\n",
            "epoch: 20 step: 3235, loss is 0.926287055015564\n",
            "epoch: 20 step: 3236, loss is 0.7194669246673584\n",
            "epoch: 20 step: 3237, loss is 0.8674604892730713\n",
            "epoch: 20 step: 3238, loss is 0.9009334444999695\n",
            "epoch: 20 step: 3239, loss is 0.9390707612037659\n",
            "epoch: 20 step: 3240, loss is 0.9617354869842529\n",
            "epoch: 20 step: 3241, loss is 0.8576678037643433\n",
            "epoch: 20 step: 3242, loss is 0.9035564661026001\n",
            "epoch: 20 step: 3243, loss is 0.8442290425300598\n",
            "epoch: 20 step: 3244, loss is 0.9691650867462158\n",
            "epoch: 20 step: 3245, loss is 0.8709307909011841\n",
            "epoch: 20 step: 3246, loss is 0.8770879507064819\n",
            "epoch: 20 step: 3247, loss is 1.0250064134597778\n",
            "epoch: 20 step: 3248, loss is 0.8391485214233398\n",
            "epoch: 20 step: 3249, loss is 0.7021471858024597\n",
            "epoch: 20 step: 3250, loss is 0.8781625628471375\n",
            "epoch: 20 step: 3251, loss is 0.795271098613739\n",
            "epoch: 20 step: 3252, loss is 0.7788212299346924\n",
            "epoch: 20 step: 3253, loss is 0.7581171989440918\n",
            "epoch: 20 step: 3254, loss is 0.9090119004249573\n",
            "epoch: 20 step: 3255, loss is 0.9051818251609802\n",
            "epoch: 20 step: 3256, loss is 0.915730357170105\n",
            "epoch: 20 step: 3257, loss is 0.7395108938217163\n",
            "epoch: 20 step: 3258, loss is 0.9767987728118896\n",
            "epoch: 20 step: 3259, loss is 0.9232922196388245\n",
            "epoch: 20 step: 3260, loss is 0.8451967835426331\n",
            "epoch: 20 step: 3261, loss is 0.8228389620780945\n",
            "epoch: 20 step: 3262, loss is 0.9622300267219543\n",
            "epoch: 20 step: 3263, loss is 0.8165387511253357\n",
            "epoch: 20 step: 3264, loss is 0.959949791431427\n",
            "epoch: 20 step: 3265, loss is 1.1248083114624023\n",
            "epoch: 20 step: 3266, loss is 1.1120893955230713\n",
            "epoch: 20 step: 3267, loss is 0.9412782192230225\n",
            "epoch: 20 step: 3268, loss is 0.9726744890213013\n",
            "epoch: 20 step: 3269, loss is 0.9019585847854614\n",
            "epoch: 20 step: 3270, loss is 0.9249503016471863\n",
            "epoch: 20 step: 3271, loss is 0.8969314694404602\n",
            "epoch: 20 step: 3272, loss is 0.7976533770561218\n",
            "epoch: 20 step: 3273, loss is 0.9488247632980347\n",
            "epoch: 20 step: 3274, loss is 0.8527286648750305\n",
            "epoch: 20 step: 3275, loss is 0.8604316115379333\n",
            "epoch: 20 step: 3276, loss is 0.994154691696167\n",
            "epoch: 20 step: 3277, loss is 0.8954059481620789\n",
            "epoch: 20 step: 3278, loss is 0.8796521425247192\n",
            "epoch: 20 step: 3279, loss is 0.7704359292984009\n",
            "epoch: 20 step: 3280, loss is 0.7972735166549683\n",
            "epoch: 20 step: 3281, loss is 0.9028605818748474\n",
            "epoch: 20 step: 3282, loss is 1.0446115732192993\n",
            "epoch: 20 step: 3283, loss is 0.9556257724761963\n",
            "epoch: 20 step: 3284, loss is 0.8848573565483093\n",
            "epoch: 20 step: 3285, loss is 0.8818774819374084\n",
            "epoch: 20 step: 3286, loss is 1.1053498983383179\n",
            "epoch: 20 step: 3287, loss is 0.9068522453308105\n",
            "epoch: 20 step: 3288, loss is 0.7754542827606201\n",
            "epoch: 20 step: 3289, loss is 0.8457313776016235\n",
            "epoch: 20 step: 3290, loss is 0.9766278266906738\n",
            "epoch: 20 step: 3291, loss is 0.8759554028511047\n",
            "epoch: 20 step: 3292, loss is 0.8631541728973389\n",
            "epoch: 20 step: 3293, loss is 0.85246741771698\n",
            "epoch: 20 step: 3294, loss is 0.8761309385299683\n",
            "epoch: 20 step: 3295, loss is 0.8681800365447998\n",
            "epoch: 20 step: 3296, loss is 0.8991075158119202\n",
            "epoch: 20 step: 3297, loss is 1.0885008573532104\n",
            "epoch: 20 step: 3298, loss is 0.8382774591445923\n",
            "epoch: 20 step: 3299, loss is 0.9101865291595459\n",
            "epoch: 20 step: 3300, loss is 1.033774733543396\n",
            "epoch: 20 step: 3301, loss is 0.7072977423667908\n",
            "epoch: 20 step: 3302, loss is 0.8990999460220337\n",
            "epoch: 20 step: 3303, loss is 0.942341685295105\n",
            "epoch: 20 step: 3304, loss is 0.9333182573318481\n",
            "epoch: 20 step: 3305, loss is 1.021208643913269\n",
            "epoch: 20 step: 3306, loss is 0.9740880131721497\n",
            "epoch: 20 step: 3307, loss is 0.8222338557243347\n",
            "epoch: 20 step: 3308, loss is 0.9753736257553101\n",
            "epoch: 20 step: 3309, loss is 0.872351884841919\n",
            "epoch: 20 step: 3310, loss is 0.9938130378723145\n",
            "epoch: 20 step: 3311, loss is 1.0735923051834106\n",
            "epoch: 20 step: 3312, loss is 1.00935697555542\n",
            "epoch: 20 step: 3313, loss is 0.7895120978355408\n",
            "epoch: 20 step: 3314, loss is 0.9112395644187927\n",
            "epoch: 20 step: 3315, loss is 0.7038920521736145\n",
            "epoch: 20 step: 3316, loss is 0.8018295764923096\n",
            "epoch: 20 step: 3317, loss is 0.860408365726471\n",
            "epoch: 20 step: 3318, loss is 0.8634292483329773\n",
            "epoch: 20 step: 3319, loss is 0.8943549990653992\n",
            "epoch: 20 step: 3320, loss is 0.7450374960899353\n",
            "epoch: 20 step: 3321, loss is 0.967559278011322\n",
            "epoch: 20 step: 3322, loss is 0.8532530069351196\n",
            "epoch: 20 step: 3323, loss is 0.9508918523788452\n",
            "epoch: 20 step: 3324, loss is 0.8007211685180664\n",
            "epoch: 20 step: 3325, loss is 0.9375171661376953\n",
            "epoch: 20 step: 3326, loss is 0.92253577709198\n",
            "epoch: 20 step: 3327, loss is 0.9030207395553589\n",
            "epoch: 20 step: 3328, loss is 0.8290337324142456\n",
            "epoch: 20 step: 3329, loss is 0.8278372287750244\n",
            "epoch: 20 step: 3330, loss is 0.9266549348831177\n",
            "epoch: 20 step: 3331, loss is 0.7931150197982788\n",
            "epoch: 20 step: 3332, loss is 0.9291322827339172\n",
            "epoch: 20 step: 3333, loss is 0.795538067817688\n",
            "epoch: 20 step: 3334, loss is 0.9313489198684692\n",
            "epoch: 20 step: 3335, loss is 0.7247978448867798\n",
            "epoch: 20 step: 3336, loss is 0.9649286270141602\n",
            "epoch: 20 step: 3337, loss is 0.8880389928817749\n",
            "epoch: 20 step: 3338, loss is 0.9571793675422668\n",
            "epoch: 20 step: 3339, loss is 0.8538951277732849\n",
            "epoch: 20 step: 3340, loss is 0.7424017786979675\n",
            "epoch: 20 step: 3341, loss is 0.9202451705932617\n",
            "epoch: 20 step: 3342, loss is 0.9475383162498474\n",
            "epoch: 20 step: 3343, loss is 0.8045475482940674\n",
            "epoch: 20 step: 3344, loss is 0.8635777235031128\n",
            "epoch: 20 step: 3345, loss is 0.9459718465805054\n",
            "epoch: 20 step: 3346, loss is 1.0155335664749146\n",
            "epoch: 20 step: 3347, loss is 1.1155258417129517\n",
            "epoch: 20 step: 3348, loss is 0.7516646385192871\n",
            "epoch: 20 step: 3349, loss is 0.9109863638877869\n",
            "epoch: 20 step: 3350, loss is 1.0221918821334839\n",
            "epoch: 20 step: 3351, loss is 0.9080797433853149\n",
            "epoch: 20 step: 3352, loss is 0.82205730676651\n",
            "epoch: 20 step: 3353, loss is 0.7929659485816956\n",
            "epoch: 20 step: 3354, loss is 0.8168351650238037\n",
            "epoch: 20 step: 3355, loss is 0.7584291696548462\n",
            "epoch: 20 step: 3356, loss is 0.784346342086792\n",
            "epoch: 20 step: 3357, loss is 0.9351029992103577\n",
            "epoch: 20 step: 3358, loss is 0.813060998916626\n",
            "epoch: 20 step: 3359, loss is 0.9120550155639648\n",
            "epoch: 20 step: 3360, loss is 0.8755344152450562\n",
            "epoch: 20 step: 3361, loss is 0.7869815826416016\n",
            "epoch: 20 step: 3362, loss is 0.8758112788200378\n",
            "epoch: 20 step: 3363, loss is 1.0180126428604126\n",
            "epoch: 20 step: 3364, loss is 0.9740344882011414\n",
            "epoch: 20 step: 3365, loss is 1.0260475873947144\n",
            "epoch: 20 step: 3366, loss is 0.9338681697845459\n",
            "epoch: 20 step: 3367, loss is 0.8909677267074585\n",
            "epoch: 20 step: 3368, loss is 0.7064698934555054\n",
            "epoch: 20 step: 3369, loss is 0.9931373000144958\n",
            "epoch: 20 step: 3370, loss is 0.9275474548339844\n",
            "epoch: 20 step: 3371, loss is 0.8175971508026123\n",
            "epoch: 20 step: 3372, loss is 0.9164402484893799\n",
            "epoch: 20 step: 3373, loss is 0.9836423397064209\n",
            "epoch: 20 step: 3374, loss is 0.8817125558853149\n",
            "epoch: 20 step: 3375, loss is 0.8294521570205688\n",
            "epoch: 20 step: 3376, loss is 1.0901596546173096\n",
            "epoch: 20 step: 3377, loss is 0.7347509264945984\n",
            "epoch: 20 step: 3378, loss is 0.9765006303787231\n",
            "epoch: 20 step: 3379, loss is 0.8813983798027039\n",
            "epoch: 20 step: 3380, loss is 0.9631699919700623\n",
            "epoch: 20 step: 3381, loss is 0.9773909449577332\n",
            "epoch: 20 step: 3382, loss is 0.774339497089386\n",
            "epoch: 20 step: 3383, loss is 0.9441741108894348\n",
            "epoch: 20 step: 3384, loss is 0.8605204820632935\n",
            "epoch: 20 step: 3385, loss is 1.0156339406967163\n",
            "epoch: 20 step: 3386, loss is 0.9046286344528198\n",
            "epoch: 20 step: 3387, loss is 0.9467064738273621\n",
            "epoch: 20 step: 3388, loss is 0.8141831755638123\n",
            "epoch: 20 step: 3389, loss is 0.9215514659881592\n",
            "epoch: 20 step: 3390, loss is 0.7905710935592651\n",
            "epoch: 20 step: 3391, loss is 0.9032090306282043\n",
            "epoch: 20 step: 3392, loss is 0.8117471933364868\n",
            "epoch: 20 step: 3393, loss is 0.7158755660057068\n",
            "epoch: 20 step: 3394, loss is 0.8454902768135071\n",
            "epoch: 20 step: 3395, loss is 0.8049377799034119\n",
            "epoch: 20 step: 3396, loss is 0.8792381286621094\n",
            "epoch: 20 step: 3397, loss is 0.8149142861366272\n",
            "epoch: 20 step: 3398, loss is 1.0285850763320923\n",
            "epoch: 20 step: 3399, loss is 0.7816588282585144\n",
            "epoch: 20 step: 3400, loss is 0.8392841219902039\n",
            "epoch: 20 step: 3401, loss is 0.8354620933532715\n",
            "epoch: 20 step: 3402, loss is 0.7255996465682983\n",
            "epoch: 20 step: 3403, loss is 1.1481088399887085\n",
            "epoch: 20 step: 3404, loss is 0.9901823997497559\n",
            "epoch: 20 step: 3405, loss is 0.7383327484130859\n",
            "epoch: 20 step: 3406, loss is 0.9893335700035095\n",
            "epoch: 20 step: 3407, loss is 0.7588926553726196\n",
            "epoch: 20 step: 3408, loss is 1.0016194581985474\n",
            "epoch: 20 step: 3409, loss is 0.8805062770843506\n",
            "epoch: 20 step: 3410, loss is 1.0563578605651855\n",
            "epoch: 20 step: 3411, loss is 1.0834161043167114\n",
            "epoch: 20 step: 3412, loss is 0.8924416303634644\n",
            "epoch: 20 step: 3413, loss is 0.9002379179000854\n",
            "epoch: 20 step: 3414, loss is 0.9027498960494995\n",
            "epoch: 20 step: 3415, loss is 1.0287364721298218\n",
            "epoch: 20 step: 3416, loss is 0.7755352854728699\n",
            "epoch: 20 step: 3417, loss is 0.9419247508049011\n",
            "epoch: 20 step: 3418, loss is 0.8843958377838135\n",
            "epoch: 20 step: 3419, loss is 0.790144681930542\n",
            "epoch: 20 step: 3420, loss is 0.7656508088111877\n",
            "epoch: 20 step: 3421, loss is 0.7994199395179749\n",
            "epoch: 20 step: 3422, loss is 0.9377790093421936\n",
            "epoch: 20 step: 3423, loss is 1.0861095190048218\n",
            "epoch: 20 step: 3424, loss is 0.9228544235229492\n",
            "epoch: 20 step: 3425, loss is 0.8419092893600464\n",
            "epoch: 20 step: 3426, loss is 0.8397605419158936\n",
            "epoch: 20 step: 3427, loss is 0.9368887543678284\n",
            "epoch: 20 step: 3428, loss is 0.959313154220581\n",
            "epoch: 20 step: 3429, loss is 0.816798210144043\n",
            "epoch: 20 step: 3430, loss is 0.784965991973877\n",
            "epoch: 20 step: 3431, loss is 0.8564571738243103\n",
            "epoch: 20 step: 3432, loss is 0.7946014404296875\n",
            "epoch: 20 step: 3433, loss is 0.8391330242156982\n",
            "epoch: 20 step: 3434, loss is 0.8670333623886108\n",
            "epoch: 20 step: 3435, loss is 0.951258659362793\n",
            "epoch: 20 step: 3436, loss is 1.0180259943008423\n",
            "epoch: 20 step: 3437, loss is 0.9115550518035889\n",
            "epoch: 20 step: 3438, loss is 1.0313969850540161\n",
            "epoch: 20 step: 3439, loss is 0.9923712015151978\n",
            "epoch: 20 step: 3440, loss is 1.1394193172454834\n",
            "epoch: 20 step: 3441, loss is 0.7182619571685791\n",
            "epoch: 20 step: 3442, loss is 0.8669410943984985\n",
            "epoch: 20 step: 3443, loss is 0.8144171833992004\n",
            "epoch: 20 step: 3444, loss is 0.9232898950576782\n",
            "epoch: 20 step: 3445, loss is 0.8804213404655457\n",
            "epoch: 20 step: 3446, loss is 0.8628320693969727\n",
            "epoch: 20 step: 3447, loss is 0.9372039437294006\n",
            "epoch: 20 step: 3448, loss is 0.8108764886856079\n",
            "epoch: 20 step: 3449, loss is 0.8582913279533386\n",
            "epoch: 20 step: 3450, loss is 0.8496689200401306\n",
            "epoch: 20 step: 3451, loss is 0.8217270970344543\n",
            "epoch: 20 step: 3452, loss is 0.7865646481513977\n",
            "epoch: 20 step: 3453, loss is 1.002968430519104\n",
            "epoch: 20 step: 3454, loss is 0.9554792642593384\n",
            "epoch: 20 step: 3455, loss is 0.8036690950393677\n",
            "epoch: 20 step: 3456, loss is 0.8215522170066833\n",
            "epoch: 20 step: 3457, loss is 0.7725411653518677\n",
            "epoch: 20 step: 3458, loss is 0.8200725317001343\n",
            "epoch: 20 step: 3459, loss is 0.8368951678276062\n",
            "epoch: 20 step: 3460, loss is 0.8700113892555237\n",
            "epoch: 20 step: 3461, loss is 0.8065078854560852\n",
            "epoch: 20 step: 3462, loss is 0.9048845171928406\n",
            "epoch: 20 step: 3463, loss is 0.9895742535591125\n",
            "epoch: 20 step: 3464, loss is 0.7939868569374084\n",
            "epoch: 20 step: 3465, loss is 0.7828707098960876\n",
            "epoch: 20 step: 3466, loss is 0.9748259782791138\n",
            "epoch: 20 step: 3467, loss is 0.9227375984191895\n",
            "epoch: 20 step: 3468, loss is 1.0749272108078003\n",
            "epoch: 20 step: 3469, loss is 0.9228351712226868\n",
            "epoch: 20 step: 3470, loss is 0.9802491664886475\n",
            "epoch: 20 step: 3471, loss is 0.9015890955924988\n",
            "epoch: 20 step: 3472, loss is 0.9388934373855591\n",
            "epoch: 20 step: 3473, loss is 0.9039320945739746\n",
            "epoch: 20 step: 3474, loss is 0.851456880569458\n",
            "epoch: 20 step: 3475, loss is 0.6016179919242859\n",
            "epoch: 20 step: 3476, loss is 0.8967314958572388\n",
            "epoch: 20 step: 3477, loss is 1.0756561756134033\n",
            "epoch: 20 step: 3478, loss is 0.9526890516281128\n",
            "epoch: 20 step: 3479, loss is 1.0262160301208496\n",
            "epoch: 20 step: 3480, loss is 0.8357104659080505\n",
            "epoch: 20 step: 3481, loss is 1.0159697532653809\n",
            "epoch: 20 step: 3482, loss is 0.748540997505188\n",
            "epoch: 20 step: 3483, loss is 0.8863494396209717\n",
            "epoch: 20 step: 3484, loss is 0.7565908432006836\n",
            "epoch: 20 step: 3485, loss is 0.8550080060958862\n",
            "epoch: 20 step: 3486, loss is 0.9851874113082886\n",
            "epoch: 20 step: 3487, loss is 0.779841959476471\n",
            "epoch: 20 step: 3488, loss is 0.9557179808616638\n",
            "epoch: 20 step: 3489, loss is 0.8063600659370422\n",
            "epoch: 20 step: 3490, loss is 1.0322896242141724\n",
            "epoch: 20 step: 3491, loss is 1.0691838264465332\n",
            "epoch: 20 step: 3492, loss is 0.8195686340332031\n",
            "epoch: 20 step: 3493, loss is 0.7466291785240173\n",
            "epoch: 20 step: 3494, loss is 0.9924129247665405\n",
            "epoch: 20 step: 3495, loss is 0.9372090697288513\n",
            "epoch: 20 step: 3496, loss is 1.0385570526123047\n",
            "epoch: 20 step: 3497, loss is 0.8622013330459595\n",
            "epoch: 20 step: 3498, loss is 0.9061022996902466\n",
            "epoch: 20 step: 3499, loss is 0.8486754298210144\n",
            "epoch: 20 step: 3500, loss is 1.0035499334335327\n",
            "epoch: 20 step: 3501, loss is 0.9763237237930298\n",
            "epoch: 20 step: 3502, loss is 0.7257546782493591\n",
            "epoch: 20 step: 3503, loss is 0.7868982553482056\n",
            "epoch: 20 step: 3504, loss is 0.8415113687515259\n",
            "epoch: 20 step: 3505, loss is 0.7732703685760498\n",
            "epoch: 20 step: 3506, loss is 0.9607216715812683\n",
            "epoch: 20 step: 3507, loss is 0.6304581761360168\n",
            "epoch: 20 step: 3508, loss is 0.9348433613777161\n",
            "epoch: 20 step: 3509, loss is 0.8972371220588684\n",
            "epoch: 20 step: 3510, loss is 1.0704271793365479\n",
            "epoch: 20 step: 3511, loss is 1.0139881372451782\n",
            "epoch: 20 step: 3512, loss is 0.9251608848571777\n",
            "epoch: 20 step: 3513, loss is 0.8825363516807556\n",
            "epoch: 20 step: 3514, loss is 1.0703860521316528\n",
            "epoch: 20 step: 3515, loss is 1.0570567846298218\n",
            "epoch: 20 step: 3516, loss is 1.0329511165618896\n",
            "epoch: 20 step: 3517, loss is 0.9561060667037964\n",
            "epoch: 20 step: 3518, loss is 0.745855450630188\n",
            "epoch: 20 step: 3519, loss is 1.0062623023986816\n",
            "epoch: 20 step: 3520, loss is 1.0472209453582764\n",
            "epoch: 20 step: 3521, loss is 0.8982037305831909\n",
            "epoch: 20 step: 3522, loss is 0.8528420925140381\n",
            "epoch: 20 step: 3523, loss is 0.8329501748085022\n",
            "epoch: 20 step: 3524, loss is 0.999079704284668\n",
            "epoch: 20 step: 3525, loss is 0.8345014452934265\n",
            "epoch: 20 step: 3526, loss is 0.7903757691383362\n",
            "epoch: 20 step: 3527, loss is 0.8707785606384277\n",
            "epoch: 20 step: 3528, loss is 0.814338207244873\n",
            "epoch: 20 step: 3529, loss is 0.6678749918937683\n",
            "epoch: 20 step: 3530, loss is 1.1305296421051025\n",
            "epoch: 20 step: 3531, loss is 0.756520688533783\n",
            "epoch: 20 step: 3532, loss is 1.0336345434188843\n",
            "epoch: 20 step: 3533, loss is 0.9114533066749573\n",
            "epoch: 20 step: 3534, loss is 0.8868746757507324\n",
            "epoch: 20 step: 3535, loss is 0.860768735408783\n",
            "epoch: 20 step: 3536, loss is 0.8101768493652344\n",
            "epoch: 20 step: 3537, loss is 0.8090529441833496\n",
            "epoch: 20 step: 3538, loss is 1.0042184591293335\n",
            "epoch: 20 step: 3539, loss is 0.7850704789161682\n",
            "epoch: 20 step: 3540, loss is 0.785666286945343\n",
            "epoch: 20 step: 3541, loss is 0.7126139402389526\n",
            "epoch: 20 step: 3542, loss is 0.915036141872406\n",
            "epoch: 20 step: 3543, loss is 0.8789461255073547\n",
            "epoch: 20 step: 3544, loss is 0.7965431213378906\n",
            "epoch: 20 step: 3545, loss is 0.9288501739501953\n",
            "epoch: 20 step: 3546, loss is 0.9997304081916809\n",
            "epoch: 20 step: 3547, loss is 1.0573776960372925\n",
            "epoch: 20 step: 3548, loss is 0.74968421459198\n",
            "epoch: 20 step: 3549, loss is 0.975521445274353\n",
            "epoch: 20 step: 3550, loss is 0.8330376148223877\n",
            "epoch: 20 step: 3551, loss is 0.8857929706573486\n",
            "epoch: 20 step: 3552, loss is 0.8188979625701904\n",
            "epoch: 20 step: 3553, loss is 1.0805109739303589\n",
            "epoch: 20 step: 3554, loss is 0.9659051299095154\n",
            "epoch: 20 step: 3555, loss is 0.7545122504234314\n",
            "epoch: 20 step: 3556, loss is 0.8926857113838196\n",
            "epoch: 20 step: 3557, loss is 0.9946465492248535\n",
            "epoch: 20 step: 3558, loss is 0.7306733727455139\n",
            "epoch: 20 step: 3559, loss is 1.1319432258605957\n",
            "epoch: 20 step: 3560, loss is 1.0418914556503296\n",
            "epoch: 20 step: 3561, loss is 0.826630175113678\n",
            "epoch: 20 step: 3562, loss is 0.9690471887588501\n",
            "epoch: 20 step: 3563, loss is 0.8740352988243103\n",
            "epoch: 20 step: 3564, loss is 0.6776350736618042\n",
            "epoch: 20 step: 3565, loss is 0.8399040699005127\n",
            "epoch: 20 step: 3566, loss is 0.8547676801681519\n",
            "epoch: 20 step: 3567, loss is 1.0047035217285156\n",
            "epoch: 20 step: 3568, loss is 0.8883476257324219\n",
            "epoch: 20 step: 3569, loss is 0.8592588901519775\n",
            "epoch: 20 step: 3570, loss is 0.7870665192604065\n",
            "epoch: 20 step: 3571, loss is 0.9170041680335999\n",
            "epoch: 20 step: 3572, loss is 0.9546850919723511\n",
            "epoch: 20 step: 3573, loss is 0.7915361523628235\n",
            "epoch: 20 step: 3574, loss is 0.888214647769928\n",
            "epoch: 20 step: 3575, loss is 0.9897180795669556\n",
            "epoch: 20 step: 3576, loss is 0.8213931322097778\n",
            "epoch: 20 step: 3577, loss is 0.7941107153892517\n",
            "epoch: 20 step: 3578, loss is 0.9031439423561096\n",
            "epoch: 20 step: 3579, loss is 0.9827157855033875\n",
            "epoch: 20 step: 3580, loss is 0.983016312122345\n",
            "epoch: 20 step: 3581, loss is 0.9305803179740906\n",
            "epoch: 20 step: 3582, loss is 0.8009008169174194\n",
            "epoch: 20 step: 3583, loss is 0.8813827633857727\n",
            "epoch: 20 step: 3584, loss is 0.9486958384513855\n",
            "epoch: 20 step: 3585, loss is 0.898177981376648\n",
            "epoch: 20 step: 3586, loss is 0.9213617444038391\n",
            "epoch: 20 step: 3587, loss is 0.9698887467384338\n",
            "epoch: 20 step: 3588, loss is 0.7852355241775513\n",
            "epoch: 20 step: 3589, loss is 0.8715497255325317\n",
            "epoch: 20 step: 3590, loss is 0.7537394165992737\n",
            "epoch: 20 step: 3591, loss is 0.8229350447654724\n",
            "epoch: 20 step: 3592, loss is 0.8013250231742859\n",
            "epoch: 20 step: 3593, loss is 1.0263797044754028\n",
            "epoch: 20 step: 3594, loss is 0.867059588432312\n",
            "epoch: 20 step: 3595, loss is 1.0614864826202393\n",
            "epoch: 20 step: 3596, loss is 1.142781138420105\n",
            "epoch: 20 step: 3597, loss is 0.9362393021583557\n",
            "epoch: 20 step: 3598, loss is 0.949999213218689\n",
            "epoch: 20 step: 3599, loss is 0.883306086063385\n",
            "epoch: 20 step: 3600, loss is 0.9544941782951355\n",
            "epoch: 20 step: 3601, loss is 1.047839879989624\n",
            "epoch: 20 step: 3602, loss is 0.8592870831489563\n",
            "epoch: 20 step: 3603, loss is 0.9727538228034973\n",
            "epoch: 20 step: 3604, loss is 0.9006797075271606\n",
            "epoch: 20 step: 3605, loss is 0.8926502466201782\n",
            "epoch: 20 step: 3606, loss is 0.9459658861160278\n",
            "epoch: 20 step: 3607, loss is 0.9173111915588379\n",
            "epoch: 20 step: 3608, loss is 0.8547011017799377\n",
            "epoch: 20 step: 3609, loss is 0.9513821005821228\n",
            "epoch: 20 step: 3610, loss is 0.7871881127357483\n",
            "epoch: 20 step: 3611, loss is 0.7842283248901367\n",
            "epoch: 20 step: 3612, loss is 0.8987618088722229\n",
            "epoch: 20 step: 3613, loss is 0.8577571511268616\n",
            "epoch: 20 step: 3614, loss is 0.706669270992279\n",
            "epoch: 20 step: 3615, loss is 0.8973886370658875\n",
            "epoch: 20 step: 3616, loss is 0.8907220363616943\n",
            "epoch: 20 step: 3617, loss is 0.8558824062347412\n",
            "epoch: 20 step: 3618, loss is 0.9289219379425049\n",
            "epoch: 20 step: 3619, loss is 0.9909615516662598\n",
            "epoch: 20 step: 3620, loss is 0.9590210914611816\n",
            "epoch: 20 step: 3621, loss is 0.858322262763977\n",
            "epoch: 20 step: 3622, loss is 0.9748684167861938\n",
            "epoch: 20 step: 3623, loss is 0.8982750773429871\n",
            "epoch: 20 step: 3624, loss is 0.892085611820221\n",
            "epoch: 20 step: 3625, loss is 1.187932014465332\n",
            "epoch: 20 step: 3626, loss is 0.9912970066070557\n",
            "epoch: 20 step: 3627, loss is 0.9005157351493835\n",
            "epoch: 20 step: 3628, loss is 0.7125464677810669\n",
            "epoch: 20 step: 3629, loss is 0.8994653224945068\n",
            "epoch: 20 step: 3630, loss is 0.9644257426261902\n",
            "epoch: 20 step: 3631, loss is 0.8540075421333313\n",
            "epoch: 20 step: 3632, loss is 0.6718608140945435\n",
            "epoch: 20 step: 3633, loss is 0.9297584891319275\n",
            "epoch: 20 step: 3634, loss is 0.8325108289718628\n",
            "epoch: 20 step: 3635, loss is 0.723842442035675\n",
            "epoch: 20 step: 3636, loss is 0.9176525473594666\n",
            "epoch: 20 step: 3637, loss is 0.8559882640838623\n",
            "epoch: 20 step: 3638, loss is 0.8195067644119263\n",
            "epoch: 20 step: 3639, loss is 0.7134438157081604\n",
            "epoch: 20 step: 3640, loss is 0.8407220840454102\n",
            "epoch: 20 step: 3641, loss is 1.1494898796081543\n",
            "epoch: 20 step: 3642, loss is 0.7936472296714783\n",
            "epoch: 20 step: 3643, loss is 0.9286320209503174\n",
            "epoch: 20 step: 3644, loss is 0.8851515650749207\n",
            "epoch: 20 step: 3645, loss is 0.8474056720733643\n",
            "epoch: 20 step: 3646, loss is 0.727005124092102\n",
            "epoch: 20 step: 3647, loss is 0.843211829662323\n",
            "epoch: 20 step: 3648, loss is 0.7515578866004944\n",
            "epoch: 20 step: 3649, loss is 1.1340858936309814\n",
            "epoch: 20 step: 3650, loss is 0.6992587447166443\n",
            "epoch: 20 step: 3651, loss is 0.7466003894805908\n",
            "epoch: 20 step: 3652, loss is 0.8199859857559204\n",
            "epoch: 20 step: 3653, loss is 0.9285791516304016\n",
            "epoch: 20 step: 3654, loss is 0.9803431630134583\n",
            "epoch: 20 step: 3655, loss is 0.9735965728759766\n",
            "epoch: 20 step: 3656, loss is 0.7617287039756775\n",
            "epoch: 20 step: 3657, loss is 1.0026963949203491\n",
            "epoch: 20 step: 3658, loss is 0.7813713550567627\n",
            "epoch: 20 step: 3659, loss is 0.9037284851074219\n",
            "epoch: 20 step: 3660, loss is 0.8661952018737793\n",
            "epoch: 20 step: 3661, loss is 0.7863188982009888\n",
            "epoch: 20 step: 3662, loss is 1.0182478427886963\n",
            "epoch: 20 step: 3663, loss is 0.9660664796829224\n",
            "epoch: 20 step: 3664, loss is 0.7886867523193359\n",
            "epoch: 20 step: 3665, loss is 0.8177564740180969\n",
            "epoch: 20 step: 3666, loss is 1.11367666721344\n",
            "epoch: 20 step: 3667, loss is 0.8624764680862427\n",
            "epoch: 20 step: 3668, loss is 0.99396812915802\n",
            "epoch: 20 step: 3669, loss is 0.7274918556213379\n",
            "epoch: 20 step: 3670, loss is 1.0125093460083008\n",
            "epoch: 20 step: 3671, loss is 0.7783505320549011\n",
            "epoch: 20 step: 3672, loss is 0.8289464116096497\n",
            "epoch: 20 step: 3673, loss is 0.8863388299942017\n",
            "epoch: 20 step: 3674, loss is 0.8463073968887329\n",
            "epoch: 20 step: 3675, loss is 0.9505060315132141\n",
            "epoch: 20 step: 3676, loss is 0.8308600783348083\n",
            "epoch: 20 step: 3677, loss is 0.9741948843002319\n",
            "epoch: 20 step: 3678, loss is 0.9713519215583801\n",
            "epoch: 20 step: 3679, loss is 0.7626405954360962\n",
            "epoch: 20 step: 3680, loss is 0.9320377111434937\n",
            "epoch: 20 step: 3681, loss is 0.8453570008277893\n",
            "epoch: 20 step: 3682, loss is 1.0710842609405518\n",
            "epoch: 20 step: 3683, loss is 0.7086524963378906\n",
            "epoch: 20 step: 3684, loss is 0.8083349466323853\n",
            "epoch: 20 step: 3685, loss is 0.6575806140899658\n",
            "epoch: 20 step: 3686, loss is 0.8979503512382507\n",
            "epoch: 20 step: 3687, loss is 0.876194417476654\n",
            "epoch: 20 step: 3688, loss is 0.8022938966751099\n",
            "epoch: 20 step: 3689, loss is 0.8827910423278809\n",
            "epoch: 20 step: 3690, loss is 0.8483166694641113\n",
            "epoch: 20 step: 3691, loss is 1.1068427562713623\n",
            "epoch: 20 step: 3692, loss is 1.1372989416122437\n",
            "epoch: 20 step: 3693, loss is 0.7831337451934814\n",
            "epoch: 20 step: 3694, loss is 0.8038021326065063\n",
            "epoch: 20 step: 3695, loss is 0.8297876119613647\n",
            "epoch: 20 step: 3696, loss is 0.8475214242935181\n",
            "epoch: 20 step: 3697, loss is 0.7861042618751526\n",
            "epoch: 20 step: 3698, loss is 0.714326024055481\n",
            "epoch: 20 step: 3699, loss is 0.9239072203636169\n",
            "epoch: 20 step: 3700, loss is 1.024492859840393\n",
            "epoch: 20 step: 3701, loss is 0.9047760367393494\n",
            "epoch: 20 step: 3702, loss is 0.9329626560211182\n",
            "epoch: 20 step: 3703, loss is 0.8599323034286499\n",
            "epoch: 20 step: 3704, loss is 0.7785663604736328\n",
            "epoch: 20 step: 3705, loss is 0.8653634190559387\n",
            "epoch: 20 step: 3706, loss is 0.994493842124939\n",
            "epoch: 20 step: 3707, loss is 0.7534940838813782\n",
            "epoch: 20 step: 3708, loss is 0.7965484857559204\n",
            "epoch: 20 step: 3709, loss is 0.8400789499282837\n",
            "epoch: 20 step: 3710, loss is 0.8261280655860901\n",
            "epoch: 20 step: 3711, loss is 0.8954975008964539\n",
            "epoch: 20 step: 3712, loss is 0.9029349088668823\n",
            "epoch: 20 step: 3713, loss is 0.807288408279419\n",
            "epoch: 20 step: 3714, loss is 0.9651859998703003\n",
            "epoch: 20 step: 3715, loss is 0.7128993272781372\n",
            "epoch: 20 step: 3716, loss is 0.7958980202674866\n",
            "epoch: 20 step: 3717, loss is 0.8861448168754578\n",
            "epoch: 20 step: 3718, loss is 1.0162415504455566\n",
            "epoch: 20 step: 3719, loss is 0.9502323865890503\n",
            "epoch: 20 step: 3720, loss is 0.965974748134613\n",
            "epoch: 20 step: 3721, loss is 0.8405442833900452\n",
            "epoch: 20 step: 3722, loss is 0.9844682812690735\n",
            "epoch: 20 step: 3723, loss is 0.8334078788757324\n",
            "epoch: 20 step: 3724, loss is 0.9704555869102478\n",
            "epoch: 20 step: 3725, loss is 0.8423662185668945\n",
            "epoch: 20 step: 3726, loss is 0.9518213868141174\n",
            "epoch: 20 step: 3727, loss is 0.8249631524085999\n",
            "epoch: 20 step: 3728, loss is 1.1053504943847656\n",
            "epoch: 20 step: 3729, loss is 0.7670661807060242\n",
            "epoch: 20 step: 3730, loss is 0.8168362379074097\n",
            "epoch: 20 step: 3731, loss is 0.938232958316803\n",
            "epoch: 20 step: 3732, loss is 0.895163357257843\n",
            "epoch: 20 step: 3733, loss is 0.8709046840667725\n",
            "epoch: 20 step: 3734, loss is 0.9739363789558411\n",
            "epoch: 20 step: 3735, loss is 0.9962934851646423\n",
            "epoch: 20 step: 3736, loss is 0.789970338344574\n",
            "epoch: 20 step: 3737, loss is 0.879079282283783\n",
            "epoch: 20 step: 3738, loss is 0.9116560816764832\n",
            "epoch: 20 step: 3739, loss is 0.8637766242027283\n",
            "epoch: 20 step: 3740, loss is 0.8759974241256714\n",
            "epoch: 20 step: 3741, loss is 1.032711386680603\n",
            "epoch: 20 step: 3742, loss is 0.9085314273834229\n",
            "epoch: 20 step: 3743, loss is 0.6929747462272644\n",
            "epoch: 20 step: 3744, loss is 0.6795392036437988\n",
            "epoch: 20 step: 3745, loss is 0.8834828734397888\n",
            "epoch: 20 step: 3746, loss is 0.8466249704360962\n",
            "epoch: 20 step: 3747, loss is 0.8831822276115417\n",
            "epoch: 20 step: 3748, loss is 0.9874220490455627\n",
            "epoch: 20 step: 3749, loss is 0.7596496343612671\n",
            "epoch: 20 step: 3750, loss is 0.8747776746749878\n",
            "epoch: 20 step: 3751, loss is 0.9177762269973755\n",
            "epoch: 20 step: 3752, loss is 0.8261341452598572\n",
            "epoch: 20 step: 3753, loss is 0.7549655437469482\n",
            "epoch: 20 step: 3754, loss is 0.9583196640014648\n",
            "epoch: 20 step: 3755, loss is 1.0197210311889648\n",
            "epoch: 20 step: 3756, loss is 1.055509328842163\n",
            "epoch: 20 step: 3757, loss is 0.8671450614929199\n",
            "epoch: 20 step: 3758, loss is 0.8156821727752686\n",
            "epoch: 20 step: 3759, loss is 1.0631002187728882\n",
            "epoch: 20 step: 3760, loss is 0.782554030418396\n",
            "epoch: 20 step: 3761, loss is 0.9237965941429138\n",
            "epoch: 20 step: 3762, loss is 0.7971387505531311\n",
            "epoch: 20 step: 3763, loss is 1.1297039985656738\n",
            "epoch: 20 step: 3764, loss is 0.983005166053772\n",
            "epoch: 20 step: 3765, loss is 1.0472917556762695\n",
            "epoch: 20 step: 3766, loss is 1.036532998085022\n",
            "epoch: 20 step: 3767, loss is 0.9026317596435547\n",
            "epoch: 20 step: 3768, loss is 1.0390230417251587\n",
            "epoch: 20 step: 3769, loss is 0.9109964370727539\n",
            "epoch: 20 step: 3770, loss is 0.9028072953224182\n",
            "epoch: 20 step: 3771, loss is 0.8813944458961487\n",
            "epoch: 20 step: 3772, loss is 1.0085244178771973\n",
            "epoch: 20 step: 3773, loss is 1.0454527139663696\n",
            "epoch: 20 step: 3774, loss is 0.9471110701560974\n",
            "epoch: 20 step: 3775, loss is 0.8541776537895203\n",
            "epoch: 20 step: 3776, loss is 0.7250331044197083\n",
            "epoch: 20 step: 3777, loss is 0.871825635433197\n",
            "epoch: 20 step: 3778, loss is 0.845518171787262\n",
            "epoch: 20 step: 3779, loss is 1.0138250589370728\n",
            "epoch: 20 step: 3780, loss is 0.8722295165061951\n",
            "epoch: 20 step: 3781, loss is 1.220002293586731\n",
            "epoch: 20 step: 3782, loss is 0.740027666091919\n",
            "epoch: 20 step: 3783, loss is 0.9334076642990112\n",
            "epoch: 20 step: 3784, loss is 0.9730671644210815\n",
            "epoch: 20 step: 3785, loss is 0.8845903873443604\n",
            "epoch: 20 step: 3786, loss is 0.9343333840370178\n",
            "epoch: 20 step: 3787, loss is 0.8040422797203064\n",
            "epoch: 20 step: 3788, loss is 0.9767001867294312\n",
            "epoch: 20 step: 3789, loss is 0.7861246466636658\n",
            "epoch: 20 step: 3790, loss is 1.075812816619873\n",
            "epoch: 20 step: 3791, loss is 0.8350182175636292\n",
            "epoch: 20 step: 3792, loss is 1.0170079469680786\n",
            "epoch: 20 step: 3793, loss is 1.050537347793579\n",
            "epoch: 20 step: 3794, loss is 0.8732935190200806\n",
            "epoch: 20 step: 3795, loss is 0.7565195560455322\n",
            "epoch: 20 step: 3796, loss is 0.8759934306144714\n",
            "epoch: 20 step: 3797, loss is 0.8372077941894531\n",
            "epoch: 20 step: 3798, loss is 1.0321382284164429\n",
            "epoch: 20 step: 3799, loss is 0.9008805155754089\n",
            "epoch: 20 step: 3800, loss is 0.7799707055091858\n",
            "epoch: 20 step: 3801, loss is 0.9370011687278748\n",
            "epoch: 20 step: 3802, loss is 0.9522922039031982\n",
            "epoch: 20 step: 3803, loss is 0.9732345342636108\n",
            "epoch: 20 step: 3804, loss is 0.8449914455413818\n",
            "epoch: 20 step: 3805, loss is 0.9902063608169556\n",
            "epoch: 20 step: 3806, loss is 0.7770906686782837\n",
            "epoch: 20 step: 3807, loss is 0.7706464529037476\n",
            "epoch: 20 step: 3808, loss is 0.8117895126342773\n",
            "epoch: 20 step: 3809, loss is 0.8079076409339905\n",
            "epoch: 20 step: 3810, loss is 0.9077956080436707\n",
            "epoch: 20 step: 3811, loss is 0.8979647159576416\n",
            "epoch: 20 step: 3812, loss is 0.8594815731048584\n",
            "epoch: 20 step: 3813, loss is 0.8146762251853943\n",
            "epoch: 20 step: 3814, loss is 0.7315494418144226\n",
            "epoch: 20 step: 3815, loss is 0.9258026480674744\n",
            "epoch: 20 step: 3816, loss is 0.8928237557411194\n",
            "epoch: 20 step: 3817, loss is 0.7953690886497498\n",
            "epoch: 20 step: 3818, loss is 1.051762342453003\n",
            "epoch: 20 step: 3819, loss is 0.9662998914718628\n",
            "epoch: 20 step: 3820, loss is 0.8136433362960815\n",
            "epoch: 20 step: 3821, loss is 0.7331706881523132\n",
            "epoch: 20 step: 3822, loss is 0.8564904928207397\n",
            "epoch: 20 step: 3823, loss is 0.828684389591217\n",
            "epoch: 20 step: 3824, loss is 0.9165109992027283\n",
            "epoch: 20 step: 3825, loss is 1.0921498537063599\n",
            "epoch: 20 step: 3826, loss is 1.0217808485031128\n",
            "epoch: 20 step: 3827, loss is 1.0403311252593994\n",
            "epoch: 20 step: 3828, loss is 0.8529126644134521\n",
            "epoch: 20 step: 3829, loss is 1.0716296434402466\n",
            "epoch: 20 step: 3830, loss is 0.7379220128059387\n",
            "epoch: 20 step: 3831, loss is 0.671021044254303\n",
            "epoch: 20 step: 3832, loss is 0.9731931090354919\n",
            "epoch: 20 step: 3833, loss is 1.0871455669403076\n",
            "epoch: 20 step: 3834, loss is 0.8798080086708069\n",
            "epoch: 20 step: 3835, loss is 1.0227038860321045\n",
            "epoch: 20 step: 3836, loss is 0.8587205410003662\n",
            "epoch: 20 step: 3837, loss is 0.7921828031539917\n",
            "epoch: 20 step: 3838, loss is 0.9168626666069031\n",
            "epoch: 20 step: 3839, loss is 0.8290507793426514\n",
            "epoch: 20 step: 3840, loss is 1.1378284692764282\n",
            "epoch: 20 step: 3841, loss is 0.9317636489868164\n",
            "epoch: 20 step: 3842, loss is 0.8816320300102234\n",
            "epoch: 20 step: 3843, loss is 0.9202209115028381\n",
            "epoch: 20 step: 3844, loss is 0.748641312122345\n",
            "epoch: 20 step: 3845, loss is 0.896861732006073\n",
            "epoch: 20 step: 3846, loss is 0.9277260899543762\n",
            "epoch: 20 step: 3847, loss is 0.8584299087524414\n",
            "epoch: 20 step: 3848, loss is 0.955967903137207\n",
            "epoch: 20 step: 3849, loss is 0.9169349670410156\n",
            "epoch: 20 step: 3850, loss is 0.8463018536567688\n",
            "epoch: 20 step: 3851, loss is 0.9715873599052429\n",
            "epoch: 20 step: 3852, loss is 0.8418001532554626\n",
            "epoch: 20 step: 3853, loss is 1.0795137882232666\n",
            "epoch: 20 step: 3854, loss is 0.9045295715332031\n",
            "epoch: 20 step: 3855, loss is 1.0555723905563354\n",
            "epoch: 20 step: 3856, loss is 0.9420086145401001\n",
            "epoch: 20 step: 3857, loss is 0.8164187073707581\n",
            "epoch: 20 step: 3858, loss is 0.7462432980537415\n",
            "epoch: 20 step: 3859, loss is 0.856334388256073\n",
            "epoch: 20 step: 3860, loss is 0.7677174210548401\n",
            "epoch: 20 step: 3861, loss is 0.8387081623077393\n",
            "epoch: 20 step: 3862, loss is 0.8603789806365967\n",
            "epoch: 20 step: 3863, loss is 0.8897663354873657\n",
            "epoch: 20 step: 3864, loss is 1.0194231271743774\n",
            "epoch: 20 step: 3865, loss is 0.7796839475631714\n",
            "epoch: 20 step: 3866, loss is 0.9713189005851746\n",
            "epoch: 20 step: 3867, loss is 0.9901566505432129\n",
            "epoch: 20 step: 3868, loss is 0.9932060241699219\n",
            "epoch: 20 step: 3869, loss is 0.9029545783996582\n",
            "epoch: 20 step: 3870, loss is 0.8297361135482788\n",
            "epoch: 20 step: 3871, loss is 0.9416954517364502\n",
            "epoch: 20 step: 3872, loss is 0.8544655442237854\n",
            "epoch: 20 step: 3873, loss is 0.796916127204895\n",
            "epoch: 20 step: 3874, loss is 0.8977340459823608\n",
            "epoch: 20 step: 3875, loss is 0.8733862638473511\n",
            "epoch: 20 step: 3876, loss is 0.9136176705360413\n",
            "epoch: 20 step: 3877, loss is 0.8520349264144897\n",
            "epoch: 20 step: 3878, loss is 1.0316921472549438\n",
            "epoch: 20 step: 3879, loss is 0.9307365417480469\n",
            "epoch: 20 step: 3880, loss is 0.7826152443885803\n",
            "epoch: 20 step: 3881, loss is 0.928158164024353\n",
            "epoch: 20 step: 3882, loss is 0.9295749664306641\n",
            "epoch: 20 step: 3883, loss is 0.7298499941825867\n",
            "epoch: 20 step: 3884, loss is 0.85530686378479\n",
            "epoch: 20 step: 3885, loss is 0.7289224863052368\n",
            "epoch: 20 step: 3886, loss is 0.9133360981941223\n",
            "epoch: 20 step: 3887, loss is 0.8676434755325317\n",
            "epoch: 20 step: 3888, loss is 0.8522215485572815\n",
            "epoch: 20 step: 3889, loss is 0.7716448903083801\n",
            "epoch: 20 step: 3890, loss is 0.8100788593292236\n",
            "epoch: 20 step: 3891, loss is 0.9276154041290283\n",
            "epoch: 20 step: 3892, loss is 1.0825518369674683\n",
            "epoch: 20 step: 3893, loss is 0.876691997051239\n",
            "epoch: 20 step: 3894, loss is 0.7813350558280945\n",
            "epoch: 20 step: 3895, loss is 1.1472450494766235\n",
            "epoch: 20 step: 3896, loss is 1.0299689769744873\n",
            "epoch: 20 step: 3897, loss is 0.8278199434280396\n",
            "epoch: 20 step: 3898, loss is 0.8209781646728516\n",
            "epoch: 20 step: 3899, loss is 1.0911736488342285\n",
            "epoch: 20 step: 3900, loss is 1.0126936435699463\n",
            "epoch: 20 step: 3901, loss is 0.9441166520118713\n",
            "epoch: 20 step: 3902, loss is 0.8947795033454895\n",
            "epoch: 20 step: 3903, loss is 0.9233728647232056\n",
            "epoch: 20 step: 3904, loss is 0.9291999340057373\n",
            "epoch: 20 step: 3905, loss is 0.8837770223617554\n",
            "epoch: 20 step: 3906, loss is 0.94414883852005\n",
            "epoch: 20 step: 3907, loss is 0.8028329610824585\n",
            "epoch: 20 step: 3908, loss is 0.7885565757751465\n",
            "epoch: 20 step: 3909, loss is 1.0080300569534302\n",
            "epoch: 20 step: 3910, loss is 0.9708240628242493\n",
            "epoch: 20 step: 3911, loss is 0.9903738498687744\n",
            "epoch: 20 step: 3912, loss is 0.8186210989952087\n",
            "epoch: 20 step: 3913, loss is 0.9484809637069702\n",
            "epoch: 20 step: 3914, loss is 0.9741773009300232\n",
            "epoch: 20 step: 3915, loss is 0.8277162313461304\n",
            "epoch: 20 step: 3916, loss is 0.9251585006713867\n",
            "epoch: 20 step: 3917, loss is 0.8701976537704468\n",
            "epoch: 20 step: 3918, loss is 0.8036676645278931\n",
            "epoch: 20 step: 3919, loss is 0.8240678310394287\n",
            "epoch: 20 step: 3920, loss is 0.8633381128311157\n",
            "epoch: 20 step: 3921, loss is 0.9059309363365173\n",
            "epoch: 20 step: 3922, loss is 0.8763224482536316\n",
            "epoch: 20 step: 3923, loss is 0.8287324905395508\n",
            "epoch: 20 step: 3924, loss is 0.8110623955726624\n",
            "epoch: 20 step: 3925, loss is 0.9603108763694763\n",
            "epoch: 20 step: 3926, loss is 0.9137862324714661\n",
            "epoch: 20 step: 3927, loss is 0.8498415946960449\n",
            "epoch: 20 step: 3928, loss is 0.8062723875045776\n",
            "epoch: 20 step: 3929, loss is 0.7965764403343201\n",
            "epoch: 20 step: 3930, loss is 0.8633164763450623\n",
            "epoch: 20 step: 3931, loss is 0.9131355285644531\n",
            "epoch: 20 step: 3932, loss is 0.7568780183792114\n",
            "epoch: 20 step: 3933, loss is 0.8536805510520935\n",
            "epoch: 20 step: 3934, loss is 0.9523229002952576\n",
            "epoch: 20 step: 3935, loss is 0.8008899092674255\n",
            "epoch: 20 step: 3936, loss is 0.8411085605621338\n",
            "epoch: 20 step: 3937, loss is 1.026282787322998\n",
            "epoch: 20 step: 3938, loss is 0.8767794966697693\n",
            "epoch: 20 step: 3939, loss is 0.7971411943435669\n",
            "epoch: 20 step: 3940, loss is 0.8165696263313293\n",
            "epoch: 20 step: 3941, loss is 0.9438860416412354\n",
            "epoch: 20 step: 3942, loss is 0.7759564518928528\n",
            "epoch: 20 step: 3943, loss is 0.7833566665649414\n",
            "epoch: 20 step: 3944, loss is 0.9916313290596008\n",
            "epoch: 20 step: 3945, loss is 0.8763447999954224\n",
            "epoch: 20 step: 3946, loss is 0.870866060256958\n",
            "epoch: 20 step: 3947, loss is 1.0054534673690796\n",
            "epoch: 20 step: 3948, loss is 0.8605570197105408\n",
            "epoch: 20 step: 3949, loss is 0.9262664914131165\n",
            "epoch: 20 step: 3950, loss is 0.8278641700744629\n",
            "epoch: 20 step: 3951, loss is 1.0465705394744873\n",
            "epoch: 20 step: 3952, loss is 0.8237484693527222\n",
            "epoch: 20 step: 3953, loss is 0.8344810605049133\n",
            "epoch: 20 step: 3954, loss is 0.9140058159828186\n",
            "epoch: 20 step: 3955, loss is 0.8419063091278076\n",
            "epoch: 20 step: 3956, loss is 0.8107178807258606\n",
            "epoch: 20 step: 3957, loss is 0.7887935042381287\n",
            "epoch: 20 step: 3958, loss is 0.813989520072937\n",
            "epoch: 20 step: 3959, loss is 0.9114747643470764\n",
            "epoch: 20 step: 3960, loss is 1.022236943244934\n",
            "epoch: 20 step: 3961, loss is 0.8523004055023193\n",
            "epoch: 20 step: 3962, loss is 0.9242050647735596\n",
            "epoch: 20 step: 3963, loss is 0.8372043371200562\n",
            "epoch: 20 step: 3964, loss is 0.9937103390693665\n",
            "epoch: 20 step: 3965, loss is 0.9531887173652649\n",
            "epoch: 20 step: 3966, loss is 1.0685629844665527\n",
            "epoch: 20 step: 3967, loss is 0.7261256575584412\n",
            "epoch: 20 step: 3968, loss is 0.83600914478302\n",
            "epoch: 20 step: 3969, loss is 1.133773922920227\n",
            "epoch: 20 step: 3970, loss is 1.0573514699935913\n",
            "epoch: 20 step: 3971, loss is 0.7390170693397522\n",
            "epoch: 20 step: 3972, loss is 0.8649515509605408\n",
            "epoch: 20 step: 3973, loss is 0.9359220266342163\n",
            "epoch: 20 step: 3974, loss is 0.892909824848175\n",
            "epoch: 20 step: 3975, loss is 0.9602741599082947\n",
            "epoch: 20 step: 3976, loss is 0.7695608139038086\n",
            "epoch: 20 step: 3977, loss is 0.8814171552658081\n",
            "epoch: 20 step: 3978, loss is 0.7821927666664124\n",
            "epoch: 20 step: 3979, loss is 1.0532857179641724\n",
            "epoch: 20 step: 3980, loss is 0.8063459992408752\n",
            "epoch: 20 step: 3981, loss is 0.8344828486442566\n",
            "epoch: 20 step: 3982, loss is 0.8592917919158936\n",
            "epoch: 20 step: 3983, loss is 0.9485192894935608\n",
            "epoch: 20 step: 3984, loss is 0.9338551163673401\n",
            "epoch: 20 step: 3985, loss is 0.9600306749343872\n",
            "epoch: 20 step: 3986, loss is 1.0602420568466187\n",
            "epoch: 20 step: 3987, loss is 1.0753799676895142\n",
            "epoch: 20 step: 3988, loss is 0.8214341402053833\n",
            "epoch: 20 step: 3989, loss is 0.956322193145752\n",
            "epoch: 20 step: 3990, loss is 0.8719322085380554\n",
            "epoch: 20 step: 3991, loss is 0.8655928373336792\n",
            "epoch: 20 step: 3992, loss is 0.8449795842170715\n",
            "epoch: 20 step: 3993, loss is 0.8631754517555237\n",
            "epoch: 20 step: 3994, loss is 0.7210953831672668\n",
            "epoch: 20 step: 3995, loss is 0.8942253589630127\n",
            "epoch: 20 step: 3996, loss is 0.9098232388496399\n",
            "epoch: 20 step: 3997, loss is 0.9871304631233215\n",
            "epoch: 20 step: 3998, loss is 0.9379880428314209\n",
            "epoch: 20 step: 3999, loss is 0.7572318315505981\n",
            "epoch: 20 step: 4000, loss is 0.8882585763931274\n",
            "epoch: 20 step: 4001, loss is 0.8810960054397583\n",
            "epoch: 20 step: 4002, loss is 0.716896653175354\n",
            "epoch: 20 step: 4003, loss is 0.97472083568573\n",
            "epoch: 20 step: 4004, loss is 0.8741465210914612\n",
            "epoch: 20 step: 4005, loss is 0.880449116230011\n",
            "epoch: 20 step: 4006, loss is 0.8877438306808472\n",
            "epoch: 20 step: 4007, loss is 0.8292843103408813\n",
            "epoch: 20 step: 4008, loss is 0.7985644340515137\n",
            "epoch: 20 step: 4009, loss is 0.8963638544082642\n",
            "epoch: 20 step: 4010, loss is 0.8272587656974792\n",
            "epoch: 20 step: 4011, loss is 0.8080607056617737\n",
            "epoch: 20 step: 4012, loss is 0.8874760866165161\n",
            "epoch: 20 step: 4013, loss is 0.7281659245491028\n",
            "epoch: 20 step: 4014, loss is 0.8274415731430054\n",
            "epoch: 20 step: 4015, loss is 1.105602741241455\n",
            "epoch: 20 step: 4016, loss is 0.9881072640419006\n",
            "epoch: 20 step: 4017, loss is 0.8832497596740723\n",
            "epoch: 20 step: 4018, loss is 0.7528784871101379\n",
            "epoch: 20 step: 4019, loss is 0.7978410124778748\n",
            "epoch: 20 step: 4020, loss is 0.9374707937240601\n",
            "epoch: 20 step: 4021, loss is 0.8793229460716248\n",
            "epoch: 20 step: 4022, loss is 0.8108504414558411\n",
            "epoch: 20 step: 4023, loss is 1.0091962814331055\n",
            "epoch: 20 step: 4024, loss is 0.8157610297203064\n",
            "epoch: 20 step: 4025, loss is 0.7605494856834412\n",
            "epoch: 20 step: 4026, loss is 0.939154326915741\n",
            "epoch: 20 step: 4027, loss is 0.8286958336830139\n",
            "epoch: 20 step: 4028, loss is 0.8388931751251221\n",
            "epoch: 20 step: 4029, loss is 0.7960928678512573\n",
            "epoch: 20 step: 4030, loss is 0.9474166035652161\n",
            "epoch: 20 step: 4031, loss is 1.0216690301895142\n",
            "epoch: 20 step: 4032, loss is 0.9103367328643799\n",
            "epoch: 20 step: 4033, loss is 1.0001564025878906\n",
            "epoch: 20 step: 4034, loss is 0.9664539694786072\n",
            "epoch: 20 step: 4035, loss is 0.9827585220336914\n",
            "epoch: 20 step: 4036, loss is 0.654682457447052\n",
            "epoch: 20 step: 4037, loss is 1.1497907638549805\n",
            "epoch: 20 step: 4038, loss is 0.9068971872329712\n",
            "epoch: 20 step: 4039, loss is 0.9492359161376953\n",
            "epoch: 20 step: 4040, loss is 1.0912487506866455\n",
            "epoch: 20 step: 4041, loss is 0.9749463200569153\n",
            "epoch: 20 step: 4042, loss is 0.7788126468658447\n",
            "epoch: 20 step: 4043, loss is 0.7985177040100098\n",
            "epoch: 20 step: 4044, loss is 0.8861735463142395\n",
            "epoch: 20 step: 4045, loss is 0.9178008437156677\n",
            "epoch: 20 step: 4046, loss is 0.9372634291648865\n",
            "epoch: 20 step: 4047, loss is 1.0518543720245361\n",
            "epoch: 20 step: 4048, loss is 0.819762110710144\n",
            "epoch: 20 step: 4049, loss is 0.8680880069732666\n",
            "epoch: 20 step: 4050, loss is 0.9032995104789734\n",
            "epoch: 20 step: 4051, loss is 0.9252190589904785\n",
            "epoch: 20 step: 4052, loss is 0.8327420353889465\n",
            "epoch: 20 step: 4053, loss is 0.8180018663406372\n",
            "epoch: 20 step: 4054, loss is 0.7580187320709229\n",
            "epoch: 20 step: 4055, loss is 0.8100287318229675\n",
            "epoch: 20 step: 4056, loss is 0.8317612409591675\n",
            "epoch: 20 step: 4057, loss is 0.9189507365226746\n",
            "epoch: 20 step: 4058, loss is 0.9579773545265198\n",
            "epoch: 20 step: 4059, loss is 1.0523518323898315\n",
            "epoch: 20 step: 4060, loss is 0.8922047019004822\n",
            "epoch: 20 step: 4061, loss is 0.9043407440185547\n",
            "epoch: 20 step: 4062, loss is 0.750788152217865\n",
            "epoch: 20 step: 4063, loss is 0.999657928943634\n",
            "epoch: 20 step: 4064, loss is 1.0914443731307983\n",
            "epoch: 20 step: 4065, loss is 0.8967756032943726\n",
            "epoch: 20 step: 4066, loss is 0.7830897569656372\n",
            "epoch: 20 step: 4067, loss is 0.7336915731430054\n",
            "epoch: 20 step: 4068, loss is 0.9600319266319275\n",
            "epoch: 20 step: 4069, loss is 0.7248200178146362\n",
            "epoch: 20 step: 4070, loss is 0.9503678679466248\n",
            "epoch: 20 step: 4071, loss is 0.9955204725265503\n",
            "epoch: 20 step: 4072, loss is 0.9835339784622192\n",
            "epoch: 20 step: 4073, loss is 0.8808935880661011\n",
            "epoch: 20 step: 4074, loss is 0.7649727463722229\n",
            "epoch: 20 step: 4075, loss is 0.7823342680931091\n",
            "epoch: 20 step: 4076, loss is 0.8956879377365112\n",
            "epoch: 20 step: 4077, loss is 0.9810001850128174\n",
            "epoch: 20 step: 4078, loss is 0.8867999911308289\n",
            "epoch: 20 step: 4079, loss is 1.0278183221817017\n",
            "epoch: 20 step: 4080, loss is 1.1062922477722168\n",
            "epoch: 20 step: 4081, loss is 0.8591076731681824\n",
            "epoch: 20 step: 4082, loss is 0.9407345652580261\n",
            "epoch: 20 step: 4083, loss is 0.842204213142395\n",
            "epoch: 20 step: 4084, loss is 0.8139715194702148\n",
            "epoch: 20 step: 4085, loss is 0.9148709177970886\n",
            "epoch: 20 step: 4086, loss is 0.920071005821228\n",
            "epoch: 20 step: 4087, loss is 0.683350682258606\n",
            "epoch: 20 step: 4088, loss is 1.04822838306427\n",
            "epoch: 20 step: 4089, loss is 0.7057358622550964\n",
            "epoch: 20 step: 4090, loss is 0.9797012209892273\n",
            "epoch: 20 step: 4091, loss is 0.8682721853256226\n",
            "epoch: 20 step: 4092, loss is 0.9686939716339111\n",
            "epoch: 20 step: 4093, loss is 0.8484680652618408\n",
            "epoch: 20 step: 4094, loss is 0.913179337978363\n",
            "epoch: 20 step: 4095, loss is 0.9471222758293152\n",
            "epoch: 20 step: 4096, loss is 1.1629059314727783\n",
            "epoch: 20 step: 4097, loss is 0.8755978941917419\n",
            "epoch: 20 step: 4098, loss is 0.9992967247962952\n",
            "epoch: 20 step: 4099, loss is 0.7626142501831055\n",
            "epoch: 20 step: 4100, loss is 0.8459327816963196\n",
            "epoch: 20 step: 4101, loss is 0.8288694024085999\n",
            "epoch: 20 step: 4102, loss is 0.7909867167472839\n",
            "epoch: 20 step: 4103, loss is 0.8404741287231445\n",
            "epoch: 20 step: 4104, loss is 0.8052923679351807\n",
            "epoch: 20 step: 4105, loss is 0.8027123808860779\n",
            "epoch: 20 step: 4106, loss is 0.7707873582839966\n",
            "epoch: 20 step: 4107, loss is 0.8909216523170471\n",
            "epoch: 20 step: 4108, loss is 0.9860203862190247\n",
            "epoch: 20 step: 4109, loss is 0.8776918649673462\n",
            "epoch: 20 step: 4110, loss is 0.9477835297584534\n",
            "epoch: 20 step: 4111, loss is 0.8130433559417725\n",
            "epoch: 20 step: 4112, loss is 0.888734757900238\n",
            "epoch: 20 step: 4113, loss is 0.8260030150413513\n",
            "epoch: 20 step: 4114, loss is 0.8706644177436829\n",
            "epoch: 20 step: 4115, loss is 0.7351304888725281\n",
            "epoch: 20 step: 4116, loss is 0.9167922735214233\n",
            "epoch: 20 step: 4117, loss is 0.8301635384559631\n",
            "epoch: 20 step: 4118, loss is 0.8147749304771423\n",
            "epoch: 20 step: 4119, loss is 0.7229440212249756\n",
            "epoch: 20 step: 4120, loss is 0.8320954442024231\n",
            "epoch: 20 step: 4121, loss is 0.8693455457687378\n",
            "epoch: 20 step: 4122, loss is 1.0408376455307007\n",
            "epoch: 20 step: 4123, loss is 0.8781108856201172\n",
            "epoch: 20 step: 4124, loss is 0.8539748191833496\n",
            "epoch: 20 step: 4125, loss is 0.9622765183448792\n",
            "epoch: 20 step: 4126, loss is 0.993592381477356\n",
            "epoch: 20 step: 4127, loss is 0.8076481223106384\n",
            "epoch: 20 step: 4128, loss is 0.8911541104316711\n",
            "epoch: 20 step: 4129, loss is 0.8386923670768738\n",
            "epoch: 20 step: 4130, loss is 1.0003801584243774\n",
            "epoch: 20 step: 4131, loss is 0.8884468674659729\n",
            "epoch: 20 step: 4132, loss is 1.0096549987792969\n",
            "epoch: 20 step: 4133, loss is 0.9907256364822388\n",
            "epoch: 20 step: 4134, loss is 0.9464945197105408\n",
            "epoch: 20 step: 4135, loss is 0.8552828431129456\n",
            "epoch: 20 step: 4136, loss is 0.879399836063385\n",
            "epoch: 20 step: 4137, loss is 0.7167291641235352\n",
            "epoch: 20 step: 4138, loss is 0.8681341409683228\n",
            "epoch: 20 step: 4139, loss is 0.8975380659103394\n",
            "epoch: 20 step: 4140, loss is 1.0012519359588623\n",
            "epoch: 20 step: 4141, loss is 0.724057674407959\n",
            "epoch: 20 step: 4142, loss is 0.9622379541397095\n",
            "epoch: 20 step: 4143, loss is 0.7901217341423035\n",
            "epoch: 20 step: 4144, loss is 0.993532657623291\n",
            "epoch: 20 step: 4145, loss is 0.6425653696060181\n",
            "epoch: 20 step: 4146, loss is 0.813916027545929\n",
            "epoch: 20 step: 4147, loss is 1.0582410097122192\n",
            "epoch: 20 step: 4148, loss is 0.8537737131118774\n",
            "epoch: 20 step: 4149, loss is 0.932107150554657\n",
            "epoch: 20 step: 4150, loss is 0.8998269438743591\n",
            "epoch: 20 step: 4151, loss is 0.829842209815979\n",
            "epoch: 20 step: 4152, loss is 0.8627682328224182\n",
            "epoch: 20 step: 4153, loss is 0.9113291501998901\n",
            "epoch: 20 step: 4154, loss is 0.968386173248291\n",
            "epoch: 20 step: 4155, loss is 1.0279748439788818\n",
            "epoch: 20 step: 4156, loss is 0.8702699542045593\n",
            "epoch: 20 step: 4157, loss is 1.0341873168945312\n",
            "epoch: 20 step: 4158, loss is 0.8132739067077637\n",
            "epoch: 20 step: 4159, loss is 1.0744267702102661\n",
            "epoch: 20 step: 4160, loss is 0.9720366597175598\n",
            "epoch: 20 step: 4161, loss is 0.8166732788085938\n",
            "epoch: 20 step: 4162, loss is 0.8447475433349609\n",
            "epoch: 20 step: 4163, loss is 0.901938259601593\n",
            "epoch: 20 step: 4164, loss is 1.0233176946640015\n",
            "epoch: 20 step: 4165, loss is 0.8053832054138184\n",
            "epoch: 20 step: 4166, loss is 0.7204738855361938\n",
            "epoch: 20 step: 4167, loss is 1.0611376762390137\n",
            "epoch: 20 step: 4168, loss is 0.8039352893829346\n",
            "epoch: 20 step: 4169, loss is 0.7657387256622314\n",
            "epoch: 20 step: 4170, loss is 0.8269076347351074\n",
            "epoch: 20 step: 4171, loss is 0.7946593761444092\n",
            "epoch: 20 step: 4172, loss is 0.881418764591217\n",
            "epoch: 20 step: 4173, loss is 0.8717435002326965\n",
            "epoch: 20 step: 4174, loss is 0.9585469961166382\n",
            "epoch: 20 step: 4175, loss is 0.9681893587112427\n",
            "epoch: 20 step: 4176, loss is 0.7485522031784058\n",
            "epoch: 20 step: 4177, loss is 0.8390378952026367\n",
            "epoch: 20 step: 4178, loss is 0.7981624007225037\n",
            "epoch: 20 step: 4179, loss is 0.8669034242630005\n",
            "epoch: 20 step: 4180, loss is 0.743534505367279\n",
            "epoch: 20 step: 4181, loss is 0.7245344519615173\n",
            "epoch: 20 step: 4182, loss is 0.6927798390388489\n",
            "epoch: 20 step: 4183, loss is 1.1127947568893433\n",
            "epoch: 20 step: 4184, loss is 0.8839253187179565\n",
            "epoch: 20 step: 4185, loss is 1.0435056686401367\n",
            "epoch: 20 step: 4186, loss is 0.9533578157424927\n",
            "epoch: 20 step: 4187, loss is 1.0076862573623657\n",
            "epoch: 20 step: 4188, loss is 0.7862927913665771\n",
            "epoch: 20 step: 4189, loss is 0.8382269740104675\n",
            "epoch: 20 step: 4190, loss is 0.8608455061912537\n",
            "epoch: 20 step: 4191, loss is 0.7949181199073792\n",
            "epoch: 20 step: 4192, loss is 1.0628758668899536\n",
            "epoch: 20 step: 4193, loss is 0.8485918641090393\n",
            "epoch: 20 step: 4194, loss is 0.984163224697113\n",
            "epoch: 20 step: 4195, loss is 0.8286857008934021\n",
            "epoch: 20 step: 4196, loss is 0.7248760461807251\n",
            "epoch: 20 step: 4197, loss is 0.8417328000068665\n",
            "epoch: 20 step: 4198, loss is 0.9848674535751343\n",
            "epoch: 20 step: 4199, loss is 0.76844322681427\n",
            "epoch: 20 step: 4200, loss is 0.8133025765419006\n",
            "epoch: 20 step: 4201, loss is 0.9195865988731384\n",
            "epoch: 20 step: 4202, loss is 0.7285668849945068\n",
            "epoch: 20 step: 4203, loss is 1.0259414911270142\n",
            "epoch: 20 step: 4204, loss is 1.0128170251846313\n",
            "epoch: 20 step: 4205, loss is 0.8867960572242737\n",
            "epoch: 20 step: 4206, loss is 0.7729777693748474\n",
            "epoch: 20 step: 4207, loss is 0.8724220395088196\n",
            "epoch: 20 step: 4208, loss is 0.8516903519630432\n",
            "epoch: 20 step: 4209, loss is 0.9268015027046204\n",
            "epoch: 20 step: 4210, loss is 1.1581724882125854\n",
            "epoch: 20 step: 4211, loss is 0.8330483436584473\n",
            "epoch: 20 step: 4212, loss is 0.8036395311355591\n",
            "epoch: 20 step: 4213, loss is 0.826776385307312\n",
            "epoch: 20 step: 4214, loss is 0.7234809398651123\n",
            "epoch: 20 step: 4215, loss is 0.8494666218757629\n",
            "epoch: 20 step: 4216, loss is 0.8297349214553833\n",
            "epoch: 20 step: 4217, loss is 0.8325548768043518\n",
            "epoch: 20 step: 4218, loss is 0.8673502802848816\n",
            "epoch: 20 step: 4219, loss is 0.8402796387672424\n",
            "epoch: 20 step: 4220, loss is 0.8923866152763367\n",
            "epoch: 20 step: 4221, loss is 0.7838545441627502\n",
            "epoch: 20 step: 4222, loss is 0.9422855973243713\n",
            "epoch: 20 step: 4223, loss is 0.9868607521057129\n",
            "epoch: 20 step: 4224, loss is 0.8905113935470581\n",
            "epoch: 20 step: 4225, loss is 1.009685754776001\n",
            "epoch: 20 step: 4226, loss is 0.7608157396316528\n",
            "epoch: 20 step: 4227, loss is 0.8646103739738464\n",
            "epoch: 20 step: 4228, loss is 0.979489266872406\n",
            "epoch: 20 step: 4229, loss is 0.9571447372436523\n",
            "epoch: 20 step: 4230, loss is 0.909066379070282\n",
            "epoch: 20 step: 4231, loss is 0.9629235863685608\n",
            "epoch: 20 step: 4232, loss is 0.901968240737915\n",
            "epoch: 20 step: 4233, loss is 0.763791561126709\n",
            "epoch: 20 step: 4234, loss is 0.9801394939422607\n",
            "epoch: 20 step: 4235, loss is 0.7813515663146973\n",
            "epoch: 20 step: 4236, loss is 0.932664155960083\n",
            "epoch: 20 step: 4237, loss is 0.7758795619010925\n",
            "epoch: 20 step: 4238, loss is 0.7481833696365356\n",
            "epoch: 20 step: 4239, loss is 0.8789573907852173\n",
            "epoch: 20 step: 4240, loss is 0.9882413148880005\n",
            "epoch: 20 step: 4241, loss is 0.9855458736419678\n",
            "epoch: 20 step: 4242, loss is 0.8172457814216614\n",
            "epoch: 20 step: 4243, loss is 0.7594524025917053\n",
            "epoch: 20 step: 4244, loss is 0.9596322178840637\n",
            "epoch: 20 step: 4245, loss is 0.6928882598876953\n",
            "epoch: 20 step: 4246, loss is 0.8468747138977051\n",
            "epoch: 20 step: 4247, loss is 0.849165141582489\n",
            "epoch: 20 step: 4248, loss is 0.8699362277984619\n",
            "epoch: 20 step: 4249, loss is 0.980580747127533\n",
            "epoch: 20 step: 4250, loss is 1.012597680091858\n",
            "epoch: 20 step: 4251, loss is 1.0778642892837524\n",
            "epoch: 20 step: 4252, loss is 0.6912347674369812\n",
            "epoch: 20 step: 4253, loss is 0.9353057146072388\n",
            "epoch: 20 step: 4254, loss is 0.8189670443534851\n",
            "epoch: 20 step: 4255, loss is 0.84528648853302\n",
            "epoch: 20 step: 4256, loss is 0.8397013545036316\n",
            "epoch: 20 step: 4257, loss is 0.7527068853378296\n",
            "epoch: 20 step: 4258, loss is 1.0015318393707275\n",
            "epoch: 20 step: 4259, loss is 0.8282983899116516\n",
            "epoch: 20 step: 4260, loss is 0.923186182975769\n",
            "epoch: 20 step: 4261, loss is 0.924487292766571\n",
            "epoch: 20 step: 4262, loss is 0.7390710115432739\n",
            "epoch: 20 step: 4263, loss is 0.8492118120193481\n",
            "epoch: 20 step: 4264, loss is 1.0171964168548584\n",
            "epoch: 20 step: 4265, loss is 0.8305139541625977\n",
            "epoch: 20 step: 4266, loss is 1.0971509218215942\n",
            "epoch: 20 step: 4267, loss is 0.7962480187416077\n",
            "epoch: 20 step: 4268, loss is 0.858134925365448\n",
            "epoch: 20 step: 4269, loss is 0.6932200789451599\n",
            "epoch: 20 step: 4270, loss is 0.9546645879745483\n",
            "epoch: 20 step: 4271, loss is 0.9296588897705078\n",
            "epoch: 20 step: 4272, loss is 0.8233059644699097\n",
            "epoch: 20 step: 4273, loss is 1.0901697874069214\n",
            "epoch: 20 step: 4274, loss is 0.8727741837501526\n",
            "epoch: 20 step: 4275, loss is 0.7520168423652649\n",
            "epoch: 20 step: 4276, loss is 0.9454550743103027\n",
            "epoch: 20 step: 4277, loss is 1.1642998456954956\n",
            "epoch: 20 step: 4278, loss is 0.9058298468589783\n",
            "epoch: 20 step: 4279, loss is 0.8560492396354675\n",
            "epoch: 20 step: 4280, loss is 0.9063251614570618\n",
            "epoch: 20 step: 4281, loss is 0.8305546045303345\n",
            "epoch: 20 step: 4282, loss is 1.113830804824829\n",
            "epoch: 20 step: 4283, loss is 1.026129126548767\n",
            "epoch: 20 step: 4284, loss is 1.1173018217086792\n",
            "epoch: 20 step: 4285, loss is 0.8228437304496765\n",
            "epoch: 20 step: 4286, loss is 1.1226528882980347\n",
            "epoch: 20 step: 4287, loss is 0.8918755054473877\n",
            "epoch: 20 step: 4288, loss is 1.0333048105239868\n",
            "epoch: 20 step: 4289, loss is 0.7699467539787292\n",
            "epoch: 20 step: 4290, loss is 1.0777283906936646\n",
            "epoch: 20 step: 4291, loss is 0.685920000076294\n",
            "epoch: 20 step: 4292, loss is 0.9484612345695496\n",
            "epoch: 20 step: 4293, loss is 0.890781819820404\n",
            "epoch: 20 step: 4294, loss is 0.9000012874603271\n",
            "epoch: 20 step: 4295, loss is 0.8884902000427246\n",
            "epoch: 20 step: 4296, loss is 0.6849554181098938\n",
            "epoch: 20 step: 4297, loss is 0.9181756973266602\n",
            "epoch: 20 step: 4298, loss is 1.0269057750701904\n",
            "epoch: 20 step: 4299, loss is 0.7516722083091736\n",
            "epoch: 20 step: 4300, loss is 0.828341543674469\n",
            "epoch: 20 step: 4301, loss is 0.8918799161911011\n",
            "epoch: 20 step: 4302, loss is 0.8315128684043884\n",
            "epoch: 20 step: 4303, loss is 0.9528826475143433\n",
            "epoch: 20 step: 4304, loss is 0.8410427570343018\n",
            "epoch: 20 step: 4305, loss is 0.8399416208267212\n",
            "epoch: 20 step: 4306, loss is 0.7712389826774597\n",
            "epoch: 20 step: 4307, loss is 0.9117944240570068\n",
            "epoch: 20 step: 4308, loss is 0.8181570768356323\n",
            "epoch: 20 step: 4309, loss is 0.806458592414856\n",
            "epoch: 20 step: 4310, loss is 0.9782914519309998\n",
            "epoch: 20 step: 4311, loss is 0.8767426609992981\n",
            "epoch: 20 step: 4312, loss is 0.8824710845947266\n",
            "epoch: 20 step: 4313, loss is 0.6820775866508484\n",
            "epoch: 20 step: 4314, loss is 0.8297600150108337\n",
            "epoch: 20 step: 4315, loss is 0.7994388341903687\n",
            "epoch: 20 step: 4316, loss is 0.7670900225639343\n",
            "epoch: 20 step: 4317, loss is 0.8277081847190857\n",
            "epoch: 20 step: 4318, loss is 0.8068240880966187\n",
            "epoch: 20 step: 4319, loss is 0.92873615026474\n",
            "epoch: 20 step: 4320, loss is 0.9949786067008972\n",
            "epoch: 20 step: 4321, loss is 0.9955453872680664\n",
            "epoch: 20 step: 4322, loss is 0.7665513753890991\n",
            "epoch: 20 step: 4323, loss is 0.7659399509429932\n",
            "epoch: 20 step: 4324, loss is 0.8524015545845032\n",
            "epoch: 20 step: 4325, loss is 0.9153176546096802\n",
            "epoch: 20 step: 4326, loss is 0.9768043756484985\n",
            "epoch: 20 step: 4327, loss is 1.0304871797561646\n",
            "epoch: 20 step: 4328, loss is 0.7127057909965515\n",
            "epoch: 20 step: 4329, loss is 1.0263283252716064\n",
            "epoch: 20 step: 4330, loss is 0.9572098255157471\n",
            "epoch: 20 step: 4331, loss is 0.943292498588562\n",
            "epoch: 20 step: 4332, loss is 0.7378023862838745\n",
            "epoch: 20 step: 4333, loss is 1.102968454360962\n",
            "epoch: 20 step: 4334, loss is 1.0370186567306519\n",
            "epoch: 20 step: 4335, loss is 1.0367391109466553\n",
            "epoch: 20 step: 4336, loss is 0.9228189587593079\n",
            "epoch: 20 step: 4337, loss is 0.9046153426170349\n",
            "epoch: 20 step: 4338, loss is 0.8881568312644958\n",
            "epoch: 20 step: 4339, loss is 0.7168974280357361\n",
            "epoch: 20 step: 4340, loss is 0.7392860651016235\n",
            "epoch: 20 step: 4341, loss is 0.7868933081626892\n",
            "epoch: 20 step: 4342, loss is 0.7703403830528259\n",
            "epoch: 20 step: 4343, loss is 0.927069902420044\n",
            "epoch: 20 step: 4344, loss is 0.7513032555580139\n",
            "epoch: 20 step: 4345, loss is 0.9212225079536438\n",
            "epoch: 20 step: 4346, loss is 0.824901819229126\n",
            "epoch: 20 step: 4347, loss is 0.836706280708313\n",
            "epoch: 20 step: 4348, loss is 1.0381522178649902\n",
            "epoch: 20 step: 4349, loss is 0.8355743885040283\n",
            "epoch: 20 step: 4350, loss is 0.7970791459083557\n",
            "epoch: 20 step: 4351, loss is 0.8409313559532166\n",
            "epoch: 20 step: 4352, loss is 0.848185122013092\n",
            "epoch: 20 step: 4353, loss is 0.9758533835411072\n",
            "epoch: 20 step: 4354, loss is 0.9014440774917603\n",
            "epoch: 20 step: 4355, loss is 1.121332049369812\n",
            "epoch: 20 step: 4356, loss is 0.9859436750411987\n",
            "epoch: 20 step: 4357, loss is 0.8037422299385071\n",
            "epoch: 20 step: 4358, loss is 0.9343576431274414\n",
            "epoch: 20 step: 4359, loss is 0.9281687140464783\n",
            "epoch: 20 step: 4360, loss is 0.8067159056663513\n",
            "epoch: 20 step: 4361, loss is 0.699444055557251\n",
            "epoch: 20 step: 4362, loss is 0.8555514216423035\n",
            "epoch: 20 step: 4363, loss is 0.8873788118362427\n",
            "epoch: 20 step: 4364, loss is 0.7559843063354492\n",
            "epoch: 20 step: 4365, loss is 0.8783809542655945\n",
            "epoch: 20 step: 4366, loss is 0.8690188527107239\n",
            "epoch: 20 step: 4367, loss is 0.8469496965408325\n",
            "epoch: 20 step: 4368, loss is 0.9599562883377075\n",
            "epoch: 20 step: 4369, loss is 0.8311728835105896\n",
            "epoch: 20 step: 4370, loss is 0.7912710309028625\n",
            "epoch: 20 step: 4371, loss is 0.9244258999824524\n",
            "epoch: 20 step: 4372, loss is 0.8224567770957947\n",
            "epoch: 20 step: 4373, loss is 0.7847309112548828\n",
            "epoch: 20 step: 4374, loss is 0.9130398035049438\n",
            "epoch: 20 step: 4375, loss is 0.8471923470497131\n",
            "epoch: 20 step: 4376, loss is 0.9900200366973877\n",
            "epoch: 20 step: 4377, loss is 0.8810612559318542\n",
            "epoch: 20 step: 4378, loss is 0.8754798769950867\n",
            "epoch: 20 step: 4379, loss is 0.9567815065383911\n",
            "epoch: 20 step: 4380, loss is 0.8203542232513428\n",
            "epoch: 20 step: 4381, loss is 0.7571638822555542\n",
            "epoch: 20 step: 4382, loss is 0.7232627272605896\n",
            "epoch: 20 step: 4383, loss is 0.9051322340965271\n",
            "epoch: 20 step: 4384, loss is 0.8936123847961426\n",
            "epoch: 20 step: 4385, loss is 0.9737944602966309\n",
            "epoch: 20 step: 4386, loss is 0.9734192490577698\n",
            "epoch: 20 step: 4387, loss is 0.991284191608429\n",
            "epoch: 20 step: 4388, loss is 1.107867956161499\n",
            "epoch: 20 step: 4389, loss is 0.8898065090179443\n",
            "epoch: 20 step: 4390, loss is 0.7736698389053345\n",
            "epoch: 20 step: 4391, loss is 0.9964086413383484\n",
            "epoch: 20 step: 4392, loss is 0.858694851398468\n",
            "epoch: 20 step: 4393, loss is 0.8377456665039062\n",
            "epoch: 20 step: 4394, loss is 0.8298120498657227\n",
            "epoch: 20 step: 4395, loss is 0.8794101476669312\n",
            "epoch: 20 step: 4396, loss is 0.9198653101921082\n",
            "epoch: 20 step: 4397, loss is 0.9426816701889038\n",
            "epoch: 20 step: 4398, loss is 0.9097484350204468\n",
            "epoch: 20 step: 4399, loss is 0.8113874197006226\n",
            "epoch: 20 step: 4400, loss is 0.9963026642799377\n",
            "epoch: 20 step: 4401, loss is 0.943498969078064\n",
            "epoch: 20 step: 4402, loss is 0.9731379747390747\n",
            "epoch: 20 step: 4403, loss is 0.8100335001945496\n",
            "epoch: 20 step: 4404, loss is 1.0741349458694458\n",
            "epoch: 20 step: 4405, loss is 1.0437496900558472\n",
            "epoch: 20 step: 4406, loss is 0.8339768648147583\n",
            "epoch: 20 step: 4407, loss is 0.8719176054000854\n",
            "epoch: 20 step: 4408, loss is 0.9101949334144592\n",
            "epoch: 20 step: 4409, loss is 1.0873498916625977\n",
            "epoch: 20 step: 4410, loss is 0.7596351504325867\n",
            "epoch: 20 step: 4411, loss is 0.8331366777420044\n",
            "epoch: 20 step: 4412, loss is 0.8370264172554016\n",
            "epoch: 20 step: 4413, loss is 0.8905619978904724\n",
            "epoch: 20 step: 4414, loss is 0.8083511590957642\n",
            "epoch: 20 step: 4415, loss is 1.0115766525268555\n",
            "epoch: 20 step: 4416, loss is 0.9195004105567932\n",
            "epoch: 20 step: 4417, loss is 0.7666065692901611\n",
            "epoch: 20 step: 4418, loss is 0.7986389398574829\n",
            "epoch: 20 step: 4419, loss is 0.9176926612854004\n",
            "epoch: 20 step: 4420, loss is 0.7473080158233643\n",
            "epoch: 20 step: 4421, loss is 0.9055749773979187\n",
            "epoch: 20 step: 4422, loss is 0.9469279050827026\n",
            "epoch: 20 step: 4423, loss is 0.9913232922554016\n",
            "epoch: 20 step: 4424, loss is 0.9902453422546387\n",
            "epoch: 20 step: 4425, loss is 1.0284651517868042\n",
            "epoch: 20 step: 4426, loss is 1.1283860206604004\n",
            "epoch: 20 step: 4427, loss is 0.8847209215164185\n",
            "epoch: 20 step: 4428, loss is 0.7631860971450806\n",
            "epoch: 20 step: 4429, loss is 0.8350404500961304\n",
            "epoch: 20 step: 4430, loss is 0.8115922808647156\n",
            "epoch: 20 step: 4431, loss is 0.8788152933120728\n",
            "epoch: 20 step: 4432, loss is 0.977128803730011\n",
            "epoch: 20 step: 4433, loss is 0.9594988226890564\n",
            "epoch: 20 step: 4434, loss is 0.7946954965591431\n",
            "epoch: 20 step: 4435, loss is 0.8633144497871399\n",
            "epoch: 20 step: 4436, loss is 0.9037508964538574\n",
            "epoch: 20 step: 4437, loss is 0.7758119106292725\n",
            "epoch: 20 step: 4438, loss is 0.818264901638031\n",
            "epoch: 20 step: 4439, loss is 0.8281958103179932\n",
            "epoch: 20 step: 4440, loss is 0.8045369982719421\n",
            "epoch: 20 step: 4441, loss is 0.8066900372505188\n",
            "epoch: 20 step: 4442, loss is 0.8156613111495972\n",
            "epoch: 20 step: 4443, loss is 0.7895761132240295\n",
            "epoch: 20 step: 4444, loss is 0.955153226852417\n",
            "epoch: 20 step: 4445, loss is 0.8687887191772461\n",
            "epoch: 20 step: 4446, loss is 0.8368215560913086\n",
            "epoch: 20 step: 4447, loss is 0.8827758431434631\n",
            "epoch: 20 step: 4448, loss is 0.9732128381729126\n",
            "epoch: 20 step: 4449, loss is 0.8815546035766602\n",
            "epoch: 20 step: 4450, loss is 0.992637574672699\n",
            "epoch: 20 step: 4451, loss is 0.8901112675666809\n",
            "epoch: 20 step: 4452, loss is 1.0842666625976562\n",
            "epoch: 20 step: 4453, loss is 0.9246538877487183\n",
            "epoch: 20 step: 4454, loss is 0.8359479904174805\n",
            "epoch: 20 step: 4455, loss is 0.8160377740859985\n",
            "epoch: 20 step: 4456, loss is 0.9869788289070129\n",
            "epoch: 20 step: 4457, loss is 0.9009308815002441\n",
            "epoch: 20 step: 4458, loss is 0.9901293516159058\n",
            "epoch: 20 step: 4459, loss is 0.8999672532081604\n",
            "epoch: 20 step: 4460, loss is 0.9997254610061646\n",
            "epoch: 20 step: 4461, loss is 1.0075042247772217\n",
            "epoch: 20 step: 4462, loss is 0.8613768219947815\n",
            "epoch: 20 step: 4463, loss is 0.9894641041755676\n",
            "epoch: 20 step: 4464, loss is 1.0347542762756348\n",
            "epoch: 20 step: 4465, loss is 0.942070722579956\n",
            "epoch: 20 step: 4466, loss is 0.7943304181098938\n",
            "epoch: 20 step: 4467, loss is 0.7447556853294373\n",
            "epoch: 20 step: 4468, loss is 0.8365554213523865\n",
            "epoch: 20 step: 4469, loss is 0.7553564310073853\n",
            "epoch: 20 step: 4470, loss is 1.0176012516021729\n",
            "epoch: 20 step: 4471, loss is 0.805328369140625\n",
            "epoch: 20 step: 4472, loss is 0.9097049236297607\n",
            "epoch: 20 step: 4473, loss is 0.8986660242080688\n",
            "epoch: 20 step: 4474, loss is 0.7412773966789246\n",
            "epoch: 20 step: 4475, loss is 0.7713987827301025\n",
            "epoch: 20 step: 4476, loss is 0.7755609750747681\n",
            "epoch: 20 step: 4477, loss is 0.9253298044204712\n",
            "epoch: 20 step: 4478, loss is 1.037163496017456\n",
            "epoch: 20 step: 4479, loss is 0.8355528116226196\n",
            "epoch: 20 step: 4480, loss is 0.806498646736145\n",
            "epoch: 20 step: 4481, loss is 0.9619511365890503\n",
            "epoch: 20 step: 4482, loss is 0.9702526926994324\n",
            "epoch: 20 step: 4483, loss is 0.8888569474220276\n",
            "epoch: 20 step: 4484, loss is 0.8553541302680969\n",
            "epoch: 20 step: 4485, loss is 0.8649570345878601\n",
            "epoch: 20 step: 4486, loss is 0.7369710803031921\n",
            "epoch: 20 step: 4487, loss is 0.8523731231689453\n",
            "epoch: 20 step: 4488, loss is 0.8418484926223755\n",
            "epoch: 20 step: 4489, loss is 0.9819101095199585\n",
            "epoch: 20 step: 4490, loss is 0.84621262550354\n",
            "epoch: 20 step: 4491, loss is 1.0117918252944946\n",
            "epoch: 20 step: 4492, loss is 0.7514607906341553\n",
            "epoch: 20 step: 4493, loss is 0.9640615582466125\n",
            "epoch: 20 step: 4494, loss is 0.9427246451377869\n",
            "epoch: 20 step: 4495, loss is 0.9116450548171997\n",
            "epoch: 20 step: 4496, loss is 0.9539384245872498\n",
            "epoch: 20 step: 4497, loss is 0.814845860004425\n",
            "epoch: 20 step: 4498, loss is 0.997184157371521\n",
            "epoch: 20 step: 4499, loss is 1.0805147886276245\n",
            "epoch: 20 step: 4500, loss is 0.8534601330757141\n",
            "epoch: 20 step: 4501, loss is 0.8828790187835693\n",
            "epoch: 20 step: 4502, loss is 1.0550540685653687\n",
            "epoch: 20 step: 4503, loss is 0.8431739807128906\n",
            "epoch: 20 step: 4504, loss is 0.9658690094947815\n",
            "epoch: 20 step: 4505, loss is 0.8055610656738281\n",
            "epoch: 20 step: 4506, loss is 1.045746922492981\n",
            "epoch: 20 step: 4507, loss is 0.9053608775138855\n",
            "epoch: 20 step: 4508, loss is 0.7239585518836975\n",
            "epoch: 20 step: 4509, loss is 1.0826183557510376\n",
            "epoch: 20 step: 4510, loss is 0.796489417552948\n",
            "epoch: 20 step: 4511, loss is 0.8231779336929321\n",
            "epoch: 20 step: 4512, loss is 0.9957648515701294\n",
            "epoch: 20 step: 4513, loss is 0.9406927227973938\n",
            "epoch: 20 step: 4514, loss is 0.9273485541343689\n",
            "epoch: 20 step: 4515, loss is 0.9180665612220764\n",
            "epoch: 20 step: 4516, loss is 0.9766459465026855\n",
            "epoch: 20 step: 4517, loss is 0.8095094561576843\n",
            "epoch: 20 step: 4518, loss is 0.9380756616592407\n",
            "epoch: 20 step: 4519, loss is 1.0716304779052734\n",
            "epoch: 20 step: 4520, loss is 1.0673611164093018\n",
            "epoch: 20 step: 4521, loss is 0.6895664930343628\n",
            "epoch: 20 step: 4522, loss is 0.8809987902641296\n",
            "epoch: 20 step: 4523, loss is 0.7901641726493835\n",
            "epoch: 20 step: 4524, loss is 0.9176083207130432\n",
            "epoch: 20 step: 4525, loss is 0.9413902163505554\n",
            "epoch: 20 step: 4526, loss is 0.8976991176605225\n",
            "epoch: 20 step: 4527, loss is 0.8377482295036316\n",
            "epoch: 20 step: 4528, loss is 0.8061438202857971\n",
            "epoch: 20 step: 4529, loss is 0.9076414108276367\n",
            "epoch: 20 step: 4530, loss is 0.7622020244598389\n",
            "epoch: 20 step: 4531, loss is 0.9208122491836548\n",
            "epoch: 20 step: 4532, loss is 0.8900966048240662\n",
            "epoch: 20 step: 4533, loss is 0.8750550150871277\n",
            "epoch: 20 step: 4534, loss is 0.8227962255477905\n",
            "epoch: 20 step: 4535, loss is 0.9120175838470459\n",
            "epoch: 20 step: 4536, loss is 0.9168363213539124\n",
            "epoch: 20 step: 4537, loss is 0.927691638469696\n",
            "epoch: 20 step: 4538, loss is 0.848335325717926\n",
            "epoch: 20 step: 4539, loss is 1.1974895000457764\n",
            "epoch: 20 step: 4540, loss is 0.8307421207427979\n",
            "epoch: 20 step: 4541, loss is 0.87337327003479\n",
            "epoch: 20 step: 4542, loss is 0.6158909201622009\n",
            "epoch: 20 step: 4543, loss is 1.0879275798797607\n",
            "epoch: 20 step: 4544, loss is 0.9660826921463013\n",
            "epoch: 20 step: 4545, loss is 0.906261682510376\n",
            "epoch: 20 step: 4546, loss is 0.9679161906242371\n",
            "epoch: 20 step: 4547, loss is 0.9010908007621765\n",
            "epoch: 20 step: 4548, loss is 0.9758025407791138\n",
            "epoch: 20 step: 4549, loss is 0.7906953692436218\n",
            "epoch: 20 step: 4550, loss is 0.8376684188842773\n",
            "epoch: 20 step: 4551, loss is 1.0751254558563232\n",
            "epoch: 20 step: 4552, loss is 0.9539066553115845\n",
            "epoch: 20 step: 4553, loss is 0.8580390810966492\n",
            "epoch: 20 step: 4554, loss is 0.9842512011528015\n",
            "epoch: 20 step: 4555, loss is 0.8083633184432983\n",
            "epoch: 20 step: 4556, loss is 0.9190267324447632\n",
            "epoch: 20 step: 4557, loss is 1.0339648723602295\n",
            "epoch: 20 step: 4558, loss is 0.9403206706047058\n",
            "epoch: 20 step: 4559, loss is 0.8586460947990417\n",
            "epoch: 20 step: 4560, loss is 0.9562884569168091\n",
            "epoch: 20 step: 4561, loss is 1.0927172899246216\n",
            "epoch: 20 step: 4562, loss is 0.8369833827018738\n",
            "epoch: 20 step: 4563, loss is 0.799727737903595\n",
            "epoch: 20 step: 4564, loss is 0.9489009380340576\n",
            "epoch: 20 step: 4565, loss is 0.8298627734184265\n",
            "epoch: 20 step: 4566, loss is 0.8623270392417908\n",
            "epoch: 20 step: 4567, loss is 0.8347160220146179\n",
            "epoch: 20 step: 4568, loss is 0.8132845163345337\n",
            "epoch: 20 step: 4569, loss is 0.98350989818573\n",
            "epoch: 20 step: 4570, loss is 0.9603685140609741\n",
            "epoch: 20 step: 4571, loss is 0.7995958924293518\n",
            "epoch: 20 step: 4572, loss is 0.8531982898712158\n",
            "epoch: 20 step: 4573, loss is 0.6716864705085754\n",
            "epoch: 20 step: 4574, loss is 0.8735519051551819\n",
            "epoch: 20 step: 4575, loss is 0.8802460432052612\n",
            "epoch: 20 step: 4576, loss is 0.9690908789634705\n",
            "epoch: 20 step: 4577, loss is 0.9337186813354492\n",
            "epoch: 20 step: 4578, loss is 0.9590379595756531\n",
            "epoch: 20 step: 4579, loss is 1.0140142440795898\n",
            "epoch: 20 step: 4580, loss is 0.7288667559623718\n",
            "epoch: 20 step: 4581, loss is 1.0850331783294678\n",
            "epoch: 20 step: 4582, loss is 0.8388708829879761\n",
            "epoch: 20 step: 4583, loss is 1.0090892314910889\n",
            "epoch: 20 step: 4584, loss is 0.8280647397041321\n",
            "epoch: 20 step: 4585, loss is 0.9299743175506592\n",
            "epoch: 20 step: 4586, loss is 0.9348394274711609\n",
            "epoch: 20 step: 4587, loss is 0.8182733058929443\n",
            "epoch: 20 step: 4588, loss is 0.9057279825210571\n",
            "epoch: 20 step: 4589, loss is 0.9247030019760132\n",
            "epoch: 20 step: 4590, loss is 0.8864765167236328\n",
            "epoch: 20 step: 4591, loss is 0.9301158785820007\n",
            "epoch: 20 step: 4592, loss is 0.942979633808136\n",
            "epoch: 20 step: 4593, loss is 0.959766149520874\n",
            "epoch: 20 step: 4594, loss is 0.8499453067779541\n",
            "epoch: 20 step: 4595, loss is 0.7686580419540405\n",
            "epoch: 20 step: 4596, loss is 0.9697973132133484\n",
            "epoch: 20 step: 4597, loss is 0.8912060856819153\n",
            "epoch: 20 step: 4598, loss is 0.8679406642913818\n",
            "epoch: 20 step: 4599, loss is 1.0203649997711182\n",
            "epoch: 20 step: 4600, loss is 0.792208194732666\n",
            "epoch: 20 step: 4601, loss is 0.8664783835411072\n",
            "epoch: 20 step: 4602, loss is 0.9518389105796814\n",
            "epoch: 20 step: 4603, loss is 0.8166977167129517\n",
            "epoch: 20 step: 4604, loss is 0.9522920250892639\n",
            "epoch: 20 step: 4605, loss is 0.8228471279144287\n",
            "epoch: 20 step: 4606, loss is 0.9814298748970032\n",
            "epoch: 20 step: 4607, loss is 0.7885093688964844\n",
            "epoch: 20 step: 4608, loss is 1.0934922695159912\n",
            "epoch: 20 step: 4609, loss is 0.963001549243927\n",
            "epoch: 20 step: 4610, loss is 0.9762707352638245\n",
            "epoch: 20 step: 4611, loss is 1.142327904701233\n",
            "epoch: 20 step: 4612, loss is 1.0254104137420654\n",
            "epoch: 20 step: 4613, loss is 1.0485308170318604\n",
            "epoch: 20 step: 4614, loss is 0.7873502373695374\n",
            "epoch: 20 step: 4615, loss is 0.794580340385437\n",
            "epoch: 20 step: 4616, loss is 0.9333966374397278\n",
            "epoch: 20 step: 4617, loss is 0.8475061058998108\n",
            "epoch: 20 step: 4618, loss is 1.236308217048645\n",
            "epoch: 20 step: 4619, loss is 1.031720519065857\n",
            "epoch: 20 step: 4620, loss is 0.8041155934333801\n",
            "epoch: 20 step: 4621, loss is 1.0343998670578003\n",
            "epoch: 20 step: 4622, loss is 0.8828454613685608\n",
            "epoch: 20 step: 4623, loss is 1.0441786050796509\n",
            "epoch: 20 step: 4624, loss is 0.8633345365524292\n",
            "epoch: 20 step: 4625, loss is 0.8481850624084473\n",
            "epoch: 20 step: 4626, loss is 0.8299036026000977\n",
            "epoch: 20 step: 4627, loss is 1.0447170734405518\n",
            "epoch: 20 step: 4628, loss is 0.9129030108451843\n",
            "epoch: 20 step: 4629, loss is 0.7576671838760376\n",
            "epoch: 20 step: 4630, loss is 0.8196071982383728\n",
            "epoch: 20 step: 4631, loss is 0.8785542845726013\n",
            "epoch: 20 step: 4632, loss is 0.8717108964920044\n",
            "epoch: 20 step: 4633, loss is 0.8694090247154236\n",
            "epoch: 20 step: 4634, loss is 0.7579603791236877\n",
            "epoch: 20 step: 4635, loss is 0.9449987411499023\n",
            "epoch: 20 step: 4636, loss is 1.0344136953353882\n",
            "epoch: 20 step: 4637, loss is 0.9759578108787537\n",
            "epoch: 20 step: 4638, loss is 0.799488365650177\n",
            "epoch: 20 step: 4639, loss is 0.9628405570983887\n",
            "epoch: 20 step: 4640, loss is 0.8739179372787476\n",
            "epoch: 20 step: 4641, loss is 0.6989025473594666\n",
            "epoch: 20 step: 4642, loss is 0.9127106666564941\n",
            "epoch: 20 step: 4643, loss is 0.9845170974731445\n",
            "epoch: 20 step: 4644, loss is 0.8361015915870667\n",
            "epoch: 20 step: 4645, loss is 0.8634821772575378\n",
            "epoch: 20 step: 4646, loss is 1.08317232131958\n",
            "epoch: 20 step: 4647, loss is 0.7262982726097107\n",
            "epoch: 20 step: 4648, loss is 0.7945992946624756\n",
            "epoch: 20 step: 4649, loss is 0.8152284026145935\n",
            "epoch: 20 step: 4650, loss is 0.7705881595611572\n",
            "epoch: 20 step: 4651, loss is 1.0133131742477417\n",
            "epoch: 20 step: 4652, loss is 0.8801509141921997\n",
            "epoch: 20 step: 4653, loss is 0.8261182904243469\n",
            "epoch: 20 step: 4654, loss is 0.7705121636390686\n",
            "epoch: 20 step: 4655, loss is 0.9800589084625244\n",
            "epoch: 20 step: 4656, loss is 0.8741638660430908\n",
            "epoch: 20 step: 4657, loss is 0.9141982197761536\n",
            "epoch: 20 step: 4658, loss is 0.9659261703491211\n",
            "epoch: 20 step: 4659, loss is 0.8189941644668579\n",
            "epoch: 20 step: 4660, loss is 0.9189432263374329\n",
            "epoch: 20 step: 4661, loss is 0.8153518438339233\n",
            "epoch: 20 step: 4662, loss is 0.9031285643577576\n",
            "epoch: 20 step: 4663, loss is 0.7293146848678589\n",
            "epoch: 20 step: 4664, loss is 0.7498578429222107\n",
            "epoch: 20 step: 4665, loss is 0.8354973196983337\n",
            "epoch: 20 step: 4666, loss is 0.75112384557724\n",
            "epoch: 20 step: 4667, loss is 0.9793696999549866\n",
            "epoch: 20 step: 4668, loss is 0.9755425453186035\n",
            "epoch: 20 step: 4669, loss is 1.0557692050933838\n",
            "epoch: 20 step: 4670, loss is 0.9562363028526306\n",
            "epoch: 20 step: 4671, loss is 0.7480008602142334\n",
            "epoch: 20 step: 4672, loss is 0.8714635968208313\n",
            "epoch: 20 step: 4673, loss is 0.9302944540977478\n",
            "epoch: 20 step: 4674, loss is 0.8292741775512695\n",
            "epoch: 20 step: 4675, loss is 0.8744025230407715\n",
            "epoch: 20 step: 4676, loss is 0.8597110509872437\n",
            "epoch: 20 step: 4677, loss is 0.9856105446815491\n",
            "epoch: 20 step: 4678, loss is 0.9978513717651367\n",
            "epoch: 20 step: 4679, loss is 0.9504222273826599\n",
            "epoch: 20 step: 4680, loss is 0.8787394165992737\n",
            "epoch: 20 step: 4681, loss is 0.8756281733512878\n",
            "epoch: 20 step: 4682, loss is 0.8441208600997925\n",
            "epoch: 20 step: 4683, loss is 0.9040893912315369\n",
            "epoch: 20 step: 4684, loss is 0.7354676723480225\n",
            "epoch: 20 step: 4685, loss is 0.9563693404197693\n",
            "epoch: 20 step: 4686, loss is 0.866278886795044\n",
            "epoch: 20 step: 4687, loss is 0.9583922624588013\n",
            "epoch: 20 step: 4688, loss is 0.8308732509613037\n",
            "epoch: 20 step: 4689, loss is 0.7752094268798828\n",
            "epoch: 20 step: 4690, loss is 0.9278600215911865\n",
            "epoch: 20 step: 4691, loss is 0.8437597155570984\n",
            "epoch: 20 step: 4692, loss is 0.9096974730491638\n",
            "epoch: 20 step: 4693, loss is 0.8009730577468872\n",
            "epoch: 20 step: 4694, loss is 0.8181023597717285\n",
            "epoch: 20 step: 4695, loss is 0.8310751914978027\n",
            "epoch: 20 step: 4696, loss is 0.7593370676040649\n",
            "epoch: 20 step: 4697, loss is 0.8249168992042542\n",
            "epoch: 20 step: 4698, loss is 0.9612920880317688\n",
            "epoch: 20 step: 4699, loss is 0.7979742288589478\n",
            "epoch: 20 step: 4700, loss is 0.8000466823577881\n",
            "epoch: 20 step: 4701, loss is 0.9001458883285522\n",
            "epoch: 20 step: 4702, loss is 1.0117360353469849\n",
            "epoch: 20 step: 4703, loss is 0.9485895037651062\n",
            "epoch: 20 step: 4704, loss is 0.9946920275688171\n",
            "epoch: 20 step: 4705, loss is 0.9567642211914062\n",
            "epoch: 20 step: 4706, loss is 0.7288501858711243\n",
            "epoch: 20 step: 4707, loss is 0.979476809501648\n",
            "epoch: 20 step: 4708, loss is 0.7894678115844727\n",
            "epoch: 20 step: 4709, loss is 0.8119967579841614\n",
            "epoch: 20 step: 4710, loss is 0.7041768431663513\n",
            "epoch: 20 step: 4711, loss is 0.817266047000885\n",
            "epoch: 20 step: 4712, loss is 0.9094658493995667\n",
            "epoch: 20 step: 4713, loss is 0.7329108119010925\n",
            "epoch: 20 step: 4714, loss is 0.8701378107070923\n",
            "epoch: 20 step: 4715, loss is 0.8498319983482361\n",
            "epoch: 20 step: 4716, loss is 0.8913958668708801\n",
            "epoch: 20 step: 4717, loss is 0.7736666202545166\n",
            "epoch: 20 step: 4718, loss is 0.8849297165870667\n",
            "epoch: 20 step: 4719, loss is 0.9405842423439026\n",
            "epoch: 20 step: 4720, loss is 0.7828474044799805\n",
            "epoch: 20 step: 4721, loss is 0.7854719161987305\n",
            "epoch: 20 step: 4722, loss is 0.8062873482704163\n",
            "epoch: 20 step: 4723, loss is 1.0316792726516724\n",
            "epoch: 20 step: 4724, loss is 0.8452996015548706\n",
            "epoch: 20 step: 4725, loss is 0.9791144132614136\n",
            "epoch: 20 step: 4726, loss is 0.9048342108726501\n",
            "epoch: 20 step: 4727, loss is 0.8269920945167542\n",
            "epoch: 20 step: 4728, loss is 0.8142333626747131\n",
            "epoch: 20 step: 4729, loss is 0.7846429347991943\n",
            "epoch: 20 step: 4730, loss is 0.9454889297485352\n",
            "epoch: 20 step: 4731, loss is 1.013871431350708\n",
            "epoch: 20 step: 4732, loss is 0.9774816632270813\n",
            "epoch: 20 step: 4733, loss is 0.8244166374206543\n",
            "epoch: 20 step: 4734, loss is 1.1132420301437378\n",
            "epoch: 20 step: 4735, loss is 0.8288204073905945\n",
            "epoch: 20 step: 4736, loss is 0.7292537093162537\n",
            "epoch: 20 step: 4737, loss is 0.8712230920791626\n",
            "epoch: 20 step: 4738, loss is 0.8774686455726624\n",
            "epoch: 20 step: 4739, loss is 0.7392463684082031\n",
            "epoch: 20 step: 4740, loss is 0.758136510848999\n",
            "epoch: 20 step: 4741, loss is 0.7796533703804016\n",
            "epoch: 20 step: 4742, loss is 0.6913599967956543\n",
            "epoch: 20 step: 4743, loss is 0.8423258662223816\n",
            "epoch: 20 step: 4744, loss is 0.8906817436218262\n",
            "epoch: 20 step: 4745, loss is 0.9167326092720032\n",
            "epoch: 20 step: 4746, loss is 0.9383783936500549\n",
            "epoch: 20 step: 4747, loss is 0.672352135181427\n",
            "epoch: 20 step: 4748, loss is 0.8401545882225037\n",
            "epoch: 20 step: 4749, loss is 1.1400272846221924\n",
            "epoch: 20 step: 4750, loss is 0.9608674645423889\n",
            "epoch: 20 step: 4751, loss is 0.8791298866271973\n",
            "epoch: 20 step: 4752, loss is 0.8028326630592346\n",
            "epoch: 20 step: 4753, loss is 0.891566276550293\n",
            "epoch: 20 step: 4754, loss is 0.8448417782783508\n",
            "epoch: 20 step: 4755, loss is 0.8561388254165649\n",
            "epoch: 20 step: 4756, loss is 1.0652477741241455\n",
            "epoch: 20 step: 4757, loss is 0.976747989654541\n",
            "epoch: 20 step: 4758, loss is 0.8136095404624939\n",
            "epoch: 20 step: 4759, loss is 0.9406070709228516\n",
            "epoch: 20 step: 4760, loss is 0.8544747829437256\n",
            "epoch: 20 step: 4761, loss is 0.8098307251930237\n",
            "epoch: 20 step: 4762, loss is 0.9365503787994385\n",
            "epoch: 20 step: 4763, loss is 0.8066621422767639\n",
            "epoch: 20 step: 4764, loss is 0.7420032620429993\n",
            "epoch: 20 step: 4765, loss is 0.7377316355705261\n",
            "epoch: 20 step: 4766, loss is 1.0244331359863281\n",
            "epoch: 20 step: 4767, loss is 0.7920466661453247\n",
            "epoch: 20 step: 4768, loss is 0.781955897808075\n",
            "epoch: 20 step: 4769, loss is 0.8582066893577576\n",
            "epoch: 20 step: 4770, loss is 0.9690713286399841\n",
            "epoch: 20 step: 4771, loss is 0.9849826693534851\n",
            "epoch: 20 step: 4772, loss is 0.9645639061927795\n",
            "epoch: 20 step: 4773, loss is 0.8326422572135925\n",
            "epoch: 20 step: 4774, loss is 0.6990464329719543\n",
            "epoch: 20 step: 4775, loss is 0.6942031979560852\n",
            "epoch: 20 step: 4776, loss is 0.712399423122406\n",
            "epoch: 20 step: 4777, loss is 0.7485030293464661\n",
            "epoch: 20 step: 4778, loss is 0.8644814491271973\n",
            "epoch: 20 step: 4779, loss is 0.8458799719810486\n",
            "epoch: 20 step: 4780, loss is 0.8800374269485474\n",
            "epoch: 20 step: 4781, loss is 0.9073358178138733\n",
            "epoch: 20 step: 4782, loss is 0.7196467518806458\n",
            "epoch: 20 step: 4783, loss is 0.8146880269050598\n",
            "epoch: 20 step: 4784, loss is 1.0238358974456787\n",
            "epoch: 20 step: 4785, loss is 0.7923440337181091\n",
            "epoch: 20 step: 4786, loss is 0.9242134690284729\n",
            "epoch: 20 step: 4787, loss is 0.847240149974823\n",
            "epoch: 20 step: 4788, loss is 0.9008102416992188\n",
            "epoch: 20 step: 4789, loss is 0.8792213201522827\n",
            "epoch: 20 step: 4790, loss is 0.8197610974311829\n",
            "epoch: 20 step: 4791, loss is 0.6624756455421448\n",
            "epoch: 20 step: 4792, loss is 0.7239255905151367\n",
            "epoch: 20 step: 4793, loss is 0.8664049506187439\n",
            "epoch: 20 step: 4794, loss is 0.9953268766403198\n",
            "epoch: 20 step: 4795, loss is 0.8377835154533386\n",
            "epoch: 20 step: 4796, loss is 0.8156024813652039\n",
            "epoch: 20 step: 4797, loss is 0.8129211068153381\n",
            "epoch: 20 step: 4798, loss is 0.8388572335243225\n",
            "epoch: 20 step: 4799, loss is 0.8148812651634216\n",
            "epoch: 20 step: 4800, loss is 0.8472939729690552\n",
            "epoch: 20 step: 4801, loss is 1.0341094732284546\n",
            "epoch: 20 step: 4802, loss is 0.9376411437988281\n",
            "epoch: 20 step: 4803, loss is 0.8406686186790466\n",
            "epoch: 20 step: 4804, loss is 0.8059062361717224\n",
            "epoch: 20 step: 4805, loss is 0.8864471912384033\n",
            "epoch: 20 step: 4806, loss is 0.7386552095413208\n",
            "epoch: 20 step: 4807, loss is 0.974662721157074\n",
            "epoch: 20 step: 4808, loss is 0.8458109498023987\n",
            "epoch: 20 step: 4809, loss is 0.8840168714523315\n",
            "epoch: 20 step: 4810, loss is 1.0311888456344604\n",
            "epoch: 20 step: 4811, loss is 0.8954140543937683\n",
            "epoch: 20 step: 4812, loss is 0.8502827882766724\n",
            "epoch: 20 step: 4813, loss is 0.9740515947341919\n",
            "epoch: 20 step: 4814, loss is 0.7450247406959534\n",
            "epoch: 20 step: 4815, loss is 0.9994945526123047\n",
            "epoch: 20 step: 4816, loss is 1.060425877571106\n",
            "epoch: 20 step: 4817, loss is 0.8932464122772217\n",
            "epoch: 20 step: 4818, loss is 0.8102076649665833\n",
            "epoch: 20 step: 4819, loss is 0.9914684891700745\n",
            "epoch: 20 step: 4820, loss is 0.7893393039703369\n",
            "epoch: 20 step: 4821, loss is 1.0078833103179932\n",
            "epoch: 20 step: 4822, loss is 0.9017547369003296\n",
            "epoch: 20 step: 4823, loss is 0.8879978656768799\n",
            "epoch: 20 step: 4824, loss is 1.017986536026001\n",
            "epoch: 20 step: 4825, loss is 0.9322637915611267\n",
            "epoch: 20 step: 4826, loss is 0.7952690124511719\n",
            "epoch: 20 step: 4827, loss is 1.0155688524246216\n",
            "epoch: 20 step: 4828, loss is 0.7708737254142761\n",
            "epoch: 20 step: 4829, loss is 0.8995344638824463\n",
            "epoch: 20 step: 4830, loss is 0.8270866870880127\n",
            "epoch: 20 step: 4831, loss is 0.7513648867607117\n",
            "epoch: 20 step: 4832, loss is 0.8957701325416565\n",
            "epoch: 20 step: 4833, loss is 0.721847414970398\n",
            "epoch: 20 step: 4834, loss is 0.9385300874710083\n",
            "epoch: 20 step: 4835, loss is 0.8745349645614624\n",
            "epoch: 20 step: 4836, loss is 0.711860716342926\n",
            "epoch: 20 step: 4837, loss is 1.1639370918273926\n",
            "epoch: 20 step: 4838, loss is 0.8149942755699158\n",
            "epoch: 20 step: 4839, loss is 0.95868319272995\n",
            "epoch: 20 step: 4840, loss is 0.8475019931793213\n",
            "epoch: 20 step: 4841, loss is 0.894892692565918\n",
            "epoch: 20 step: 4842, loss is 0.9720801711082458\n",
            "epoch: 20 step: 4843, loss is 1.015862226486206\n",
            "epoch: 20 step: 4844, loss is 0.9010839462280273\n",
            "epoch: 20 step: 4845, loss is 1.0665740966796875\n",
            "epoch: 20 step: 4846, loss is 0.951050877571106\n",
            "epoch: 20 step: 4847, loss is 0.8526991605758667\n",
            "epoch: 20 step: 4848, loss is 0.8448470234870911\n",
            "epoch: 20 step: 4849, loss is 0.8509265184402466\n",
            "epoch: 20 step: 4850, loss is 1.0675015449523926\n",
            "epoch: 20 step: 4851, loss is 1.0263856649398804\n",
            "epoch: 20 step: 4852, loss is 1.0710856914520264\n",
            "epoch: 20 step: 4853, loss is 0.9984642863273621\n",
            "epoch: 20 step: 4854, loss is 0.9544328451156616\n",
            "epoch: 20 step: 4855, loss is 0.8691232800483704\n",
            "epoch: 20 step: 4856, loss is 1.0411946773529053\n",
            "epoch: 20 step: 4857, loss is 1.1174389123916626\n",
            "epoch: 20 step: 4858, loss is 0.8392371535301208\n",
            "epoch: 20 step: 4859, loss is 0.9236650466918945\n",
            "epoch: 20 step: 4860, loss is 1.0155072212219238\n",
            "epoch: 20 step: 4861, loss is 0.9590356945991516\n",
            "epoch: 20 step: 4862, loss is 1.1478337049484253\n",
            "epoch: 20 step: 4863, loss is 0.7832831740379333\n",
            "epoch: 20 step: 4864, loss is 0.8177080750465393\n",
            "epoch: 20 step: 4865, loss is 1.081530213356018\n",
            "epoch: 20 step: 4866, loss is 0.8024030327796936\n",
            "epoch: 20 step: 4867, loss is 0.9446468353271484\n",
            "epoch: 20 step: 4868, loss is 0.8841977715492249\n",
            "epoch: 20 step: 4869, loss is 1.0051178932189941\n",
            "epoch: 20 step: 4870, loss is 0.9251198172569275\n",
            "epoch: 20 step: 4871, loss is 0.877074658870697\n",
            "epoch: 20 step: 4872, loss is 0.8743780851364136\n",
            "epoch: 20 step: 4873, loss is 0.7280904054641724\n",
            "epoch: 20 step: 4874, loss is 0.9866129755973816\n",
            "epoch: 20 step: 4875, loss is 0.9021333456039429\n",
            "epoch: 20 step: 4876, loss is 1.1485189199447632\n",
            "epoch: 20 step: 4877, loss is 1.067783236503601\n",
            "epoch: 20 step: 4878, loss is 0.7947987914085388\n",
            "epoch: 20 step: 4879, loss is 0.7623254060745239\n",
            "epoch: 20 step: 4880, loss is 0.9005028009414673\n",
            "epoch: 20 step: 4881, loss is 0.9025620818138123\n",
            "epoch: 20 step: 4882, loss is 0.7846184968948364\n",
            "epoch: 20 step: 4883, loss is 0.911231279373169\n",
            "epoch: 20 step: 4884, loss is 0.9164888858795166\n",
            "epoch: 20 step: 4885, loss is 0.7734803557395935\n",
            "epoch: 20 step: 4886, loss is 0.804326057434082\n",
            "epoch: 20 step: 4887, loss is 0.885288417339325\n",
            "epoch: 20 step: 4888, loss is 0.9612998962402344\n",
            "epoch: 20 step: 4889, loss is 0.8832806944847107\n",
            "epoch: 20 step: 4890, loss is 0.8479414582252502\n",
            "epoch: 20 step: 4891, loss is 0.7861682176589966\n",
            "epoch: 20 step: 4892, loss is 1.0727336406707764\n",
            "epoch: 20 step: 4893, loss is 0.9598730802536011\n",
            "epoch: 20 step: 4894, loss is 0.7536134123802185\n",
            "epoch: 20 step: 4895, loss is 0.8461259603500366\n",
            "epoch: 20 step: 4896, loss is 0.8353719115257263\n",
            "epoch: 20 step: 4897, loss is 0.9285715818405151\n",
            "epoch: 20 step: 4898, loss is 0.806823194026947\n",
            "epoch: 20 step: 4899, loss is 1.0421090126037598\n",
            "epoch: 20 step: 4900, loss is 0.8284753561019897\n",
            "epoch: 20 step: 4901, loss is 0.9511134624481201\n",
            "epoch: 20 step: 4902, loss is 1.0092365741729736\n",
            "epoch: 20 step: 4903, loss is 1.012869954109192\n",
            "epoch: 20 step: 4904, loss is 0.9136648178100586\n",
            "epoch: 20 step: 4905, loss is 0.9040799140930176\n",
            "epoch: 20 step: 4906, loss is 0.7793583273887634\n",
            "epoch: 20 step: 4907, loss is 0.8269623517990112\n",
            "epoch: 20 step: 4908, loss is 1.0210051536560059\n",
            "epoch: 20 step: 4909, loss is 0.8176462054252625\n",
            "epoch: 20 step: 4910, loss is 1.007898211479187\n",
            "epoch: 20 step: 4911, loss is 0.814801037311554\n",
            "epoch: 20 step: 4912, loss is 0.886654257774353\n",
            "epoch: 20 step: 4913, loss is 0.8915416598320007\n",
            "epoch: 20 step: 4914, loss is 1.0342390537261963\n",
            "epoch: 20 step: 4915, loss is 0.9307766556739807\n",
            "epoch: 20 step: 4916, loss is 0.8790410161018372\n",
            "epoch: 20 step: 4917, loss is 0.9020177125930786\n",
            "epoch: 20 step: 4918, loss is 0.9657057523727417\n",
            "epoch: 20 step: 4919, loss is 0.9303838014602661\n",
            "epoch: 20 step: 4920, loss is 1.0397530794143677\n",
            "epoch: 20 step: 4921, loss is 0.7537767887115479\n",
            "epoch: 20 step: 4922, loss is 0.8500708341598511\n",
            "epoch: 20 step: 4923, loss is 0.8823100328445435\n",
            "epoch: 20 step: 4924, loss is 0.8582811951637268\n",
            "epoch: 20 step: 4925, loss is 0.8928472995758057\n",
            "epoch: 20 step: 4926, loss is 0.8454588651657104\n",
            "epoch: 20 step: 4927, loss is 1.187360405921936\n",
            "epoch: 20 step: 4928, loss is 0.9440625905990601\n",
            "epoch: 20 step: 4929, loss is 0.7869200110435486\n",
            "epoch: 20 step: 4930, loss is 0.8554364442825317\n",
            "epoch: 20 step: 4931, loss is 0.8676811456680298\n",
            "epoch: 20 step: 4932, loss is 0.9758266806602478\n",
            "epoch: 20 step: 4933, loss is 0.8574713468551636\n",
            "epoch: 20 step: 4934, loss is 0.9919118285179138\n",
            "epoch: 20 step: 4935, loss is 0.9793934226036072\n",
            "epoch: 20 step: 4936, loss is 0.9774990677833557\n",
            "epoch: 20 step: 4937, loss is 0.8925033807754517\n",
            "epoch: 20 step: 4938, loss is 1.0240181684494019\n",
            "epoch: 20 step: 4939, loss is 0.8870387673377991\n",
            "epoch: 20 step: 4940, loss is 0.8056442141532898\n",
            "epoch: 20 step: 4941, loss is 0.7514779567718506\n",
            "epoch: 20 step: 4942, loss is 0.9651063084602356\n",
            "epoch: 20 step: 4943, loss is 1.0081381797790527\n",
            "epoch: 20 step: 4944, loss is 0.8028761148452759\n",
            "epoch: 20 step: 4945, loss is 0.9957439303398132\n",
            "epoch: 20 step: 4946, loss is 0.9478272199630737\n",
            "epoch: 20 step: 4947, loss is 0.7587747573852539\n",
            "epoch: 20 step: 4948, loss is 0.7698831558227539\n",
            "epoch: 20 step: 4949, loss is 1.0012855529785156\n",
            "epoch: 20 step: 4950, loss is 0.8498464822769165\n",
            "epoch: 20 step: 4951, loss is 0.9712888598442078\n",
            "epoch: 20 step: 4952, loss is 0.9787010550498962\n",
            "epoch: 20 step: 4953, loss is 0.7403306365013123\n",
            "epoch: 20 step: 4954, loss is 0.945490300655365\n",
            "epoch: 20 step: 4955, loss is 0.9146413803100586\n",
            "epoch: 20 step: 4956, loss is 0.8463265299797058\n",
            "epoch: 20 step: 4957, loss is 0.8423945307731628\n",
            "epoch: 20 step: 4958, loss is 0.8508610129356384\n",
            "epoch: 20 step: 4959, loss is 0.9766443371772766\n",
            "epoch: 20 step: 4960, loss is 0.8632352948188782\n",
            "epoch: 20 step: 4961, loss is 1.0733344554901123\n",
            "epoch: 20 step: 4962, loss is 0.8822079300880432\n",
            "epoch: 20 step: 4963, loss is 0.7617111206054688\n",
            "epoch: 20 step: 4964, loss is 0.9091266989707947\n",
            "epoch: 20 step: 4965, loss is 0.7519019842147827\n",
            "epoch: 20 step: 4966, loss is 0.8047617077827454\n",
            "epoch: 20 step: 4967, loss is 0.9302582144737244\n",
            "epoch: 20 step: 4968, loss is 0.8656730055809021\n",
            "epoch: 20 step: 4969, loss is 0.8103873133659363\n",
            "epoch: 20 step: 4970, loss is 0.7714776992797852\n",
            "epoch: 20 step: 4971, loss is 0.8091651201248169\n",
            "epoch: 20 step: 4972, loss is 0.802141010761261\n",
            "epoch: 20 step: 4973, loss is 0.9419824481010437\n",
            "epoch: 20 step: 4974, loss is 0.8906208276748657\n",
            "epoch: 20 step: 4975, loss is 0.7799206972122192\n",
            "epoch: 20 step: 4976, loss is 0.8016489744186401\n",
            "epoch: 20 step: 4977, loss is 0.7375279068946838\n",
            "epoch: 20 step: 4978, loss is 0.7382256388664246\n",
            "epoch: 20 step: 4979, loss is 0.8244906067848206\n",
            "epoch: 20 step: 4980, loss is 0.9097148180007935\n",
            "epoch: 20 step: 4981, loss is 1.0933998823165894\n",
            "epoch: 20 step: 4982, loss is 0.8801991939544678\n",
            "epoch: 20 step: 4983, loss is 0.7608721852302551\n",
            "epoch: 20 step: 4984, loss is 0.8886002898216248\n",
            "epoch: 20 step: 4985, loss is 0.8041666746139526\n",
            "epoch: 20 step: 4986, loss is 0.8648078441619873\n",
            "epoch: 20 step: 4987, loss is 0.7423933148384094\n",
            "epoch: 20 step: 4988, loss is 0.8110550045967102\n",
            "epoch: 20 step: 4989, loss is 0.7347831726074219\n",
            "epoch: 20 step: 4990, loss is 0.8478896617889404\n",
            "epoch: 20 step: 4991, loss is 0.8660753965377808\n",
            "epoch: 20 step: 4992, loss is 0.8765150308609009\n",
            "epoch: 20 step: 4993, loss is 0.8317626714706421\n",
            "epoch: 20 step: 4994, loss is 0.8104497194290161\n",
            "epoch: 20 step: 4995, loss is 0.7925089001655579\n",
            "epoch: 20 step: 4996, loss is 0.8954973220825195\n",
            "epoch: 20 step: 4997, loss is 0.9765821695327759\n",
            "epoch: 20 step: 4998, loss is 0.8309471011161804\n",
            "epoch: 20 step: 4999, loss is 0.8335099816322327\n",
            "epoch: 20 step: 5000, loss is 0.7075467109680176\n",
            "epoch: 20 step: 5001, loss is 0.8388572335243225\n",
            "epoch: 20 step: 5002, loss is 0.7650446891784668\n",
            "epoch: 20 step: 5003, loss is 0.9706739187240601\n",
            "epoch: 20 step: 5004, loss is 0.9162104725837708\n",
            "epoch: 20 step: 5005, loss is 0.9555066227912903\n",
            "epoch: 20 step: 5006, loss is 0.8568211197853088\n",
            "epoch: 20 step: 5007, loss is 0.9924593567848206\n",
            "epoch: 20 step: 5008, loss is 0.7225314974784851\n",
            "epoch: 20 step: 5009, loss is 0.9939140677452087\n",
            "epoch: 20 step: 5010, loss is 0.809230387210846\n",
            "epoch: 20 step: 5011, loss is 1.0847725868225098\n",
            "epoch: 20 step: 5012, loss is 0.9402787685394287\n",
            "epoch: 20 step: 5013, loss is 1.0268149375915527\n",
            "epoch: 20 step: 5014, loss is 0.7080144286155701\n",
            "epoch: 20 step: 5015, loss is 0.8537507057189941\n",
            "epoch: 20 step: 5016, loss is 0.7544352412223816\n",
            "epoch: 20 step: 5017, loss is 0.9095388650894165\n",
            "epoch: 20 step: 5018, loss is 0.9171233177185059\n",
            "epoch: 20 step: 5019, loss is 1.0209227800369263\n",
            "epoch: 20 step: 5020, loss is 0.9606977105140686\n",
            "epoch: 20 step: 5021, loss is 0.7101131081581116\n",
            "epoch: 20 step: 5022, loss is 0.8100181221961975\n",
            "epoch: 20 step: 5023, loss is 0.9522830247879028\n",
            "epoch: 20 step: 5024, loss is 0.7832257151603699\n",
            "epoch: 20 step: 5025, loss is 0.980254590511322\n",
            "epoch: 20 step: 5026, loss is 0.8930623531341553\n",
            "epoch: 20 step: 5027, loss is 0.7348175048828125\n",
            "epoch: 20 step: 5028, loss is 1.153835415840149\n",
            "epoch: 20 step: 5029, loss is 0.9586089849472046\n",
            "epoch: 20 step: 5030, loss is 0.9948501586914062\n",
            "epoch: 20 step: 5031, loss is 0.879364013671875\n",
            "epoch: 20 step: 5032, loss is 0.9086176156997681\n",
            "epoch: 20 step: 5033, loss is 0.8614186644554138\n",
            "epoch: 20 step: 5034, loss is 0.9224504232406616\n",
            "epoch: 20 step: 5035, loss is 1.0333662033081055\n",
            "epoch: 20 step: 5036, loss is 0.8872745037078857\n",
            "epoch: 20 step: 5037, loss is 0.8872153759002686\n",
            "epoch: 20 step: 5038, loss is 0.9899240732192993\n",
            "epoch: 20 step: 5039, loss is 1.0545923709869385\n",
            "epoch: 20 step: 5040, loss is 0.9557766914367676\n",
            "epoch: 20 step: 5041, loss is 0.8534996509552002\n",
            "epoch: 20 step: 5042, loss is 0.999534010887146\n",
            "epoch: 20 step: 5043, loss is 0.9390846490859985\n",
            "epoch: 20 step: 5044, loss is 0.9416497349739075\n",
            "epoch: 20 step: 5045, loss is 1.1935391426086426\n",
            "epoch: 20 step: 5046, loss is 1.185567855834961\n",
            "epoch: 20 step: 5047, loss is 0.8817765712738037\n",
            "epoch: 20 step: 5048, loss is 0.9667313098907471\n",
            "epoch: 20 step: 5049, loss is 0.9003364443778992\n",
            "epoch: 20 step: 5050, loss is 0.8718253374099731\n",
            "epoch: 20 step: 5051, loss is 0.8906399011611938\n",
            "epoch: 20 step: 5052, loss is 1.0328209400177002\n",
            "epoch: 20 step: 5053, loss is 0.9279800057411194\n",
            "epoch: 20 step: 5054, loss is 0.9212711453437805\n",
            "epoch: 20 step: 5055, loss is 0.9070896506309509\n",
            "epoch: 20 step: 5056, loss is 1.0454051494598389\n",
            "epoch: 20 step: 5057, loss is 0.8374396562576294\n",
            "epoch: 20 step: 5058, loss is 0.8760346174240112\n",
            "epoch: 20 step: 5059, loss is 1.0367110967636108\n",
            "epoch: 20 step: 5060, loss is 0.8244155645370483\n",
            "epoch: 20 step: 5061, loss is 0.9447574615478516\n",
            "epoch: 20 step: 5062, loss is 0.9584760665893555\n",
            "epoch: 20 step: 5063, loss is 0.8532393574714661\n",
            "epoch: 20 step: 5064, loss is 0.8699124455451965\n",
            "epoch: 20 step: 5065, loss is 0.8962132930755615\n",
            "epoch: 20 step: 5066, loss is 1.0657665729522705\n",
            "epoch: 20 step: 5067, loss is 0.8369099497795105\n",
            "epoch: 20 step: 5068, loss is 0.9208776354789734\n",
            "epoch: 20 step: 5069, loss is 0.920312762260437\n",
            "epoch: 20 step: 5070, loss is 0.8421922326087952\n",
            "epoch: 20 step: 5071, loss is 1.0485886335372925\n",
            "epoch: 20 step: 5072, loss is 0.8776715993881226\n",
            "epoch: 20 step: 5073, loss is 1.0416532754898071\n",
            "epoch: 20 step: 5074, loss is 1.0468056201934814\n",
            "epoch: 20 step: 5075, loss is 0.866769015789032\n",
            "epoch: 20 step: 5076, loss is 0.7853739857673645\n",
            "epoch: 20 step: 5077, loss is 0.9368950724601746\n",
            "epoch: 20 step: 5078, loss is 0.8365647196769714\n",
            "epoch: 20 step: 5079, loss is 0.8952178359031677\n",
            "epoch: 20 step: 5080, loss is 0.9694759249687195\n",
            "epoch: 20 step: 5081, loss is 0.8970571756362915\n",
            "epoch: 20 step: 5082, loss is 0.7966280579566956\n",
            "epoch: 20 step: 5083, loss is 1.0041282176971436\n",
            "epoch: 20 step: 5084, loss is 0.734745442867279\n",
            "epoch: 20 step: 5085, loss is 0.8015608787536621\n",
            "epoch: 20 step: 5086, loss is 0.8989689946174622\n",
            "epoch: 20 step: 5087, loss is 0.8731133937835693\n",
            "epoch: 20 step: 5088, loss is 0.782224178314209\n",
            "epoch: 20 step: 5089, loss is 0.9036439657211304\n",
            "epoch: 20 step: 5090, loss is 0.8029251098632812\n",
            "epoch: 20 step: 5091, loss is 0.861024796962738\n",
            "epoch: 20 step: 5092, loss is 1.021525502204895\n",
            "epoch: 20 step: 5093, loss is 0.9846293926239014\n",
            "epoch: 20 step: 5094, loss is 0.9458627104759216\n",
            "epoch: 20 step: 5095, loss is 0.8451197743415833\n",
            "epoch: 20 step: 5096, loss is 0.7935803532600403\n",
            "epoch: 20 step: 5097, loss is 0.9437488317489624\n",
            "epoch: 20 step: 5098, loss is 1.2469767332077026\n",
            "epoch: 20 step: 5099, loss is 0.7304372191429138\n",
            "epoch: 20 step: 5100, loss is 0.674112856388092\n",
            "epoch: 20 step: 5101, loss is 0.8018447756767273\n",
            "epoch: 20 step: 5102, loss is 0.9127331376075745\n",
            "epoch: 20 step: 5103, loss is 0.8492739200592041\n",
            "epoch: 20 step: 5104, loss is 1.1183884143829346\n",
            "epoch: 20 step: 5105, loss is 0.9152028560638428\n",
            "epoch: 20 step: 5106, loss is 0.9165769219398499\n",
            "epoch: 20 step: 5107, loss is 1.0090105533599854\n",
            "epoch: 20 step: 5108, loss is 0.8385447263717651\n",
            "epoch: 20 step: 5109, loss is 1.0033057928085327\n",
            "epoch: 20 step: 5110, loss is 0.8664931058883667\n",
            "epoch: 20 step: 5111, loss is 0.9304510354995728\n",
            "epoch: 20 step: 5112, loss is 0.8488646149635315\n",
            "epoch: 20 step: 5113, loss is 0.9757207036018372\n",
            "epoch: 20 step: 5114, loss is 0.8102914094924927\n",
            "epoch: 20 step: 5115, loss is 0.9094290137290955\n",
            "epoch: 20 step: 5116, loss is 0.8558173179626465\n",
            "epoch: 20 step: 5117, loss is 0.8177320957183838\n",
            "epoch: 20 step: 5118, loss is 0.9193120002746582\n",
            "epoch: 20 step: 5119, loss is 0.9283111691474915\n",
            "epoch: 20 step: 5120, loss is 0.8811820149421692\n",
            "epoch: 20 step: 5121, loss is 0.6722797155380249\n",
            "epoch: 20 step: 5122, loss is 0.9856767058372498\n",
            "epoch: 20 step: 5123, loss is 0.8236037492752075\n",
            "epoch: 20 step: 5124, loss is 0.8435279726982117\n",
            "epoch: 20 step: 5125, loss is 0.9794121384620667\n",
            "epoch: 20 step: 5126, loss is 0.8633076548576355\n",
            "epoch: 20 step: 5127, loss is 1.0319199562072754\n",
            "epoch: 20 step: 5128, loss is 1.0481619834899902\n",
            "epoch: 20 step: 5129, loss is 1.120790719985962\n",
            "epoch: 20 step: 5130, loss is 1.0070546865463257\n",
            "epoch: 20 step: 5131, loss is 0.9483187794685364\n",
            "epoch: 20 step: 5132, loss is 0.9426721334457397\n",
            "epoch: 20 step: 5133, loss is 0.8739452958106995\n",
            "epoch: 20 step: 5134, loss is 0.7536851167678833\n",
            "epoch: 20 step: 5135, loss is 0.8416754603385925\n",
            "epoch: 20 step: 5136, loss is 0.903451144695282\n",
            "epoch: 20 step: 5137, loss is 0.9247857332229614\n",
            "epoch: 20 step: 5138, loss is 0.8107223510742188\n",
            "epoch: 20 step: 5139, loss is 0.9978424310684204\n",
            "epoch: 20 step: 5140, loss is 1.0766162872314453\n",
            "epoch: 20 step: 5141, loss is 0.987846314907074\n",
            "epoch: 20 step: 5142, loss is 1.0292603969573975\n",
            "epoch: 20 step: 5143, loss is 0.8754157423973083\n",
            "epoch: 20 step: 5144, loss is 0.8495495319366455\n",
            "epoch: 20 step: 5145, loss is 0.903188943862915\n",
            "epoch: 20 step: 5146, loss is 0.9852820038795471\n",
            "epoch: 20 step: 5147, loss is 0.8398774862289429\n",
            "epoch: 20 step: 5148, loss is 0.9125656485557556\n",
            "epoch: 20 step: 5149, loss is 0.9069223403930664\n",
            "epoch: 20 step: 5150, loss is 0.7682600021362305\n",
            "epoch: 20 step: 5151, loss is 0.9043469429016113\n",
            "epoch: 20 step: 5152, loss is 0.7898802161216736\n",
            "epoch: 20 step: 5153, loss is 0.8934245109558105\n",
            "epoch: 20 step: 5154, loss is 0.8056841492652893\n",
            "epoch: 20 step: 5155, loss is 0.7602132558822632\n",
            "epoch: 20 step: 5156, loss is 0.8026397824287415\n",
            "epoch: 20 step: 5157, loss is 0.8566826581954956\n",
            "epoch: 20 step: 5158, loss is 1.0701673030853271\n",
            "epoch: 20 step: 5159, loss is 0.8800350427627563\n",
            "epoch: 20 step: 5160, loss is 0.9111464023590088\n",
            "epoch: 20 step: 5161, loss is 1.0882580280303955\n",
            "epoch: 20 step: 5162, loss is 0.7835991978645325\n",
            "epoch: 20 step: 5163, loss is 0.8168339729309082\n",
            "epoch: 20 step: 5164, loss is 0.9497406482696533\n",
            "epoch: 20 step: 5165, loss is 0.9900202751159668\n",
            "epoch: 20 step: 5166, loss is 0.8017656207084656\n",
            "epoch: 20 step: 5167, loss is 1.0123323202133179\n",
            "epoch: 20 step: 5168, loss is 0.9634584188461304\n",
            "epoch: 20 step: 5169, loss is 0.8333409428596497\n",
            "epoch: 20 step: 5170, loss is 0.8530976176261902\n",
            "epoch: 20 step: 5171, loss is 0.8868077397346497\n",
            "epoch: 20 step: 5172, loss is 1.10272216796875\n",
            "epoch: 20 step: 5173, loss is 0.8724327087402344\n",
            "epoch: 20 step: 5174, loss is 0.7492210268974304\n",
            "epoch: 20 step: 5175, loss is 0.8082384467124939\n",
            "epoch: 20 step: 5176, loss is 0.8213720321655273\n",
            "epoch: 20 step: 5177, loss is 0.8684508204460144\n",
            "epoch: 20 step: 5178, loss is 0.8538310527801514\n",
            "epoch: 20 step: 5179, loss is 0.9475532174110413\n",
            "epoch: 20 step: 5180, loss is 0.8407565951347351\n",
            "epoch: 20 step: 5181, loss is 0.9089887142181396\n",
            "epoch: 20 step: 5182, loss is 0.7924832701683044\n",
            "epoch: 20 step: 5183, loss is 1.0137702226638794\n",
            "epoch: 20 step: 5184, loss is 0.880149781703949\n",
            "epoch: 20 step: 5185, loss is 0.7923089861869812\n",
            "epoch: 20 step: 5186, loss is 0.8149645328521729\n",
            "epoch: 20 step: 5187, loss is 0.7311817407608032\n",
            "epoch: 20 step: 5188, loss is 0.7359594106674194\n",
            "epoch: 20 step: 5189, loss is 0.9495787620544434\n",
            "epoch: 20 step: 5190, loss is 0.8368419408798218\n",
            "epoch: 20 step: 5191, loss is 0.8561822175979614\n",
            "epoch: 20 step: 5192, loss is 0.8623056411743164\n",
            "epoch: 20 step: 5193, loss is 1.0614607334136963\n",
            "epoch: 20 step: 5194, loss is 0.8810790181159973\n",
            "epoch: 20 step: 5195, loss is 0.7863544821739197\n",
            "epoch: 20 step: 5196, loss is 1.1933021545410156\n",
            "epoch: 20 step: 5197, loss is 0.9220593571662903\n",
            "epoch: 20 step: 5198, loss is 0.8582165837287903\n",
            "epoch: 20 step: 5199, loss is 0.8254895210266113\n",
            "epoch: 20 step: 5200, loss is 0.7960898280143738\n",
            "epoch: 20 step: 5201, loss is 0.788711667060852\n",
            "epoch: 20 step: 5202, loss is 0.9239214062690735\n",
            "epoch: 20 step: 5203, loss is 0.8629254698753357\n",
            "epoch: 20 step: 5204, loss is 0.8860043287277222\n",
            "epoch: 20 step: 5205, loss is 0.8085450530052185\n",
            "epoch: 20 step: 5206, loss is 0.8754836916923523\n",
            "epoch: 20 step: 5207, loss is 0.9295697212219238\n",
            "epoch: 20 step: 5208, loss is 0.9540069103240967\n",
            "epoch: 20 step: 5209, loss is 0.7358344197273254\n",
            "epoch: 20 step: 5210, loss is 0.8795490860939026\n",
            "epoch: 20 step: 5211, loss is 0.8878132700920105\n",
            "epoch: 20 step: 5212, loss is 0.9082395434379578\n",
            "epoch: 20 step: 5213, loss is 0.8358635306358337\n",
            "epoch: 20 step: 5214, loss is 0.7545634508132935\n",
            "epoch: 20 step: 5215, loss is 0.7610236406326294\n",
            "epoch: 20 step: 5216, loss is 0.8884208798408508\n",
            "epoch: 20 step: 5217, loss is 1.0501213073730469\n",
            "epoch: 20 step: 5218, loss is 0.834139347076416\n",
            "epoch: 20 step: 5219, loss is 0.9143214225769043\n",
            "epoch: 20 step: 5220, loss is 0.9584790468215942\n",
            "epoch: 20 step: 5221, loss is 0.7817330360412598\n",
            "epoch: 20 step: 5222, loss is 0.9498427510261536\n",
            "epoch: 20 step: 5223, loss is 0.8873249292373657\n",
            "epoch: 20 step: 5224, loss is 0.8178437352180481\n",
            "epoch: 20 step: 5225, loss is 0.946452260017395\n",
            "epoch: 20 step: 5226, loss is 0.7336910367012024\n",
            "epoch: 20 step: 5227, loss is 0.788232147693634\n",
            "epoch: 20 step: 5228, loss is 0.9516330361366272\n",
            "epoch: 20 step: 5229, loss is 0.8950128555297852\n",
            "epoch: 20 step: 5230, loss is 0.952942430973053\n",
            "epoch: 20 step: 5231, loss is 0.7053250670433044\n",
            "epoch: 20 step: 5232, loss is 0.8601269125938416\n",
            "epoch: 20 step: 5233, loss is 0.8453661799430847\n",
            "epoch: 20 step: 5234, loss is 0.8961843848228455\n",
            "epoch: 20 step: 5235, loss is 0.8845969438552856\n",
            "epoch: 20 step: 5236, loss is 0.7839481830596924\n",
            "epoch: 20 step: 5237, loss is 0.7474682927131653\n",
            "epoch: 20 step: 5238, loss is 1.1907292604446411\n",
            "epoch: 20 step: 5239, loss is 1.0109548568725586\n",
            "epoch: 20 step: 5240, loss is 0.8699977993965149\n",
            "epoch: 20 step: 5241, loss is 0.9322503209114075\n",
            "epoch: 20 step: 5242, loss is 0.8325377106666565\n",
            "epoch: 20 step: 5243, loss is 0.8395026326179504\n",
            "epoch: 20 step: 5244, loss is 0.9830703139305115\n",
            "epoch: 20 step: 5245, loss is 0.8826072812080383\n",
            "epoch: 20 step: 5246, loss is 0.7343037128448486\n",
            "epoch: 20 step: 5247, loss is 0.7409537434577942\n",
            "epoch: 20 step: 5248, loss is 0.8744029998779297\n",
            "epoch: 20 step: 5249, loss is 0.7752907276153564\n",
            "epoch: 20 step: 5250, loss is 0.8238669037818909\n",
            "epoch: 20 step: 5251, loss is 1.053828477859497\n",
            "epoch: 20 step: 5252, loss is 0.8590621948242188\n",
            "epoch: 20 step: 5253, loss is 0.9751223921775818\n",
            "epoch: 20 step: 5254, loss is 0.8653872013092041\n",
            "epoch: 20 step: 5255, loss is 0.6749173998832703\n",
            "epoch: 20 step: 5256, loss is 0.7140296101570129\n",
            "epoch: 20 step: 5257, loss is 0.777850866317749\n",
            "epoch: 20 step: 5258, loss is 0.9243584275245667\n",
            "epoch: 20 step: 5259, loss is 0.8689911961555481\n",
            "epoch: 20 step: 5260, loss is 0.9210134744644165\n",
            "epoch: 20 step: 5261, loss is 0.8264632225036621\n",
            "epoch: 20 step: 5262, loss is 1.0680296421051025\n",
            "epoch: 20 step: 5263, loss is 1.010895848274231\n",
            "epoch: 20 step: 5264, loss is 1.023632526397705\n",
            "epoch: 20 step: 5265, loss is 0.7233591079711914\n",
            "epoch: 20 step: 5266, loss is 0.8749035596847534\n",
            "epoch: 20 step: 5267, loss is 1.1551735401153564\n",
            "epoch: 20 step: 5268, loss is 0.8191837072372437\n",
            "epoch: 20 step: 5269, loss is 0.8020082712173462\n",
            "epoch: 20 step: 5270, loss is 0.918524444103241\n",
            "epoch: 20 step: 5271, loss is 0.8518461585044861\n",
            "epoch: 20 step: 5272, loss is 0.7979735732078552\n",
            "epoch: 20 step: 5273, loss is 0.9574785828590393\n",
            "epoch: 20 step: 5274, loss is 0.7754784226417542\n",
            "epoch: 20 step: 5275, loss is 0.9420703649520874\n",
            "epoch: 20 step: 5276, loss is 0.97255939245224\n",
            "epoch: 20 step: 5277, loss is 0.9196419715881348\n",
            "epoch: 20 step: 5278, loss is 1.1033365726470947\n",
            "epoch: 20 step: 5279, loss is 1.1319899559020996\n",
            "epoch: 20 step: 5280, loss is 0.9903419613838196\n",
            "epoch: 20 step: 5281, loss is 0.9837011098861694\n",
            "epoch: 20 step: 5282, loss is 0.8062566518783569\n",
            "epoch: 20 step: 5283, loss is 1.014920711517334\n",
            "epoch: 20 step: 5284, loss is 1.0539602041244507\n",
            "epoch: 20 step: 5285, loss is 0.852789044380188\n",
            "epoch: 20 step: 5286, loss is 0.8348535299301147\n",
            "epoch: 20 step: 5287, loss is 0.8788831830024719\n",
            "epoch: 20 step: 5288, loss is 0.8173910975456238\n",
            "epoch: 20 step: 5289, loss is 0.962159276008606\n",
            "epoch: 20 step: 5290, loss is 0.8635414838790894\n",
            "epoch: 20 step: 5291, loss is 0.8507423400878906\n",
            "epoch: 20 step: 5292, loss is 0.9384163618087769\n",
            "epoch: 20 step: 5293, loss is 0.8341691493988037\n",
            "epoch: 20 step: 5294, loss is 0.9697167277336121\n",
            "epoch: 20 step: 5295, loss is 0.8926808834075928\n",
            "epoch: 20 step: 5296, loss is 1.0058281421661377\n",
            "epoch: 20 step: 5297, loss is 0.997856855392456\n",
            "epoch: 20 step: 5298, loss is 0.8610206842422485\n",
            "epoch: 20 step: 5299, loss is 0.8819940090179443\n",
            "epoch: 20 step: 5300, loss is 0.7713736891746521\n",
            "epoch: 20 step: 5301, loss is 0.9276904463768005\n",
            "üß™ Evaluating...\n",
            "‚úÖ Test Accuracy: {'Accuracy': 0.5952032827091783}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1: Define the Neural Network\n",
        "import mindspore.nn as nn\n",
        "\n",
        "# Define a simple feedforward neural network\n",
        "class CropNet(nn.Cell):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(CropNet, self).__init__()\n",
        "        self.fc = nn.SequentialCell(\n",
        "            nn.Dense(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dense(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dense(32, output_dim)\n",
        "        )\n",
        "\n",
        "    def construct(self, x):\n",
        "        return self.fc(x)\n"
      ],
      "metadata": {
        "id": "sjjNjp4KKmIz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 2: Set Up the Training Pipeline\n",
        "from mindspore import Model\n",
        "from mindspore.nn import SoftmaxCrossEntropyWithLogits, Accuracy, Momentum\n",
        "from mindspore.train.callback import LossMonitor\n",
        "\n",
        "# Input and output dimensions\n",
        "input_dim = X_train_tensor.shape[1]\n",
        "num_classes = len(np.unique(y_train_tensor.asnumpy()))\n",
        "\n",
        "# Create model\n",
        "net = CropNet(input_dim=input_dim, output_dim=num_classes)\n",
        "\n",
        "# Loss, optimizer, and metrics\n",
        "loss_fn = SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
        "optimizer = Momentum(net.trainable_params(), learning_rate=0.01, momentum=0.9)\n",
        "model = Model(net, loss_fn=loss_fn, optimizer=optimizer, metrics={\"Accuracy\": Accuracy()})\n"
      ],
      "metadata": {
        "id": "ByTKcBrGK6hb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_train_tensor shape:\", X_train_tensor.shape)\n",
        "print(\"After asnumpy():\", X_train_tensor.asnumpy().shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qztvSC_ORBZV",
        "outputId": "77c9e370-b176-4a31-fcf2-e159db16a2f1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train_tensor shape: (169615, 21)\n",
            "After asnumpy(): (169615, 21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3: Train the Model\n",
        "\n",
        "# Train for 1 epochs\n",
        "model.train(\n",
        "    epoch=1,\n",
        "    train_dataset=train_dataset,\n",
        "    callbacks=[LossMonitor()],\n",
        "    dataset_sink_mode=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GH3fFiaaLj2s",
        "outputId": "43bf27d5-2d25-4e6f-8ad9-50ef2038d1ad"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "epoch: 1 step: 302, loss is 0.9726265072822571\n",
            "epoch: 1 step: 303, loss is 0.7965282201766968\n",
            "epoch: 1 step: 304, loss is 0.7968557476997375\n",
            "epoch: 1 step: 305, loss is 0.8813217878341675\n",
            "epoch: 1 step: 306, loss is 0.8237060904502869\n",
            "epoch: 1 step: 307, loss is 1.0256707668304443\n",
            "epoch: 1 step: 308, loss is 0.7984513640403748\n",
            "epoch: 1 step: 309, loss is 0.7822090983390808\n",
            "epoch: 1 step: 310, loss is 0.8958194851875305\n",
            "epoch: 1 step: 311, loss is 0.8872965574264526\n",
            "epoch: 1 step: 312, loss is 0.7342862486839294\n",
            "epoch: 1 step: 313, loss is 0.9370300769805908\n",
            "epoch: 1 step: 314, loss is 1.010238766670227\n",
            "epoch: 1 step: 315, loss is 0.9975439310073853\n",
            "epoch: 1 step: 316, loss is 0.799129068851471\n",
            "epoch: 1 step: 317, loss is 0.676120936870575\n",
            "epoch: 1 step: 318, loss is 0.9615422487258911\n",
            "epoch: 1 step: 319, loss is 0.7449369430541992\n",
            "epoch: 1 step: 320, loss is 0.8294854760169983\n",
            "epoch: 1 step: 321, loss is 0.8970722556114197\n",
            "epoch: 1 step: 322, loss is 0.9298617243766785\n",
            "epoch: 1 step: 323, loss is 0.8637769222259521\n",
            "epoch: 1 step: 324, loss is 0.8392634391784668\n",
            "epoch: 1 step: 325, loss is 0.9970474243164062\n",
            "epoch: 1 step: 326, loss is 0.8851955533027649\n",
            "epoch: 1 step: 327, loss is 0.8750477433204651\n",
            "epoch: 1 step: 328, loss is 0.8534905910491943\n",
            "epoch: 1 step: 329, loss is 0.9075074791908264\n",
            "epoch: 1 step: 330, loss is 0.9965202212333679\n",
            "epoch: 1 step: 331, loss is 1.1665561199188232\n",
            "epoch: 1 step: 332, loss is 0.9350348114967346\n",
            "epoch: 1 step: 333, loss is 0.8985549211502075\n",
            "epoch: 1 step: 334, loss is 0.8498334884643555\n",
            "epoch: 1 step: 335, loss is 0.9027934670448303\n",
            "epoch: 1 step: 336, loss is 0.8943436145782471\n",
            "epoch: 1 step: 337, loss is 0.76841139793396\n",
            "epoch: 1 step: 338, loss is 0.7617998719215393\n",
            "epoch: 1 step: 339, loss is 0.6375651955604553\n",
            "epoch: 1 step: 340, loss is 1.0148566961288452\n",
            "epoch: 1 step: 341, loss is 0.8439152240753174\n",
            "epoch: 1 step: 342, loss is 1.0464292764663696\n",
            "epoch: 1 step: 343, loss is 0.865169107913971\n",
            "epoch: 1 step: 344, loss is 0.9349854588508606\n",
            "epoch: 1 step: 345, loss is 0.9886667728424072\n",
            "epoch: 1 step: 346, loss is 0.8947329521179199\n",
            "epoch: 1 step: 347, loss is 0.8734285235404968\n",
            "epoch: 1 step: 348, loss is 0.750312864780426\n",
            "epoch: 1 step: 349, loss is 0.7578882575035095\n",
            "epoch: 1 step: 350, loss is 0.6311286687850952\n",
            "epoch: 1 step: 351, loss is 0.8636108040809631\n",
            "epoch: 1 step: 352, loss is 0.8686231970787048\n",
            "epoch: 1 step: 353, loss is 0.7628430724143982\n",
            "epoch: 1 step: 354, loss is 0.9371911287307739\n",
            "epoch: 1 step: 355, loss is 0.9390203356742859\n",
            "epoch: 1 step: 356, loss is 0.7202516198158264\n",
            "epoch: 1 step: 357, loss is 0.866523027420044\n",
            "epoch: 1 step: 358, loss is 0.9971008896827698\n",
            "epoch: 1 step: 359, loss is 0.9863654375076294\n",
            "epoch: 1 step: 360, loss is 0.8891264796257019\n",
            "epoch: 1 step: 361, loss is 0.773946225643158\n",
            "epoch: 1 step: 362, loss is 0.7467793822288513\n",
            "epoch: 1 step: 363, loss is 1.0688307285308838\n",
            "epoch: 1 step: 364, loss is 0.8713037371635437\n",
            "epoch: 1 step: 365, loss is 1.1277267932891846\n",
            "epoch: 1 step: 366, loss is 0.9780468344688416\n",
            "epoch: 1 step: 367, loss is 0.971316933631897\n",
            "epoch: 1 step: 368, loss is 1.0161991119384766\n",
            "epoch: 1 step: 369, loss is 0.8074153065681458\n",
            "epoch: 1 step: 370, loss is 0.6468288898468018\n",
            "epoch: 1 step: 371, loss is 0.8045313954353333\n",
            "epoch: 1 step: 372, loss is 0.9118868708610535\n",
            "epoch: 1 step: 373, loss is 0.9154326319694519\n",
            "epoch: 1 step: 374, loss is 1.0781534910202026\n",
            "epoch: 1 step: 375, loss is 0.8998266458511353\n",
            "epoch: 1 step: 376, loss is 0.9618678092956543\n",
            "epoch: 1 step: 377, loss is 0.9680719375610352\n",
            "epoch: 1 step: 378, loss is 0.9904639720916748\n",
            "epoch: 1 step: 379, loss is 1.0293735265731812\n",
            "epoch: 1 step: 380, loss is 0.9468134641647339\n",
            "epoch: 1 step: 381, loss is 0.8614814281463623\n",
            "epoch: 1 step: 382, loss is 1.0724610090255737\n",
            "epoch: 1 step: 383, loss is 0.8652291297912598\n",
            "epoch: 1 step: 384, loss is 0.8641974329948425\n",
            "epoch: 1 step: 385, loss is 0.9918303489685059\n",
            "epoch: 1 step: 386, loss is 0.9440876245498657\n",
            "epoch: 1 step: 387, loss is 1.1023027896881104\n",
            "epoch: 1 step: 388, loss is 0.9286906719207764\n",
            "epoch: 1 step: 389, loss is 0.9145306348800659\n",
            "epoch: 1 step: 390, loss is 0.7313080430030823\n",
            "epoch: 1 step: 391, loss is 1.0225193500518799\n",
            "epoch: 1 step: 392, loss is 0.8350239992141724\n",
            "epoch: 1 step: 393, loss is 0.9281755089759827\n",
            "epoch: 1 step: 394, loss is 0.738662838935852\n",
            "epoch: 1 step: 395, loss is 0.8596675992012024\n",
            "epoch: 1 step: 396, loss is 0.8062710165977478\n",
            "epoch: 1 step: 397, loss is 0.9848158955574036\n",
            "epoch: 1 step: 398, loss is 0.9465914368629456\n",
            "epoch: 1 step: 399, loss is 0.9834995865821838\n",
            "epoch: 1 step: 400, loss is 0.8491653203964233\n",
            "epoch: 1 step: 401, loss is 0.7690399885177612\n",
            "epoch: 1 step: 402, loss is 0.858434796333313\n",
            "epoch: 1 step: 403, loss is 0.7625218033790588\n",
            "epoch: 1 step: 404, loss is 0.8136732578277588\n",
            "epoch: 1 step: 405, loss is 0.7719656229019165\n",
            "epoch: 1 step: 406, loss is 0.6582711935043335\n",
            "epoch: 1 step: 407, loss is 0.8903394341468811\n",
            "epoch: 1 step: 408, loss is 0.8407058119773865\n",
            "epoch: 1 step: 409, loss is 0.7424811720848083\n",
            "epoch: 1 step: 410, loss is 1.0001962184906006\n",
            "epoch: 1 step: 411, loss is 0.9567290544509888\n",
            "epoch: 1 step: 412, loss is 0.8081676959991455\n",
            "epoch: 1 step: 413, loss is 0.8720515370368958\n",
            "epoch: 1 step: 414, loss is 0.7426064610481262\n",
            "epoch: 1 step: 415, loss is 0.8443797826766968\n",
            "epoch: 1 step: 416, loss is 0.8559043407440186\n",
            "epoch: 1 step: 417, loss is 0.9926965236663818\n",
            "epoch: 1 step: 418, loss is 0.9298861622810364\n",
            "epoch: 1 step: 419, loss is 0.9624179601669312\n",
            "epoch: 1 step: 420, loss is 1.0616705417633057\n",
            "epoch: 1 step: 421, loss is 0.8404694199562073\n",
            "epoch: 1 step: 422, loss is 0.8695819973945618\n",
            "epoch: 1 step: 423, loss is 0.837142825126648\n",
            "epoch: 1 step: 424, loss is 1.1581218242645264\n",
            "epoch: 1 step: 425, loss is 0.9164914488792419\n",
            "epoch: 1 step: 426, loss is 0.775572657585144\n",
            "epoch: 1 step: 427, loss is 0.9226717352867126\n",
            "epoch: 1 step: 428, loss is 0.7865142226219177\n",
            "epoch: 1 step: 429, loss is 0.7764679193496704\n",
            "epoch: 1 step: 430, loss is 0.9507842659950256\n",
            "epoch: 1 step: 431, loss is 0.9125897884368896\n",
            "epoch: 1 step: 432, loss is 0.9241392016410828\n",
            "epoch: 1 step: 433, loss is 0.7731960415840149\n",
            "epoch: 1 step: 434, loss is 0.9549484848976135\n",
            "epoch: 1 step: 435, loss is 0.8279767632484436\n",
            "epoch: 1 step: 436, loss is 0.956203818321228\n",
            "epoch: 1 step: 437, loss is 0.8654654026031494\n",
            "epoch: 1 step: 438, loss is 0.6925377249717712\n",
            "epoch: 1 step: 439, loss is 0.9399192333221436\n",
            "epoch: 1 step: 440, loss is 0.8768384456634521\n",
            "epoch: 1 step: 441, loss is 0.9341912865638733\n",
            "epoch: 1 step: 442, loss is 0.9760048389434814\n",
            "epoch: 1 step: 443, loss is 0.7250481843948364\n",
            "epoch: 1 step: 444, loss is 0.8126854300498962\n",
            "epoch: 1 step: 445, loss is 1.034214735031128\n",
            "epoch: 1 step: 446, loss is 1.032341480255127\n",
            "epoch: 1 step: 447, loss is 0.7440222501754761\n",
            "epoch: 1 step: 448, loss is 0.8528591394424438\n",
            "epoch: 1 step: 449, loss is 0.8012867569923401\n",
            "epoch: 1 step: 450, loss is 0.8066850304603577\n",
            "epoch: 1 step: 451, loss is 0.9315518736839294\n",
            "epoch: 1 step: 452, loss is 0.8509994745254517\n",
            "epoch: 1 step: 453, loss is 0.7816099524497986\n",
            "epoch: 1 step: 454, loss is 0.8158660531044006\n",
            "epoch: 1 step: 455, loss is 0.692615270614624\n",
            "epoch: 1 step: 456, loss is 0.9143187403678894\n",
            "epoch: 1 step: 457, loss is 0.8834853172302246\n",
            "epoch: 1 step: 458, loss is 0.8045845031738281\n",
            "epoch: 1 step: 459, loss is 0.8587271571159363\n",
            "epoch: 1 step: 460, loss is 1.052320122718811\n",
            "epoch: 1 step: 461, loss is 0.8999255299568176\n",
            "epoch: 1 step: 462, loss is 0.8680299520492554\n",
            "epoch: 1 step: 463, loss is 0.8405682444572449\n",
            "epoch: 1 step: 464, loss is 0.9900592565536499\n",
            "epoch: 1 step: 465, loss is 0.9431512355804443\n",
            "epoch: 1 step: 466, loss is 1.1649975776672363\n",
            "epoch: 1 step: 467, loss is 0.7828192114830017\n",
            "epoch: 1 step: 468, loss is 0.7071108222007751\n",
            "epoch: 1 step: 469, loss is 0.9973216652870178\n",
            "epoch: 1 step: 470, loss is 0.91817706823349\n",
            "epoch: 1 step: 471, loss is 0.9013622999191284\n",
            "epoch: 1 step: 472, loss is 0.9549540281295776\n",
            "epoch: 1 step: 473, loss is 0.864446222782135\n",
            "epoch: 1 step: 474, loss is 0.9254501461982727\n",
            "epoch: 1 step: 475, loss is 0.8545215129852295\n",
            "epoch: 1 step: 476, loss is 0.9572737812995911\n",
            "epoch: 1 step: 477, loss is 0.8434629440307617\n",
            "epoch: 1 step: 478, loss is 0.9395362734794617\n",
            "epoch: 1 step: 479, loss is 0.9319437146186829\n",
            "epoch: 1 step: 480, loss is 0.9438571929931641\n",
            "epoch: 1 step: 481, loss is 0.8731104135513306\n",
            "epoch: 1 step: 482, loss is 0.8921080827713013\n",
            "epoch: 1 step: 483, loss is 0.9652595520019531\n",
            "epoch: 1 step: 484, loss is 0.8440337181091309\n",
            "epoch: 1 step: 485, loss is 0.77827388048172\n",
            "epoch: 1 step: 486, loss is 0.8076388835906982\n",
            "epoch: 1 step: 487, loss is 1.1212835311889648\n",
            "epoch: 1 step: 488, loss is 0.830386221408844\n",
            "epoch: 1 step: 489, loss is 0.8806816935539246\n",
            "epoch: 1 step: 490, loss is 0.9732911586761475\n",
            "epoch: 1 step: 491, loss is 0.8451923727989197\n",
            "epoch: 1 step: 492, loss is 0.7610474824905396\n",
            "epoch: 1 step: 493, loss is 0.9326050877571106\n",
            "epoch: 1 step: 494, loss is 1.0622750520706177\n",
            "epoch: 1 step: 495, loss is 1.0360543727874756\n",
            "epoch: 1 step: 496, loss is 0.9745106101036072\n",
            "epoch: 1 step: 497, loss is 0.8838813304901123\n",
            "epoch: 1 step: 498, loss is 0.7937524318695068\n",
            "epoch: 1 step: 499, loss is 0.8577778935432434\n",
            "epoch: 1 step: 500, loss is 0.8440301418304443\n",
            "epoch: 1 step: 501, loss is 1.062093734741211\n",
            "epoch: 1 step: 502, loss is 0.8949806094169617\n",
            "epoch: 1 step: 503, loss is 0.9396868944168091\n",
            "epoch: 1 step: 504, loss is 0.8353538513183594\n",
            "epoch: 1 step: 505, loss is 0.9347376823425293\n",
            "epoch: 1 step: 506, loss is 1.0131958723068237\n",
            "epoch: 1 step: 507, loss is 0.9115749597549438\n",
            "epoch: 1 step: 508, loss is 0.9382893443107605\n",
            "epoch: 1 step: 509, loss is 0.8272183537483215\n",
            "epoch: 1 step: 510, loss is 0.7651960849761963\n",
            "epoch: 1 step: 511, loss is 1.0238828659057617\n",
            "epoch: 1 step: 512, loss is 0.9741491079330444\n",
            "epoch: 1 step: 513, loss is 0.8800904750823975\n",
            "epoch: 1 step: 514, loss is 0.8614251613616943\n",
            "epoch: 1 step: 515, loss is 1.023140788078308\n",
            "epoch: 1 step: 516, loss is 0.8477458357810974\n",
            "epoch: 1 step: 517, loss is 0.7156287431716919\n",
            "epoch: 1 step: 518, loss is 0.823564887046814\n",
            "epoch: 1 step: 519, loss is 0.941892683506012\n",
            "epoch: 1 step: 520, loss is 0.8898986577987671\n",
            "epoch: 1 step: 521, loss is 0.7860010266304016\n",
            "epoch: 1 step: 522, loss is 0.8257635235786438\n",
            "epoch: 1 step: 523, loss is 0.7599425911903381\n",
            "epoch: 1 step: 524, loss is 0.7681763172149658\n",
            "epoch: 1 step: 525, loss is 0.7720861434936523\n",
            "epoch: 1 step: 526, loss is 0.8682785034179688\n",
            "epoch: 1 step: 527, loss is 0.8692737817764282\n",
            "epoch: 1 step: 528, loss is 0.894910454750061\n",
            "epoch: 1 step: 529, loss is 0.7448189854621887\n",
            "epoch: 1 step: 530, loss is 0.8214399218559265\n",
            "epoch: 1 step: 531, loss is 1.119931697845459\n",
            "epoch: 1 step: 532, loss is 0.786751925945282\n",
            "epoch: 1 step: 533, loss is 0.8769711852073669\n",
            "epoch: 1 step: 534, loss is 0.8854870200157166\n",
            "epoch: 1 step: 535, loss is 0.8789854645729065\n",
            "epoch: 1 step: 536, loss is 0.8128058910369873\n",
            "epoch: 1 step: 537, loss is 0.91901034116745\n",
            "epoch: 1 step: 538, loss is 0.9163930416107178\n",
            "epoch: 1 step: 539, loss is 0.9020495414733887\n",
            "epoch: 1 step: 540, loss is 0.8999090790748596\n",
            "epoch: 1 step: 541, loss is 0.8735248446464539\n",
            "epoch: 1 step: 542, loss is 0.9024519324302673\n",
            "epoch: 1 step: 543, loss is 1.0902847051620483\n",
            "epoch: 1 step: 544, loss is 0.9175340533256531\n",
            "epoch: 1 step: 545, loss is 0.8431606292724609\n",
            "epoch: 1 step: 546, loss is 0.960442066192627\n",
            "epoch: 1 step: 547, loss is 0.9056105017662048\n",
            "epoch: 1 step: 548, loss is 0.852875828742981\n",
            "epoch: 1 step: 549, loss is 0.9369853138923645\n",
            "epoch: 1 step: 550, loss is 0.8720893859863281\n",
            "epoch: 1 step: 551, loss is 0.8888629078865051\n",
            "epoch: 1 step: 552, loss is 1.0039300918579102\n",
            "epoch: 1 step: 553, loss is 0.8195075988769531\n",
            "epoch: 1 step: 554, loss is 0.8647474646568298\n",
            "epoch: 1 step: 555, loss is 0.6820007562637329\n",
            "epoch: 1 step: 556, loss is 0.8327703475952148\n",
            "epoch: 1 step: 557, loss is 0.7429169416427612\n",
            "epoch: 1 step: 558, loss is 0.8793147802352905\n",
            "epoch: 1 step: 559, loss is 0.8810617327690125\n",
            "epoch: 1 step: 560, loss is 0.8149828910827637\n",
            "epoch: 1 step: 561, loss is 0.888261616230011\n",
            "epoch: 1 step: 562, loss is 0.8331077098846436\n",
            "epoch: 1 step: 563, loss is 1.0772676467895508\n",
            "epoch: 1 step: 564, loss is 0.833116888999939\n",
            "epoch: 1 step: 565, loss is 0.9912111163139343\n",
            "epoch: 1 step: 566, loss is 0.8774952292442322\n",
            "epoch: 1 step: 567, loss is 0.666121780872345\n",
            "epoch: 1 step: 568, loss is 1.0903587341308594\n",
            "epoch: 1 step: 569, loss is 0.918342649936676\n",
            "epoch: 1 step: 570, loss is 0.8775342106819153\n",
            "epoch: 1 step: 571, loss is 0.7664971947669983\n",
            "epoch: 1 step: 572, loss is 0.8701500296592712\n",
            "epoch: 1 step: 573, loss is 0.8991656303405762\n",
            "epoch: 1 step: 574, loss is 0.8729956150054932\n",
            "epoch: 1 step: 575, loss is 0.8685693144798279\n",
            "epoch: 1 step: 576, loss is 0.9122976064682007\n",
            "epoch: 1 step: 577, loss is 0.9074485301971436\n",
            "epoch: 1 step: 578, loss is 1.0200002193450928\n",
            "epoch: 1 step: 579, loss is 0.8048914670944214\n",
            "epoch: 1 step: 580, loss is 1.013185977935791\n",
            "epoch: 1 step: 581, loss is 0.7170898914337158\n",
            "epoch: 1 step: 582, loss is 0.7736333608627319\n",
            "epoch: 1 step: 583, loss is 0.9329718947410583\n",
            "epoch: 1 step: 584, loss is 1.013893723487854\n",
            "epoch: 1 step: 585, loss is 0.8999277353286743\n",
            "epoch: 1 step: 586, loss is 0.9131277799606323\n",
            "epoch: 1 step: 587, loss is 0.9679299592971802\n",
            "epoch: 1 step: 588, loss is 0.9108087420463562\n",
            "epoch: 1 step: 589, loss is 0.8658292889595032\n",
            "epoch: 1 step: 590, loss is 0.8900437355041504\n",
            "epoch: 1 step: 591, loss is 0.8635936379432678\n",
            "epoch: 1 step: 592, loss is 0.8513342142105103\n",
            "epoch: 1 step: 593, loss is 0.8133626580238342\n",
            "epoch: 1 step: 594, loss is 0.7906277775764465\n",
            "epoch: 1 step: 595, loss is 0.8763074278831482\n",
            "epoch: 1 step: 596, loss is 1.0200656652450562\n",
            "epoch: 1 step: 597, loss is 0.8212547302246094\n",
            "epoch: 1 step: 598, loss is 0.7497392892837524\n",
            "epoch: 1 step: 599, loss is 0.7280388474464417\n",
            "epoch: 1 step: 600, loss is 0.9206709265708923\n",
            "epoch: 1 step: 601, loss is 1.0065081119537354\n",
            "epoch: 1 step: 602, loss is 0.7865280508995056\n",
            "epoch: 1 step: 603, loss is 0.9939659237861633\n",
            "epoch: 1 step: 604, loss is 0.92286217212677\n",
            "epoch: 1 step: 605, loss is 0.7331514954566956\n",
            "epoch: 1 step: 606, loss is 1.0973293781280518\n",
            "epoch: 1 step: 607, loss is 0.9435886740684509\n",
            "epoch: 1 step: 608, loss is 0.9987869262695312\n",
            "epoch: 1 step: 609, loss is 0.7007488012313843\n",
            "epoch: 1 step: 610, loss is 0.9087454080581665\n",
            "epoch: 1 step: 611, loss is 0.9249593615531921\n",
            "epoch: 1 step: 612, loss is 1.0095206499099731\n",
            "epoch: 1 step: 613, loss is 0.8575899004936218\n",
            "epoch: 1 step: 614, loss is 0.8176366090774536\n",
            "epoch: 1 step: 615, loss is 0.8564313650131226\n",
            "epoch: 1 step: 616, loss is 0.9578039050102234\n",
            "epoch: 1 step: 617, loss is 0.9094188809394836\n",
            "epoch: 1 step: 618, loss is 0.8419489860534668\n",
            "epoch: 1 step: 619, loss is 0.8196783661842346\n",
            "epoch: 1 step: 620, loss is 0.7752501964569092\n",
            "epoch: 1 step: 621, loss is 0.7917066216468811\n",
            "epoch: 1 step: 622, loss is 0.796876847743988\n",
            "epoch: 1 step: 623, loss is 1.0495737791061401\n",
            "epoch: 1 step: 624, loss is 0.9434026479721069\n",
            "epoch: 1 step: 625, loss is 0.6884118318557739\n",
            "epoch: 1 step: 626, loss is 0.9402771592140198\n",
            "epoch: 1 step: 627, loss is 0.933251678943634\n",
            "epoch: 1 step: 628, loss is 0.7575863599777222\n",
            "epoch: 1 step: 629, loss is 0.7859435677528381\n",
            "epoch: 1 step: 630, loss is 0.9001674056053162\n",
            "epoch: 1 step: 631, loss is 0.892630934715271\n",
            "epoch: 1 step: 632, loss is 0.906170129776001\n",
            "epoch: 1 step: 633, loss is 0.8344322443008423\n",
            "epoch: 1 step: 634, loss is 0.7673934698104858\n",
            "epoch: 1 step: 635, loss is 0.9196254014968872\n",
            "epoch: 1 step: 636, loss is 0.9980230927467346\n",
            "epoch: 1 step: 637, loss is 1.074556827545166\n",
            "epoch: 1 step: 638, loss is 0.8617123961448669\n",
            "epoch: 1 step: 639, loss is 0.8580954670906067\n",
            "epoch: 1 step: 640, loss is 0.9504572153091431\n",
            "epoch: 1 step: 641, loss is 0.8138088583946228\n",
            "epoch: 1 step: 642, loss is 0.9821330904960632\n",
            "epoch: 1 step: 643, loss is 0.8514449596405029\n",
            "epoch: 1 step: 644, loss is 0.7718385457992554\n",
            "epoch: 1 step: 645, loss is 0.8606818318367004\n",
            "epoch: 1 step: 646, loss is 1.027658224105835\n",
            "epoch: 1 step: 647, loss is 0.8870163559913635\n",
            "epoch: 1 step: 648, loss is 0.882977306842804\n",
            "epoch: 1 step: 649, loss is 0.9387977123260498\n",
            "epoch: 1 step: 650, loss is 0.8447621464729309\n",
            "epoch: 1 step: 651, loss is 0.6640762686729431\n",
            "epoch: 1 step: 652, loss is 0.8742361664772034\n",
            "epoch: 1 step: 653, loss is 0.9142997860908508\n",
            "epoch: 1 step: 654, loss is 0.960293710231781\n",
            "epoch: 1 step: 655, loss is 1.014678955078125\n",
            "epoch: 1 step: 656, loss is 0.9824095964431763\n",
            "epoch: 1 step: 657, loss is 0.9607179760932922\n",
            "epoch: 1 step: 658, loss is 0.8677748441696167\n",
            "epoch: 1 step: 659, loss is 1.0982767343521118\n",
            "epoch: 1 step: 660, loss is 0.8845135569572449\n",
            "epoch: 1 step: 661, loss is 0.8370947241783142\n",
            "epoch: 1 step: 662, loss is 0.8814263343811035\n",
            "epoch: 1 step: 663, loss is 0.7541456818580627\n",
            "epoch: 1 step: 664, loss is 0.9461730718612671\n",
            "epoch: 1 step: 665, loss is 0.7019021511077881\n",
            "epoch: 1 step: 666, loss is 0.8262031674385071\n",
            "epoch: 1 step: 667, loss is 0.7451937794685364\n",
            "epoch: 1 step: 668, loss is 0.8060949444770813\n",
            "epoch: 1 step: 669, loss is 0.8235068917274475\n",
            "epoch: 1 step: 670, loss is 1.0045177936553955\n",
            "epoch: 1 step: 671, loss is 1.1981337070465088\n",
            "epoch: 1 step: 672, loss is 0.8949016332626343\n",
            "epoch: 1 step: 673, loss is 0.9076620936393738\n",
            "epoch: 1 step: 674, loss is 0.8876645565032959\n",
            "epoch: 1 step: 675, loss is 0.9358885288238525\n",
            "epoch: 1 step: 676, loss is 0.7904680967330933\n",
            "epoch: 1 step: 677, loss is 0.7312262654304504\n",
            "epoch: 1 step: 678, loss is 0.832804262638092\n",
            "epoch: 1 step: 679, loss is 1.0109199285507202\n",
            "epoch: 1 step: 680, loss is 1.0728555917739868\n",
            "epoch: 1 step: 681, loss is 0.8866977095603943\n",
            "epoch: 1 step: 682, loss is 0.8820760250091553\n",
            "epoch: 1 step: 683, loss is 0.9450293183326721\n",
            "epoch: 1 step: 684, loss is 0.9463026523590088\n",
            "epoch: 1 step: 685, loss is 0.8995241522789001\n",
            "epoch: 1 step: 686, loss is 1.0348798036575317\n",
            "epoch: 1 step: 687, loss is 0.9698739051818848\n",
            "epoch: 1 step: 688, loss is 1.0307101011276245\n",
            "epoch: 1 step: 689, loss is 1.0340280532836914\n",
            "epoch: 1 step: 690, loss is 0.8769033551216125\n",
            "epoch: 1 step: 691, loss is 0.8808164596557617\n",
            "epoch: 1 step: 692, loss is 0.9001114964485168\n",
            "epoch: 1 step: 693, loss is 0.8683188557624817\n",
            "epoch: 1 step: 694, loss is 0.9013321995735168\n",
            "epoch: 1 step: 695, loss is 0.8195880651473999\n",
            "epoch: 1 step: 696, loss is 0.791941225528717\n",
            "epoch: 1 step: 697, loss is 0.8705688118934631\n",
            "epoch: 1 step: 698, loss is 0.8855438828468323\n",
            "epoch: 1 step: 699, loss is 0.8476157188415527\n",
            "epoch: 1 step: 700, loss is 0.950037956237793\n",
            "epoch: 1 step: 701, loss is 0.8514678478240967\n",
            "epoch: 1 step: 702, loss is 0.8900649547576904\n",
            "epoch: 1 step: 703, loss is 0.8731677532196045\n",
            "epoch: 1 step: 704, loss is 0.9071497917175293\n",
            "epoch: 1 step: 705, loss is 0.9502120018005371\n",
            "epoch: 1 step: 706, loss is 0.9732963442802429\n",
            "epoch: 1 step: 707, loss is 0.8322088122367859\n",
            "epoch: 1 step: 708, loss is 1.0171724557876587\n",
            "epoch: 1 step: 709, loss is 0.919242799282074\n",
            "epoch: 1 step: 710, loss is 0.9057701230049133\n",
            "epoch: 1 step: 711, loss is 0.9779282808303833\n",
            "epoch: 1 step: 712, loss is 0.9345132112503052\n",
            "epoch: 1 step: 713, loss is 0.7977948784828186\n",
            "epoch: 1 step: 714, loss is 0.8899956345558167\n",
            "epoch: 1 step: 715, loss is 0.7201175093650818\n",
            "epoch: 1 step: 716, loss is 0.8292834162712097\n",
            "epoch: 1 step: 717, loss is 0.9911849498748779\n",
            "epoch: 1 step: 718, loss is 0.835063636302948\n",
            "epoch: 1 step: 719, loss is 1.0188454389572144\n",
            "epoch: 1 step: 720, loss is 0.8529056906700134\n",
            "epoch: 1 step: 721, loss is 0.7805508971214294\n",
            "epoch: 1 step: 722, loss is 0.9980624914169312\n",
            "epoch: 1 step: 723, loss is 0.8756355047225952\n",
            "epoch: 1 step: 724, loss is 0.9020060896873474\n",
            "epoch: 1 step: 725, loss is 0.9304108619689941\n",
            "epoch: 1 step: 726, loss is 0.9953821897506714\n",
            "epoch: 1 step: 727, loss is 0.903844952583313\n",
            "epoch: 1 step: 728, loss is 0.8363717198371887\n",
            "epoch: 1 step: 729, loss is 0.9049320220947266\n",
            "epoch: 1 step: 730, loss is 0.9458365440368652\n",
            "epoch: 1 step: 731, loss is 0.7704523205757141\n",
            "epoch: 1 step: 732, loss is 0.8302174806594849\n",
            "epoch: 1 step: 733, loss is 1.0475525856018066\n",
            "epoch: 1 step: 734, loss is 0.9105578064918518\n",
            "epoch: 1 step: 735, loss is 0.8959764838218689\n",
            "epoch: 1 step: 736, loss is 1.027686357498169\n",
            "epoch: 1 step: 737, loss is 0.7185426354408264\n",
            "epoch: 1 step: 738, loss is 0.8820744752883911\n",
            "epoch: 1 step: 739, loss is 0.9380202889442444\n",
            "epoch: 1 step: 740, loss is 0.719762921333313\n",
            "epoch: 1 step: 741, loss is 0.7880768775939941\n",
            "epoch: 1 step: 742, loss is 1.2475290298461914\n",
            "epoch: 1 step: 743, loss is 0.8584875464439392\n",
            "epoch: 1 step: 744, loss is 0.7607733607292175\n",
            "epoch: 1 step: 745, loss is 0.8004380464553833\n",
            "epoch: 1 step: 746, loss is 0.8440302014350891\n",
            "epoch: 1 step: 747, loss is 0.9444907903671265\n",
            "epoch: 1 step: 748, loss is 0.9024947881698608\n",
            "epoch: 1 step: 749, loss is 0.7707418203353882\n",
            "epoch: 1 step: 750, loss is 1.1510668992996216\n",
            "epoch: 1 step: 751, loss is 1.023004412651062\n",
            "epoch: 1 step: 752, loss is 0.7548227310180664\n",
            "epoch: 1 step: 753, loss is 0.8059895038604736\n",
            "epoch: 1 step: 754, loss is 0.8671355247497559\n",
            "epoch: 1 step: 755, loss is 0.956261932849884\n",
            "epoch: 1 step: 756, loss is 1.0610761642456055\n",
            "epoch: 1 step: 757, loss is 1.0399309396743774\n",
            "epoch: 1 step: 758, loss is 0.7525106072425842\n",
            "epoch: 1 step: 759, loss is 0.823672890663147\n",
            "epoch: 1 step: 760, loss is 0.832399308681488\n",
            "epoch: 1 step: 761, loss is 0.8360301852226257\n",
            "epoch: 1 step: 762, loss is 1.0475660562515259\n",
            "epoch: 1 step: 763, loss is 0.8119028210639954\n",
            "epoch: 1 step: 764, loss is 0.8869572281837463\n",
            "epoch: 1 step: 765, loss is 0.9388054013252258\n",
            "epoch: 1 step: 766, loss is 0.7587177157402039\n",
            "epoch: 1 step: 767, loss is 0.8935794234275818\n",
            "epoch: 1 step: 768, loss is 1.020354986190796\n",
            "epoch: 1 step: 769, loss is 0.8852475881576538\n",
            "epoch: 1 step: 770, loss is 0.8794995546340942\n",
            "epoch: 1 step: 771, loss is 1.0630077123641968\n",
            "epoch: 1 step: 772, loss is 1.0079104900360107\n",
            "epoch: 1 step: 773, loss is 0.9144628047943115\n",
            "epoch: 1 step: 774, loss is 0.7723801136016846\n",
            "epoch: 1 step: 775, loss is 0.9281870126724243\n",
            "epoch: 1 step: 776, loss is 1.0160714387893677\n",
            "epoch: 1 step: 777, loss is 0.8162453174591064\n",
            "epoch: 1 step: 778, loss is 1.0397042036056519\n",
            "epoch: 1 step: 779, loss is 0.9450914859771729\n",
            "epoch: 1 step: 780, loss is 0.7307761311531067\n",
            "epoch: 1 step: 781, loss is 0.7747766971588135\n",
            "epoch: 1 step: 782, loss is 0.739328145980835\n",
            "epoch: 1 step: 783, loss is 0.8008150458335876\n",
            "epoch: 1 step: 784, loss is 0.9013623595237732\n",
            "epoch: 1 step: 785, loss is 0.9479588270187378\n",
            "epoch: 1 step: 786, loss is 0.8883567452430725\n",
            "epoch: 1 step: 787, loss is 0.8981335759162903\n",
            "epoch: 1 step: 788, loss is 0.9717473983764648\n",
            "epoch: 1 step: 789, loss is 0.7803567051887512\n",
            "epoch: 1 step: 790, loss is 0.8803899884223938\n",
            "epoch: 1 step: 791, loss is 0.9943151473999023\n",
            "epoch: 1 step: 792, loss is 0.9098974466323853\n",
            "epoch: 1 step: 793, loss is 1.0745255947113037\n",
            "epoch: 1 step: 794, loss is 0.9848103523254395\n",
            "epoch: 1 step: 795, loss is 0.9798316359519958\n",
            "epoch: 1 step: 796, loss is 0.9722484350204468\n",
            "epoch: 1 step: 797, loss is 0.7943860292434692\n",
            "epoch: 1 step: 798, loss is 1.026183843612671\n",
            "epoch: 1 step: 799, loss is 0.9150615334510803\n",
            "epoch: 1 step: 800, loss is 0.9749929904937744\n",
            "epoch: 1 step: 801, loss is 0.804828405380249\n",
            "epoch: 1 step: 802, loss is 0.9092822670936584\n",
            "epoch: 1 step: 803, loss is 0.9343149662017822\n",
            "epoch: 1 step: 804, loss is 0.8946236371994019\n",
            "epoch: 1 step: 805, loss is 0.9188551306724548\n",
            "epoch: 1 step: 806, loss is 0.7696659564971924\n",
            "epoch: 1 step: 807, loss is 0.8402726054191589\n",
            "epoch: 1 step: 808, loss is 0.8860366344451904\n",
            "epoch: 1 step: 809, loss is 0.8180707693099976\n",
            "epoch: 1 step: 810, loss is 0.9427937269210815\n",
            "epoch: 1 step: 811, loss is 0.8246312141418457\n",
            "epoch: 1 step: 812, loss is 0.9251254796981812\n",
            "epoch: 1 step: 813, loss is 0.7303909063339233\n",
            "epoch: 1 step: 814, loss is 0.7779218554496765\n",
            "epoch: 1 step: 815, loss is 0.9244397282600403\n",
            "epoch: 1 step: 816, loss is 0.7322750091552734\n",
            "epoch: 1 step: 817, loss is 0.7775354981422424\n",
            "epoch: 1 step: 818, loss is 0.8103135824203491\n",
            "epoch: 1 step: 819, loss is 0.7317948937416077\n",
            "epoch: 1 step: 820, loss is 0.8402589559555054\n",
            "epoch: 1 step: 821, loss is 0.9794857501983643\n",
            "epoch: 1 step: 822, loss is 0.8166008591651917\n",
            "epoch: 1 step: 823, loss is 0.8870617151260376\n",
            "epoch: 1 step: 824, loss is 1.1266999244689941\n",
            "epoch: 1 step: 825, loss is 0.8277942538261414\n",
            "epoch: 1 step: 826, loss is 0.9363332986831665\n",
            "epoch: 1 step: 827, loss is 0.8095871210098267\n",
            "epoch: 1 step: 828, loss is 0.6651450395584106\n",
            "epoch: 1 step: 829, loss is 0.7570312023162842\n",
            "epoch: 1 step: 830, loss is 0.9009861946105957\n",
            "epoch: 1 step: 831, loss is 0.8114980459213257\n",
            "epoch: 1 step: 832, loss is 0.8450355529785156\n",
            "epoch: 1 step: 833, loss is 0.9368212819099426\n",
            "epoch: 1 step: 834, loss is 0.9185646176338196\n",
            "epoch: 1 step: 835, loss is 0.9426048398017883\n",
            "epoch: 1 step: 836, loss is 0.8963448405265808\n",
            "epoch: 1 step: 837, loss is 0.9876797795295715\n",
            "epoch: 1 step: 838, loss is 0.8367344737052917\n",
            "epoch: 1 step: 839, loss is 0.8600749969482422\n",
            "epoch: 1 step: 840, loss is 0.8319833278656006\n",
            "epoch: 1 step: 841, loss is 0.9377784132957458\n",
            "epoch: 1 step: 842, loss is 0.920494019985199\n",
            "epoch: 1 step: 843, loss is 1.0312221050262451\n",
            "epoch: 1 step: 844, loss is 0.8472254276275635\n",
            "epoch: 1 step: 845, loss is 1.0307316780090332\n",
            "epoch: 1 step: 846, loss is 0.9741999506950378\n",
            "epoch: 1 step: 847, loss is 0.8266245126724243\n",
            "epoch: 1 step: 848, loss is 1.071698546409607\n",
            "epoch: 1 step: 849, loss is 0.9310272336006165\n",
            "epoch: 1 step: 850, loss is 0.8423885107040405\n",
            "epoch: 1 step: 851, loss is 0.9040761590003967\n",
            "epoch: 1 step: 852, loss is 0.9556587934494019\n",
            "epoch: 1 step: 853, loss is 0.8161340355873108\n",
            "epoch: 1 step: 854, loss is 0.9293060898780823\n",
            "epoch: 1 step: 855, loss is 0.8323987722396851\n",
            "epoch: 1 step: 856, loss is 1.0678095817565918\n",
            "epoch: 1 step: 857, loss is 0.9071847200393677\n",
            "epoch: 1 step: 858, loss is 0.9275577068328857\n",
            "epoch: 1 step: 859, loss is 0.9408470988273621\n",
            "epoch: 1 step: 860, loss is 0.9556635618209839\n",
            "epoch: 1 step: 861, loss is 0.9679407477378845\n",
            "epoch: 1 step: 862, loss is 0.8282410502433777\n",
            "epoch: 1 step: 863, loss is 0.9274511337280273\n",
            "epoch: 1 step: 864, loss is 0.847683310508728\n",
            "epoch: 1 step: 865, loss is 1.0501768589019775\n",
            "epoch: 1 step: 866, loss is 0.9467313289642334\n",
            "epoch: 1 step: 867, loss is 0.8686544895172119\n",
            "epoch: 1 step: 868, loss is 0.8836075663566589\n",
            "epoch: 1 step: 869, loss is 0.809501588344574\n",
            "epoch: 1 step: 870, loss is 0.8582407236099243\n",
            "epoch: 1 step: 871, loss is 0.9352318644523621\n",
            "epoch: 1 step: 872, loss is 1.0293349027633667\n",
            "epoch: 1 step: 873, loss is 0.8387751579284668\n",
            "epoch: 1 step: 874, loss is 0.7593177556991577\n",
            "epoch: 1 step: 875, loss is 0.9359919428825378\n",
            "epoch: 1 step: 876, loss is 0.8216146230697632\n",
            "epoch: 1 step: 877, loss is 0.9577203392982483\n",
            "epoch: 1 step: 878, loss is 0.8070096373558044\n",
            "epoch: 1 step: 879, loss is 0.8731052279472351\n",
            "epoch: 1 step: 880, loss is 1.011410117149353\n",
            "epoch: 1 step: 881, loss is 0.780524492263794\n",
            "epoch: 1 step: 882, loss is 0.908846914768219\n",
            "epoch: 1 step: 883, loss is 0.8674120306968689\n",
            "epoch: 1 step: 884, loss is 0.8969625234603882\n",
            "epoch: 1 step: 885, loss is 0.8264341354370117\n",
            "epoch: 1 step: 886, loss is 0.7998620271682739\n",
            "epoch: 1 step: 887, loss is 0.9612389802932739\n",
            "epoch: 1 step: 888, loss is 1.1056404113769531\n",
            "epoch: 1 step: 889, loss is 0.7753380537033081\n",
            "epoch: 1 step: 890, loss is 0.9200652837753296\n",
            "epoch: 1 step: 891, loss is 1.0727506875991821\n",
            "epoch: 1 step: 892, loss is 0.9723082780838013\n",
            "epoch: 1 step: 893, loss is 0.8881471157073975\n",
            "epoch: 1 step: 894, loss is 0.9534947276115417\n",
            "epoch: 1 step: 895, loss is 0.7219197154045105\n",
            "epoch: 1 step: 896, loss is 0.9287234544754028\n",
            "epoch: 1 step: 897, loss is 0.9298746585845947\n",
            "epoch: 1 step: 898, loss is 0.8621677756309509\n",
            "epoch: 1 step: 899, loss is 1.0577296018600464\n",
            "epoch: 1 step: 900, loss is 0.961021363735199\n",
            "epoch: 1 step: 901, loss is 1.008548378944397\n",
            "epoch: 1 step: 902, loss is 0.7874143123626709\n",
            "epoch: 1 step: 903, loss is 0.9564940929412842\n",
            "epoch: 1 step: 904, loss is 0.8388689756393433\n",
            "epoch: 1 step: 905, loss is 0.8638572096824646\n",
            "epoch: 1 step: 906, loss is 0.8148281574249268\n",
            "epoch: 1 step: 907, loss is 0.8403246402740479\n",
            "epoch: 1 step: 908, loss is 0.9065893888473511\n",
            "epoch: 1 step: 909, loss is 0.8270826935768127\n",
            "epoch: 1 step: 910, loss is 0.8531745672225952\n",
            "epoch: 1 step: 911, loss is 0.8365144729614258\n",
            "epoch: 1 step: 912, loss is 0.979040801525116\n",
            "epoch: 1 step: 913, loss is 0.8641582727432251\n",
            "epoch: 1 step: 914, loss is 0.8603349328041077\n",
            "epoch: 1 step: 915, loss is 0.865928053855896\n",
            "epoch: 1 step: 916, loss is 0.8553152680397034\n",
            "epoch: 1 step: 917, loss is 0.7554822564125061\n",
            "epoch: 1 step: 918, loss is 1.0203648805618286\n",
            "epoch: 1 step: 919, loss is 0.698020339012146\n",
            "epoch: 1 step: 920, loss is 0.8538900017738342\n",
            "epoch: 1 step: 921, loss is 0.6578329205513\n",
            "epoch: 1 step: 922, loss is 0.6614134311676025\n",
            "epoch: 1 step: 923, loss is 0.8739932179450989\n",
            "epoch: 1 step: 924, loss is 1.160064458847046\n",
            "epoch: 1 step: 925, loss is 1.004696011543274\n",
            "epoch: 1 step: 926, loss is 0.9725537896156311\n",
            "epoch: 1 step: 927, loss is 0.9800851941108704\n",
            "epoch: 1 step: 928, loss is 0.7640102505683899\n",
            "epoch: 1 step: 929, loss is 1.0128599405288696\n",
            "epoch: 1 step: 930, loss is 0.8559896945953369\n",
            "epoch: 1 step: 931, loss is 0.9282596111297607\n",
            "epoch: 1 step: 932, loss is 0.8583857417106628\n",
            "epoch: 1 step: 933, loss is 0.9525274038314819\n",
            "epoch: 1 step: 934, loss is 0.8560298085212708\n",
            "epoch: 1 step: 935, loss is 0.8977576494216919\n",
            "epoch: 1 step: 936, loss is 0.7883008122444153\n",
            "epoch: 1 step: 937, loss is 0.967094898223877\n",
            "epoch: 1 step: 938, loss is 1.0014801025390625\n",
            "epoch: 1 step: 939, loss is 1.001979947090149\n",
            "epoch: 1 step: 940, loss is 0.9103255867958069\n",
            "epoch: 1 step: 941, loss is 0.9242902994155884\n",
            "epoch: 1 step: 942, loss is 0.9307035803794861\n",
            "epoch: 1 step: 943, loss is 0.9949836730957031\n",
            "epoch: 1 step: 944, loss is 0.8261470198631287\n",
            "epoch: 1 step: 945, loss is 0.8762257695198059\n",
            "epoch: 1 step: 946, loss is 0.9845737218856812\n",
            "epoch: 1 step: 947, loss is 0.922623872756958\n",
            "epoch: 1 step: 948, loss is 0.9933306574821472\n",
            "epoch: 1 step: 949, loss is 0.9763001799583435\n",
            "epoch: 1 step: 950, loss is 0.8893651962280273\n",
            "epoch: 1 step: 951, loss is 0.8707020878791809\n",
            "epoch: 1 step: 952, loss is 0.8421580195426941\n",
            "epoch: 1 step: 953, loss is 0.7765185832977295\n",
            "epoch: 1 step: 954, loss is 1.0500431060791016\n",
            "epoch: 1 step: 955, loss is 0.8592122197151184\n",
            "epoch: 1 step: 956, loss is 0.7639518976211548\n",
            "epoch: 1 step: 957, loss is 1.0103179216384888\n",
            "epoch: 1 step: 958, loss is 0.7875742316246033\n",
            "epoch: 1 step: 959, loss is 0.755089521408081\n",
            "epoch: 1 step: 960, loss is 0.9108870029449463\n",
            "epoch: 1 step: 961, loss is 0.9765902757644653\n",
            "epoch: 1 step: 962, loss is 0.9206182956695557\n",
            "epoch: 1 step: 963, loss is 0.9558596014976501\n",
            "epoch: 1 step: 964, loss is 0.9078558087348938\n",
            "epoch: 1 step: 965, loss is 0.8721516132354736\n",
            "epoch: 1 step: 966, loss is 0.9268120527267456\n",
            "epoch: 1 step: 967, loss is 0.9686160683631897\n",
            "epoch: 1 step: 968, loss is 0.9205323457717896\n",
            "epoch: 1 step: 969, loss is 0.876555860042572\n",
            "epoch: 1 step: 970, loss is 0.7946420311927795\n",
            "epoch: 1 step: 971, loss is 0.8186823725700378\n",
            "epoch: 1 step: 972, loss is 0.7636367678642273\n",
            "epoch: 1 step: 973, loss is 0.7926597595214844\n",
            "epoch: 1 step: 974, loss is 0.9833076000213623\n",
            "epoch: 1 step: 975, loss is 0.8895178437232971\n",
            "epoch: 1 step: 976, loss is 1.0240122079849243\n",
            "epoch: 1 step: 977, loss is 0.945040225982666\n",
            "epoch: 1 step: 978, loss is 0.9489108920097351\n",
            "epoch: 1 step: 979, loss is 0.8583003282546997\n",
            "epoch: 1 step: 980, loss is 1.0991190671920776\n",
            "epoch: 1 step: 981, loss is 0.7376269698143005\n",
            "epoch: 1 step: 982, loss is 0.8043932318687439\n",
            "epoch: 1 step: 983, loss is 0.948125958442688\n",
            "epoch: 1 step: 984, loss is 0.9329783916473389\n",
            "epoch: 1 step: 985, loss is 0.727570652961731\n",
            "epoch: 1 step: 986, loss is 1.0232734680175781\n",
            "epoch: 1 step: 987, loss is 0.9838688373565674\n",
            "epoch: 1 step: 988, loss is 0.9682525396347046\n",
            "epoch: 1 step: 989, loss is 0.9942739605903625\n",
            "epoch: 1 step: 990, loss is 0.8527603149414062\n",
            "epoch: 1 step: 991, loss is 0.7460379600524902\n",
            "epoch: 1 step: 992, loss is 0.9883620142936707\n",
            "epoch: 1 step: 993, loss is 1.0350546836853027\n",
            "epoch: 1 step: 994, loss is 1.1367825269699097\n",
            "epoch: 1 step: 995, loss is 0.913400411605835\n",
            "epoch: 1 step: 996, loss is 1.128495693206787\n",
            "epoch: 1 step: 997, loss is 0.8100897669792175\n",
            "epoch: 1 step: 998, loss is 0.7346076965332031\n",
            "epoch: 1 step: 999, loss is 0.6936798691749573\n",
            "epoch: 1 step: 1000, loss is 0.9867740869522095\n",
            "epoch: 1 step: 1001, loss is 0.894766628742218\n",
            "epoch: 1 step: 1002, loss is 0.9728797078132629\n",
            "epoch: 1 step: 1003, loss is 0.7943918108940125\n",
            "epoch: 1 step: 1004, loss is 0.9336017370223999\n",
            "epoch: 1 step: 1005, loss is 0.94324791431427\n",
            "epoch: 1 step: 1006, loss is 0.8014617562294006\n",
            "epoch: 1 step: 1007, loss is 0.9627901911735535\n",
            "epoch: 1 step: 1008, loss is 0.7509185671806335\n",
            "epoch: 1 step: 1009, loss is 1.0639894008636475\n",
            "epoch: 1 step: 1010, loss is 0.9778873920440674\n",
            "epoch: 1 step: 1011, loss is 0.7293136715888977\n",
            "epoch: 1 step: 1012, loss is 0.9845090508460999\n",
            "epoch: 1 step: 1013, loss is 0.7719507813453674\n",
            "epoch: 1 step: 1014, loss is 0.9713620543479919\n",
            "epoch: 1 step: 1015, loss is 0.9441936612129211\n",
            "epoch: 1 step: 1016, loss is 0.9271010160446167\n",
            "epoch: 1 step: 1017, loss is 0.9675700068473816\n",
            "epoch: 1 step: 1018, loss is 0.8396151661872864\n",
            "epoch: 1 step: 1019, loss is 0.8737095594406128\n",
            "epoch: 1 step: 1020, loss is 0.8965765237808228\n",
            "epoch: 1 step: 1021, loss is 0.8987379670143127\n",
            "epoch: 1 step: 1022, loss is 0.9354939460754395\n",
            "epoch: 1 step: 1023, loss is 0.9316177368164062\n",
            "epoch: 1 step: 1024, loss is 0.7582874894142151\n",
            "epoch: 1 step: 1025, loss is 0.8785190582275391\n",
            "epoch: 1 step: 1026, loss is 0.8950378894805908\n",
            "epoch: 1 step: 1027, loss is 1.0201308727264404\n",
            "epoch: 1 step: 1028, loss is 0.7971879243850708\n",
            "epoch: 1 step: 1029, loss is 0.8385435938835144\n",
            "epoch: 1 step: 1030, loss is 1.0567519664764404\n",
            "epoch: 1 step: 1031, loss is 0.9529792070388794\n",
            "epoch: 1 step: 1032, loss is 0.8081993460655212\n",
            "epoch: 1 step: 1033, loss is 1.0223406553268433\n",
            "epoch: 1 step: 1034, loss is 0.9795190095901489\n",
            "epoch: 1 step: 1035, loss is 0.8812703490257263\n",
            "epoch: 1 step: 1036, loss is 0.8954665064811707\n",
            "epoch: 1 step: 1037, loss is 0.9256427884101868\n",
            "epoch: 1 step: 1038, loss is 0.9211121201515198\n",
            "epoch: 1 step: 1039, loss is 0.9754945635795593\n",
            "epoch: 1 step: 1040, loss is 0.97941654920578\n",
            "epoch: 1 step: 1041, loss is 0.8676685094833374\n",
            "epoch: 1 step: 1042, loss is 0.7150072455406189\n",
            "epoch: 1 step: 1043, loss is 0.9099177122116089\n",
            "epoch: 1 step: 1044, loss is 0.9017921686172485\n",
            "epoch: 1 step: 1045, loss is 0.8514258861541748\n",
            "epoch: 1 step: 1046, loss is 0.8854950666427612\n",
            "epoch: 1 step: 1047, loss is 0.8660610914230347\n",
            "epoch: 1 step: 1048, loss is 0.8433210849761963\n",
            "epoch: 1 step: 1049, loss is 0.9575352668762207\n",
            "epoch: 1 step: 1050, loss is 0.8703334331512451\n",
            "epoch: 1 step: 1051, loss is 0.9085681438446045\n",
            "epoch: 1 step: 1052, loss is 0.8315696716308594\n",
            "epoch: 1 step: 1053, loss is 0.8431713581085205\n",
            "epoch: 1 step: 1054, loss is 1.0175596475601196\n",
            "epoch: 1 step: 1055, loss is 0.9441343545913696\n",
            "epoch: 1 step: 1056, loss is 0.8049159646034241\n",
            "epoch: 1 step: 1057, loss is 0.7842600345611572\n",
            "epoch: 1 step: 1058, loss is 0.8563536405563354\n",
            "epoch: 1 step: 1059, loss is 0.7647258043289185\n",
            "epoch: 1 step: 1060, loss is 0.9838482737541199\n",
            "epoch: 1 step: 1061, loss is 0.9129037857055664\n",
            "epoch: 1 step: 1062, loss is 0.8383967876434326\n",
            "epoch: 1 step: 1063, loss is 1.0792431831359863\n",
            "epoch: 1 step: 1064, loss is 0.8502229452133179\n",
            "epoch: 1 step: 1065, loss is 1.0290334224700928\n",
            "epoch: 1 step: 1066, loss is 0.8796316981315613\n",
            "epoch: 1 step: 1067, loss is 0.972507655620575\n",
            "epoch: 1 step: 1068, loss is 0.8708220720291138\n",
            "epoch: 1 step: 1069, loss is 0.8758416771888733\n",
            "epoch: 1 step: 1070, loss is 0.8645539879798889\n",
            "epoch: 1 step: 1071, loss is 0.9658142924308777\n",
            "epoch: 1 step: 1072, loss is 0.8861971497535706\n",
            "epoch: 1 step: 1073, loss is 0.8092029690742493\n",
            "epoch: 1 step: 1074, loss is 0.8334800601005554\n",
            "epoch: 1 step: 1075, loss is 0.850570559501648\n",
            "epoch: 1 step: 1076, loss is 0.6954026222229004\n",
            "epoch: 1 step: 1077, loss is 0.8770037889480591\n",
            "epoch: 1 step: 1078, loss is 0.8454963564872742\n",
            "epoch: 1 step: 1079, loss is 0.750598132610321\n",
            "epoch: 1 step: 1080, loss is 0.8363006711006165\n",
            "epoch: 1 step: 1081, loss is 0.9006808400154114\n",
            "epoch: 1 step: 1082, loss is 1.0507664680480957\n",
            "epoch: 1 step: 1083, loss is 0.8733113408088684\n",
            "epoch: 1 step: 1084, loss is 0.867609977722168\n",
            "epoch: 1 step: 1085, loss is 0.8630699515342712\n",
            "epoch: 1 step: 1086, loss is 0.9838540554046631\n",
            "epoch: 1 step: 1087, loss is 1.050423502922058\n",
            "epoch: 1 step: 1088, loss is 1.0149753093719482\n",
            "epoch: 1 step: 1089, loss is 0.9959412217140198\n",
            "epoch: 1 step: 1090, loss is 0.8473214507102966\n",
            "epoch: 1 step: 1091, loss is 0.7146943807601929\n",
            "epoch: 1 step: 1092, loss is 0.8958622217178345\n",
            "epoch: 1 step: 1093, loss is 1.027998447418213\n",
            "epoch: 1 step: 1094, loss is 0.8088036775588989\n",
            "epoch: 1 step: 1095, loss is 0.8570443391799927\n",
            "epoch: 1 step: 1096, loss is 0.9890292882919312\n",
            "epoch: 1 step: 1097, loss is 0.959465503692627\n",
            "epoch: 1 step: 1098, loss is 0.8533326387405396\n",
            "epoch: 1 step: 1099, loss is 1.1150972843170166\n",
            "epoch: 1 step: 1100, loss is 0.9280158281326294\n",
            "epoch: 1 step: 1101, loss is 0.6868231296539307\n",
            "epoch: 1 step: 1102, loss is 0.9209875464439392\n",
            "epoch: 1 step: 1103, loss is 0.8955675959587097\n",
            "epoch: 1 step: 1104, loss is 1.0724290609359741\n",
            "epoch: 1 step: 1105, loss is 0.7070498466491699\n",
            "epoch: 1 step: 1106, loss is 0.9040791988372803\n",
            "epoch: 1 step: 1107, loss is 0.9315622448921204\n",
            "epoch: 1 step: 1108, loss is 0.7629246711730957\n",
            "epoch: 1 step: 1109, loss is 0.9545148611068726\n",
            "epoch: 1 step: 1110, loss is 0.9107015132904053\n",
            "epoch: 1 step: 1111, loss is 1.0446372032165527\n",
            "epoch: 1 step: 1112, loss is 0.8541077971458435\n",
            "epoch: 1 step: 1113, loss is 0.9181655049324036\n",
            "epoch: 1 step: 1114, loss is 0.8380603790283203\n",
            "epoch: 1 step: 1115, loss is 0.8585667610168457\n",
            "epoch: 1 step: 1116, loss is 0.7560039162635803\n",
            "epoch: 1 step: 1117, loss is 1.0746541023254395\n",
            "epoch: 1 step: 1118, loss is 0.9138104319572449\n",
            "epoch: 1 step: 1119, loss is 1.1887502670288086\n",
            "epoch: 1 step: 1120, loss is 0.9192950129508972\n",
            "epoch: 1 step: 1121, loss is 1.0281223058700562\n",
            "epoch: 1 step: 1122, loss is 1.046730637550354\n",
            "epoch: 1 step: 1123, loss is 0.9412241578102112\n",
            "epoch: 1 step: 1124, loss is 1.0325582027435303\n",
            "epoch: 1 step: 1125, loss is 0.8435326218605042\n",
            "epoch: 1 step: 1126, loss is 0.8833089470863342\n",
            "epoch: 1 step: 1127, loss is 0.8796133995056152\n",
            "epoch: 1 step: 1128, loss is 0.7556487321853638\n",
            "epoch: 1 step: 1129, loss is 0.8931394815444946\n",
            "epoch: 1 step: 1130, loss is 0.9720331430435181\n",
            "epoch: 1 step: 1131, loss is 0.9836784601211548\n",
            "epoch: 1 step: 1132, loss is 0.8386419415473938\n",
            "epoch: 1 step: 1133, loss is 0.9014767408370972\n",
            "epoch: 1 step: 1134, loss is 1.0220247507095337\n",
            "epoch: 1 step: 1135, loss is 0.6861597895622253\n",
            "epoch: 1 step: 1136, loss is 0.8557353019714355\n",
            "epoch: 1 step: 1137, loss is 0.921593427658081\n",
            "epoch: 1 step: 1138, loss is 0.9190467000007629\n",
            "epoch: 1 step: 1139, loss is 0.7756394743919373\n",
            "epoch: 1 step: 1140, loss is 0.9102370142936707\n",
            "epoch: 1 step: 1141, loss is 0.7452536225318909\n",
            "epoch: 1 step: 1142, loss is 0.8929454684257507\n",
            "epoch: 1 step: 1143, loss is 0.9215430617332458\n",
            "epoch: 1 step: 1144, loss is 0.8687852025032043\n",
            "epoch: 1 step: 1145, loss is 0.8722935914993286\n",
            "epoch: 1 step: 1146, loss is 0.8560442924499512\n",
            "epoch: 1 step: 1147, loss is 0.940049946308136\n",
            "epoch: 1 step: 1148, loss is 0.9303188323974609\n",
            "epoch: 1 step: 1149, loss is 0.8045092821121216\n",
            "epoch: 1 step: 1150, loss is 0.9956837296485901\n",
            "epoch: 1 step: 1151, loss is 0.8090962767601013\n",
            "epoch: 1 step: 1152, loss is 0.9409502148628235\n",
            "epoch: 1 step: 1153, loss is 1.0141409635543823\n",
            "epoch: 1 step: 1154, loss is 0.8792678117752075\n",
            "epoch: 1 step: 1155, loss is 0.8330972790718079\n",
            "epoch: 1 step: 1156, loss is 0.745353102684021\n",
            "epoch: 1 step: 1157, loss is 0.7156189680099487\n",
            "epoch: 1 step: 1158, loss is 1.0049331188201904\n",
            "epoch: 1 step: 1159, loss is 0.853671669960022\n",
            "epoch: 1 step: 1160, loss is 0.838761031627655\n",
            "epoch: 1 step: 1161, loss is 0.903245747089386\n",
            "epoch: 1 step: 1162, loss is 0.9694302082061768\n",
            "epoch: 1 step: 1163, loss is 0.8225940465927124\n",
            "epoch: 1 step: 1164, loss is 0.948764443397522\n",
            "epoch: 1 step: 1165, loss is 0.8570472002029419\n",
            "epoch: 1 step: 1166, loss is 0.9160041213035583\n",
            "epoch: 1 step: 1167, loss is 0.8144603371620178\n",
            "epoch: 1 step: 1168, loss is 0.91192227602005\n",
            "epoch: 1 step: 1169, loss is 0.7782089114189148\n",
            "epoch: 1 step: 1170, loss is 0.9971956014633179\n",
            "epoch: 1 step: 1171, loss is 0.8896791934967041\n",
            "epoch: 1 step: 1172, loss is 0.8142193555831909\n",
            "epoch: 1 step: 1173, loss is 0.878171443939209\n",
            "epoch: 1 step: 1174, loss is 0.7778125405311584\n",
            "epoch: 1 step: 1175, loss is 0.8897254467010498\n",
            "epoch: 1 step: 1176, loss is 1.0252721309661865\n",
            "epoch: 1 step: 1177, loss is 0.9207595586776733\n",
            "epoch: 1 step: 1178, loss is 0.7887199521064758\n",
            "epoch: 1 step: 1179, loss is 0.8941539525985718\n",
            "epoch: 1 step: 1180, loss is 0.7871307730674744\n",
            "epoch: 1 step: 1181, loss is 0.856337308883667\n",
            "epoch: 1 step: 1182, loss is 1.0055367946624756\n",
            "epoch: 1 step: 1183, loss is 0.7131401300430298\n",
            "epoch: 1 step: 1184, loss is 0.8967874646186829\n",
            "epoch: 1 step: 1185, loss is 0.943696141242981\n",
            "epoch: 1 step: 1186, loss is 0.8657115697860718\n",
            "epoch: 1 step: 1187, loss is 0.822481632232666\n",
            "epoch: 1 step: 1188, loss is 0.9996898174285889\n",
            "epoch: 1 step: 1189, loss is 0.9969215989112854\n",
            "epoch: 1 step: 1190, loss is 1.1500937938690186\n",
            "epoch: 1 step: 1191, loss is 1.028935194015503\n",
            "epoch: 1 step: 1192, loss is 0.7747138142585754\n",
            "epoch: 1 step: 1193, loss is 1.2822535037994385\n",
            "epoch: 1 step: 1194, loss is 0.9670190811157227\n",
            "epoch: 1 step: 1195, loss is 0.8307521343231201\n",
            "epoch: 1 step: 1196, loss is 0.8927745819091797\n",
            "epoch: 1 step: 1197, loss is 0.8126858472824097\n",
            "epoch: 1 step: 1198, loss is 0.9356674551963806\n",
            "epoch: 1 step: 1199, loss is 1.0226826667785645\n",
            "epoch: 1 step: 1200, loss is 0.7793843150138855\n",
            "epoch: 1 step: 1201, loss is 1.0029025077819824\n",
            "epoch: 1 step: 1202, loss is 0.8618630766868591\n",
            "epoch: 1 step: 1203, loss is 0.850872278213501\n",
            "epoch: 1 step: 1204, loss is 0.9810428619384766\n",
            "epoch: 1 step: 1205, loss is 0.7254159450531006\n",
            "epoch: 1 step: 1206, loss is 1.0071953535079956\n",
            "epoch: 1 step: 1207, loss is 0.9774391651153564\n",
            "epoch: 1 step: 1208, loss is 0.9185115694999695\n",
            "epoch: 1 step: 1209, loss is 1.016479730606079\n",
            "epoch: 1 step: 1210, loss is 0.9039742946624756\n",
            "epoch: 1 step: 1211, loss is 0.78486168384552\n",
            "epoch: 1 step: 1212, loss is 0.785483181476593\n",
            "epoch: 1 step: 1213, loss is 0.8080105185508728\n",
            "epoch: 1 step: 1214, loss is 0.8752913475036621\n",
            "epoch: 1 step: 1215, loss is 0.9356256127357483\n",
            "epoch: 1 step: 1216, loss is 0.9324170351028442\n",
            "epoch: 1 step: 1217, loss is 0.9980242252349854\n",
            "epoch: 1 step: 1218, loss is 0.9721864461898804\n",
            "epoch: 1 step: 1219, loss is 1.09954833984375\n",
            "epoch: 1 step: 1220, loss is 0.800981879234314\n",
            "epoch: 1 step: 1221, loss is 0.8876333832740784\n",
            "epoch: 1 step: 1222, loss is 0.9728643298149109\n",
            "epoch: 1 step: 1223, loss is 0.9556750059127808\n",
            "epoch: 1 step: 1224, loss is 1.017111897468567\n",
            "epoch: 1 step: 1225, loss is 0.8561158776283264\n",
            "epoch: 1 step: 1226, loss is 1.027780294418335\n",
            "epoch: 1 step: 1227, loss is 0.7464907765388489\n",
            "epoch: 1 step: 1228, loss is 0.935677707195282\n",
            "epoch: 1 step: 1229, loss is 0.8564788103103638\n",
            "epoch: 1 step: 1230, loss is 0.9631126523017883\n",
            "epoch: 1 step: 1231, loss is 0.8919188380241394\n",
            "epoch: 1 step: 1232, loss is 0.9413138628005981\n",
            "epoch: 1 step: 1233, loss is 0.787716269493103\n",
            "epoch: 1 step: 1234, loss is 0.8491449356079102\n",
            "epoch: 1 step: 1235, loss is 0.7992100715637207\n",
            "epoch: 1 step: 1236, loss is 0.741337776184082\n",
            "epoch: 1 step: 1237, loss is 0.8012215495109558\n",
            "epoch: 1 step: 1238, loss is 0.9822283983230591\n",
            "epoch: 1 step: 1239, loss is 0.9877577424049377\n",
            "epoch: 1 step: 1240, loss is 0.767731249332428\n",
            "epoch: 1 step: 1241, loss is 0.7668226957321167\n",
            "epoch: 1 step: 1242, loss is 0.8364956378936768\n",
            "epoch: 1 step: 1243, loss is 1.050178050994873\n",
            "epoch: 1 step: 1244, loss is 1.0651031732559204\n",
            "epoch: 1 step: 1245, loss is 1.0608872175216675\n",
            "epoch: 1 step: 1246, loss is 0.9616246223449707\n",
            "epoch: 1 step: 1247, loss is 0.814561665058136\n",
            "epoch: 1 step: 1248, loss is 0.729449987411499\n",
            "epoch: 1 step: 1249, loss is 0.9129593372344971\n",
            "epoch: 1 step: 1250, loss is 0.9802724123001099\n",
            "epoch: 1 step: 1251, loss is 0.8132203817367554\n",
            "epoch: 1 step: 1252, loss is 0.8827977180480957\n",
            "epoch: 1 step: 1253, loss is 0.8454153537750244\n",
            "epoch: 1 step: 1254, loss is 0.7016757130622864\n",
            "epoch: 1 step: 1255, loss is 0.6880916953086853\n",
            "epoch: 1 step: 1256, loss is 1.072133183479309\n",
            "epoch: 1 step: 1257, loss is 0.9831917881965637\n",
            "epoch: 1 step: 1258, loss is 0.8433849811553955\n",
            "epoch: 1 step: 1259, loss is 1.0687986612319946\n",
            "epoch: 1 step: 1260, loss is 0.7794378399848938\n",
            "epoch: 1 step: 1261, loss is 0.7498195171356201\n",
            "epoch: 1 step: 1262, loss is 0.7223068475723267\n",
            "epoch: 1 step: 1263, loss is 0.6582736372947693\n",
            "epoch: 1 step: 1264, loss is 0.7118411064147949\n",
            "epoch: 1 step: 1265, loss is 0.7905577421188354\n",
            "epoch: 1 step: 1266, loss is 0.8719696402549744\n",
            "epoch: 1 step: 1267, loss is 0.9069446921348572\n",
            "epoch: 1 step: 1268, loss is 0.7835317254066467\n",
            "epoch: 1 step: 1269, loss is 0.8531018495559692\n",
            "epoch: 1 step: 1270, loss is 1.06523597240448\n",
            "epoch: 1 step: 1271, loss is 0.8328901529312134\n",
            "epoch: 1 step: 1272, loss is 0.9950769543647766\n",
            "epoch: 1 step: 1273, loss is 0.8825333118438721\n",
            "epoch: 1 step: 1274, loss is 0.7340054512023926\n",
            "epoch: 1 step: 1275, loss is 0.7996466755867004\n",
            "epoch: 1 step: 1276, loss is 0.7575982809066772\n",
            "epoch: 1 step: 1277, loss is 0.6908435821533203\n",
            "epoch: 1 step: 1278, loss is 0.9407665729522705\n",
            "epoch: 1 step: 1279, loss is 0.9622479677200317\n",
            "epoch: 1 step: 1280, loss is 0.8047477602958679\n",
            "epoch: 1 step: 1281, loss is 0.93067866563797\n",
            "epoch: 1 step: 1282, loss is 1.0697829723358154\n",
            "epoch: 1 step: 1283, loss is 0.8230528235435486\n",
            "epoch: 1 step: 1284, loss is 0.7921479940414429\n",
            "epoch: 1 step: 1285, loss is 0.9478580355644226\n",
            "epoch: 1 step: 1286, loss is 0.7022482752799988\n",
            "epoch: 1 step: 1287, loss is 0.9847182631492615\n",
            "epoch: 1 step: 1288, loss is 0.7714976072311401\n",
            "epoch: 1 step: 1289, loss is 0.674131453037262\n",
            "epoch: 1 step: 1290, loss is 0.7453523874282837\n",
            "epoch: 1 step: 1291, loss is 0.7491127252578735\n",
            "epoch: 1 step: 1292, loss is 1.144690990447998\n",
            "epoch: 1 step: 1293, loss is 0.9288750886917114\n",
            "epoch: 1 step: 1294, loss is 0.8153089284896851\n",
            "epoch: 1 step: 1295, loss is 0.9422199726104736\n",
            "epoch: 1 step: 1296, loss is 0.7823200821876526\n",
            "epoch: 1 step: 1297, loss is 0.8511399626731873\n",
            "epoch: 1 step: 1298, loss is 1.0585176944732666\n",
            "epoch: 1 step: 1299, loss is 0.8442791700363159\n",
            "epoch: 1 step: 1300, loss is 1.0448211431503296\n",
            "epoch: 1 step: 1301, loss is 1.0471277236938477\n",
            "epoch: 1 step: 1302, loss is 0.8997704982757568\n",
            "epoch: 1 step: 1303, loss is 0.8364281058311462\n",
            "epoch: 1 step: 1304, loss is 0.8227662444114685\n",
            "epoch: 1 step: 1305, loss is 1.0449600219726562\n",
            "epoch: 1 step: 1306, loss is 1.011733055114746\n",
            "epoch: 1 step: 1307, loss is 1.0586278438568115\n",
            "epoch: 1 step: 1308, loss is 0.9908614754676819\n",
            "epoch: 1 step: 1309, loss is 0.9548311829566956\n",
            "epoch: 1 step: 1310, loss is 0.7650710940361023\n",
            "epoch: 1 step: 1311, loss is 0.6825037598609924\n",
            "epoch: 1 step: 1312, loss is 0.825705885887146\n",
            "epoch: 1 step: 1313, loss is 0.7227814197540283\n",
            "epoch: 1 step: 1314, loss is 0.8809895515441895\n",
            "epoch: 1 step: 1315, loss is 1.0151115655899048\n",
            "epoch: 1 step: 1316, loss is 1.0434868335723877\n",
            "epoch: 1 step: 1317, loss is 0.8230072259902954\n",
            "epoch: 1 step: 1318, loss is 1.0345110893249512\n",
            "epoch: 1 step: 1319, loss is 0.9179667830467224\n",
            "epoch: 1 step: 1320, loss is 0.7155013084411621\n",
            "epoch: 1 step: 1321, loss is 1.1128243207931519\n",
            "epoch: 1 step: 1322, loss is 0.8201135993003845\n",
            "epoch: 1 step: 1323, loss is 0.8904014229774475\n",
            "epoch: 1 step: 1324, loss is 0.8065382242202759\n",
            "epoch: 1 step: 1325, loss is 0.9413943290710449\n",
            "epoch: 1 step: 1326, loss is 1.0115418434143066\n",
            "epoch: 1 step: 1327, loss is 1.0616620779037476\n",
            "epoch: 1 step: 1328, loss is 0.7975053191184998\n",
            "epoch: 1 step: 1329, loss is 0.7839239239692688\n",
            "epoch: 1 step: 1330, loss is 0.7611653804779053\n",
            "epoch: 1 step: 1331, loss is 0.9569737911224365\n",
            "epoch: 1 step: 1332, loss is 0.8499599099159241\n",
            "epoch: 1 step: 1333, loss is 0.9541417956352234\n",
            "epoch: 1 step: 1334, loss is 0.8583890795707703\n",
            "epoch: 1 step: 1335, loss is 0.9063029289245605\n",
            "epoch: 1 step: 1336, loss is 0.8883872032165527\n",
            "epoch: 1 step: 1337, loss is 0.8885980248451233\n",
            "epoch: 1 step: 1338, loss is 0.9016962647438049\n",
            "epoch: 1 step: 1339, loss is 0.8457931876182556\n",
            "epoch: 1 step: 1340, loss is 0.9566965103149414\n",
            "epoch: 1 step: 1341, loss is 0.9753477573394775\n",
            "epoch: 1 step: 1342, loss is 0.7974536418914795\n",
            "epoch: 1 step: 1343, loss is 0.8056467771530151\n",
            "epoch: 1 step: 1344, loss is 0.7791551947593689\n",
            "epoch: 1 step: 1345, loss is 0.9740297794342041\n",
            "epoch: 1 step: 1346, loss is 1.236515998840332\n",
            "epoch: 1 step: 1347, loss is 0.931603729724884\n",
            "epoch: 1 step: 1348, loss is 0.9240979552268982\n",
            "epoch: 1 step: 1349, loss is 0.8092325925827026\n",
            "epoch: 1 step: 1350, loss is 0.8959127068519592\n",
            "epoch: 1 step: 1351, loss is 0.9048024415969849\n",
            "epoch: 1 step: 1352, loss is 0.7190490961074829\n",
            "epoch: 1 step: 1353, loss is 0.8967593312263489\n",
            "epoch: 1 step: 1354, loss is 0.8224173188209534\n",
            "epoch: 1 step: 1355, loss is 0.7781155109405518\n",
            "epoch: 1 step: 1356, loss is 0.981696605682373\n",
            "epoch: 1 step: 1357, loss is 0.7633646130561829\n",
            "epoch: 1 step: 1358, loss is 1.0229167938232422\n",
            "epoch: 1 step: 1359, loss is 0.9822238087654114\n",
            "epoch: 1 step: 1360, loss is 1.0910286903381348\n",
            "epoch: 1 step: 1361, loss is 0.8419898748397827\n",
            "epoch: 1 step: 1362, loss is 0.9457210898399353\n",
            "epoch: 1 step: 1363, loss is 0.845468282699585\n",
            "epoch: 1 step: 1364, loss is 0.9243837594985962\n",
            "epoch: 1 step: 1365, loss is 0.8613706231117249\n",
            "epoch: 1 step: 1366, loss is 0.7533527612686157\n",
            "epoch: 1 step: 1367, loss is 0.8715744018554688\n",
            "epoch: 1 step: 1368, loss is 0.9660170078277588\n",
            "epoch: 1 step: 1369, loss is 0.7131071090698242\n",
            "epoch: 1 step: 1370, loss is 0.8820708990097046\n",
            "epoch: 1 step: 1371, loss is 0.7780367136001587\n",
            "epoch: 1 step: 1372, loss is 0.8472078442573547\n",
            "epoch: 1 step: 1373, loss is 0.9754389524459839\n",
            "epoch: 1 step: 1374, loss is 1.076378583908081\n",
            "epoch: 1 step: 1375, loss is 0.8905196189880371\n",
            "epoch: 1 step: 1376, loss is 0.941368043422699\n",
            "epoch: 1 step: 1377, loss is 0.8759551644325256\n",
            "epoch: 1 step: 1378, loss is 0.8948063254356384\n",
            "epoch: 1 step: 1379, loss is 0.84014493227005\n",
            "epoch: 1 step: 1380, loss is 0.8435412049293518\n",
            "epoch: 1 step: 1381, loss is 0.8705581426620483\n",
            "epoch: 1 step: 1382, loss is 0.8314459919929504\n",
            "epoch: 1 step: 1383, loss is 0.8886361122131348\n",
            "epoch: 1 step: 1384, loss is 0.8653450012207031\n",
            "epoch: 1 step: 1385, loss is 1.010614037513733\n",
            "epoch: 1 step: 1386, loss is 0.7587755918502808\n",
            "epoch: 1 step: 1387, loss is 0.9802918434143066\n",
            "epoch: 1 step: 1388, loss is 0.8542588949203491\n",
            "epoch: 1 step: 1389, loss is 0.7814657688140869\n",
            "epoch: 1 step: 1390, loss is 0.8966550230979919\n",
            "epoch: 1 step: 1391, loss is 0.9401229023933411\n",
            "epoch: 1 step: 1392, loss is 0.7811108827590942\n",
            "epoch: 1 step: 1393, loss is 0.8071754574775696\n",
            "epoch: 1 step: 1394, loss is 0.8055316209793091\n",
            "epoch: 1 step: 1395, loss is 1.0994346141815186\n",
            "epoch: 1 step: 1396, loss is 0.8819758892059326\n",
            "epoch: 1 step: 1397, loss is 0.637060821056366\n",
            "epoch: 1 step: 1398, loss is 1.0171749591827393\n",
            "epoch: 1 step: 1399, loss is 0.7485551238059998\n",
            "epoch: 1 step: 1400, loss is 0.8971537351608276\n",
            "epoch: 1 step: 1401, loss is 0.7756617069244385\n",
            "epoch: 1 step: 1402, loss is 0.905154824256897\n",
            "epoch: 1 step: 1403, loss is 0.8253177404403687\n",
            "epoch: 1 step: 1404, loss is 0.7991803884506226\n",
            "epoch: 1 step: 1405, loss is 0.8982646465301514\n",
            "epoch: 1 step: 1406, loss is 0.8592758774757385\n",
            "epoch: 1 step: 1407, loss is 0.8503128290176392\n",
            "epoch: 1 step: 1408, loss is 0.9263707995414734\n",
            "epoch: 1 step: 1409, loss is 0.811710774898529\n",
            "epoch: 1 step: 1410, loss is 0.9679627418518066\n",
            "epoch: 1 step: 1411, loss is 0.976563572883606\n",
            "epoch: 1 step: 1412, loss is 1.1290642023086548\n",
            "epoch: 1 step: 1413, loss is 0.9778360724449158\n",
            "epoch: 1 step: 1414, loss is 0.7628984451293945\n",
            "epoch: 1 step: 1415, loss is 0.7962350845336914\n",
            "epoch: 1 step: 1416, loss is 0.7937138080596924\n",
            "epoch: 1 step: 1417, loss is 0.8771271705627441\n",
            "epoch: 1 step: 1418, loss is 0.9977034330368042\n",
            "epoch: 1 step: 1419, loss is 0.7680081725120544\n",
            "epoch: 1 step: 1420, loss is 1.0206960439682007\n",
            "epoch: 1 step: 1421, loss is 0.8952388167381287\n",
            "epoch: 1 step: 1422, loss is 0.8709105253219604\n",
            "epoch: 1 step: 1423, loss is 0.8446279764175415\n",
            "epoch: 1 step: 1424, loss is 0.9663228988647461\n",
            "epoch: 1 step: 1425, loss is 0.9011747241020203\n",
            "epoch: 1 step: 1426, loss is 1.040095567703247\n",
            "epoch: 1 step: 1427, loss is 0.9895477890968323\n",
            "epoch: 1 step: 1428, loss is 0.8173437118530273\n",
            "epoch: 1 step: 1429, loss is 1.1255038976669312\n",
            "epoch: 1 step: 1430, loss is 0.9323981404304504\n",
            "epoch: 1 step: 1431, loss is 0.902419924736023\n",
            "epoch: 1 step: 1432, loss is 0.8083678483963013\n",
            "epoch: 1 step: 1433, loss is 0.8644164800643921\n",
            "epoch: 1 step: 1434, loss is 1.2066307067871094\n",
            "epoch: 1 step: 1435, loss is 0.847568690776825\n",
            "epoch: 1 step: 1436, loss is 0.9197574853897095\n",
            "epoch: 1 step: 1437, loss is 0.8665059804916382\n",
            "epoch: 1 step: 1438, loss is 0.8974539637565613\n",
            "epoch: 1 step: 1439, loss is 1.0086504220962524\n",
            "epoch: 1 step: 1440, loss is 0.8062134981155396\n",
            "epoch: 1 step: 1441, loss is 1.0585721731185913\n",
            "epoch: 1 step: 1442, loss is 0.9036136269569397\n",
            "epoch: 1 step: 1443, loss is 0.9361363053321838\n",
            "epoch: 1 step: 1444, loss is 1.0165846347808838\n",
            "epoch: 1 step: 1445, loss is 0.8503187894821167\n",
            "epoch: 1 step: 1446, loss is 0.9163532853126526\n",
            "epoch: 1 step: 1447, loss is 0.9635895490646362\n",
            "epoch: 1 step: 1448, loss is 0.7658059000968933\n",
            "epoch: 1 step: 1449, loss is 1.142046570777893\n",
            "epoch: 1 step: 1450, loss is 0.8163615465164185\n",
            "epoch: 1 step: 1451, loss is 0.7821445465087891\n",
            "epoch: 1 step: 1452, loss is 0.9381756782531738\n",
            "epoch: 1 step: 1453, loss is 0.7737196087837219\n",
            "epoch: 1 step: 1454, loss is 0.8035614490509033\n",
            "epoch: 1 step: 1455, loss is 0.8817973136901855\n",
            "epoch: 1 step: 1456, loss is 0.8871548175811768\n",
            "epoch: 1 step: 1457, loss is 0.8210617899894714\n",
            "epoch: 1 step: 1458, loss is 0.9498013257980347\n",
            "epoch: 1 step: 1459, loss is 0.83333420753479\n",
            "epoch: 1 step: 1460, loss is 0.932529866695404\n",
            "epoch: 1 step: 1461, loss is 0.8058577179908752\n",
            "epoch: 1 step: 1462, loss is 0.9391276836395264\n",
            "epoch: 1 step: 1463, loss is 0.9116450548171997\n",
            "epoch: 1 step: 1464, loss is 0.8169479370117188\n",
            "epoch: 1 step: 1465, loss is 0.8238652944564819\n",
            "epoch: 1 step: 1466, loss is 0.8482626080513\n",
            "epoch: 1 step: 1467, loss is 0.9860263466835022\n",
            "epoch: 1 step: 1468, loss is 0.9007050395011902\n",
            "epoch: 1 step: 1469, loss is 0.8977042436599731\n",
            "epoch: 1 step: 1470, loss is 0.9222880005836487\n",
            "epoch: 1 step: 1471, loss is 0.9312212467193604\n",
            "epoch: 1 step: 1472, loss is 0.7273881435394287\n",
            "epoch: 1 step: 1473, loss is 0.8284637331962585\n",
            "epoch: 1 step: 1474, loss is 0.980307400226593\n",
            "epoch: 1 step: 1475, loss is 0.8623085618019104\n",
            "epoch: 1 step: 1476, loss is 0.9643914699554443\n",
            "epoch: 1 step: 1477, loss is 0.9262188673019409\n",
            "epoch: 1 step: 1478, loss is 0.9179469347000122\n",
            "epoch: 1 step: 1479, loss is 0.958585262298584\n",
            "epoch: 1 step: 1480, loss is 0.9035733938217163\n",
            "epoch: 1 step: 1481, loss is 0.8254562020301819\n",
            "epoch: 1 step: 1482, loss is 0.8332787156105042\n",
            "epoch: 1 step: 1483, loss is 0.9221847057342529\n",
            "epoch: 1 step: 1484, loss is 1.0496894121170044\n",
            "epoch: 1 step: 1485, loss is 0.9710343480110168\n",
            "epoch: 1 step: 1486, loss is 0.8841606378555298\n",
            "epoch: 1 step: 1487, loss is 0.9308398962020874\n",
            "epoch: 1 step: 1488, loss is 1.0661886930465698\n",
            "epoch: 1 step: 1489, loss is 0.8129000067710876\n",
            "epoch: 1 step: 1490, loss is 0.7671079039573669\n",
            "epoch: 1 step: 1491, loss is 0.8320909142494202\n",
            "epoch: 1 step: 1492, loss is 0.9246249794960022\n",
            "epoch: 1 step: 1493, loss is 1.0466610193252563\n",
            "epoch: 1 step: 1494, loss is 0.9669235348701477\n",
            "epoch: 1 step: 1495, loss is 0.745729386806488\n",
            "epoch: 1 step: 1496, loss is 0.9840773344039917\n",
            "epoch: 1 step: 1497, loss is 0.9379587769508362\n",
            "epoch: 1 step: 1498, loss is 0.8996143341064453\n",
            "epoch: 1 step: 1499, loss is 0.8702507019042969\n",
            "epoch: 1 step: 1500, loss is 0.765354573726654\n",
            "epoch: 1 step: 1501, loss is 0.9135856628417969\n",
            "epoch: 1 step: 1502, loss is 0.9069772958755493\n",
            "epoch: 1 step: 1503, loss is 0.830952525138855\n",
            "epoch: 1 step: 1504, loss is 0.9374399185180664\n",
            "epoch: 1 step: 1505, loss is 1.0167909860610962\n",
            "epoch: 1 step: 1506, loss is 0.906561553478241\n",
            "epoch: 1 step: 1507, loss is 0.9071905016899109\n",
            "epoch: 1 step: 1508, loss is 1.0282927751541138\n",
            "epoch: 1 step: 1509, loss is 0.8846319317817688\n",
            "epoch: 1 step: 1510, loss is 0.7553814053535461\n",
            "epoch: 1 step: 1511, loss is 0.9388686418533325\n",
            "epoch: 1 step: 1512, loss is 0.8287042379379272\n",
            "epoch: 1 step: 1513, loss is 0.9335418343544006\n",
            "epoch: 1 step: 1514, loss is 1.020208716392517\n",
            "epoch: 1 step: 1515, loss is 0.9092073440551758\n",
            "epoch: 1 step: 1516, loss is 0.85063236951828\n",
            "epoch: 1 step: 1517, loss is 0.943282961845398\n",
            "epoch: 1 step: 1518, loss is 0.9856996536254883\n",
            "epoch: 1 step: 1519, loss is 0.8125950694084167\n",
            "epoch: 1 step: 1520, loss is 0.8213706016540527\n",
            "epoch: 1 step: 1521, loss is 1.067257285118103\n",
            "epoch: 1 step: 1522, loss is 0.8145476579666138\n",
            "epoch: 1 step: 1523, loss is 0.6428107023239136\n",
            "epoch: 1 step: 1524, loss is 0.8522639870643616\n",
            "epoch: 1 step: 1525, loss is 0.9937886595726013\n",
            "epoch: 1 step: 1526, loss is 1.0728346109390259\n",
            "epoch: 1 step: 1527, loss is 0.9169257879257202\n",
            "epoch: 1 step: 1528, loss is 0.814289391040802\n",
            "epoch: 1 step: 1529, loss is 0.975713312625885\n",
            "epoch: 1 step: 1530, loss is 0.6892249584197998\n",
            "epoch: 1 step: 1531, loss is 0.904979944229126\n",
            "epoch: 1 step: 1532, loss is 0.8157804608345032\n",
            "epoch: 1 step: 1533, loss is 0.7792931795120239\n",
            "epoch: 1 step: 1534, loss is 0.8155345916748047\n",
            "epoch: 1 step: 1535, loss is 0.9575968980789185\n",
            "epoch: 1 step: 1536, loss is 0.912285566329956\n",
            "epoch: 1 step: 1537, loss is 1.12962007522583\n",
            "epoch: 1 step: 1538, loss is 0.8176531791687012\n",
            "epoch: 1 step: 1539, loss is 0.948388934135437\n",
            "epoch: 1 step: 1540, loss is 0.9053502678871155\n",
            "epoch: 1 step: 1541, loss is 0.9528406858444214\n",
            "epoch: 1 step: 1542, loss is 0.8309292793273926\n",
            "epoch: 1 step: 1543, loss is 0.8324227333068848\n",
            "epoch: 1 step: 1544, loss is 0.7342764139175415\n",
            "epoch: 1 step: 1545, loss is 0.9440022110939026\n",
            "epoch: 1 step: 1546, loss is 0.977488100528717\n",
            "epoch: 1 step: 1547, loss is 0.9059398174285889\n",
            "epoch: 1 step: 1548, loss is 0.7693320512771606\n",
            "epoch: 1 step: 1549, loss is 0.8542353510856628\n",
            "epoch: 1 step: 1550, loss is 1.0129997730255127\n",
            "epoch: 1 step: 1551, loss is 1.0316747426986694\n",
            "epoch: 1 step: 1552, loss is 0.9502793550491333\n",
            "epoch: 1 step: 1553, loss is 0.998336672782898\n",
            "epoch: 1 step: 1554, loss is 0.9981405138969421\n",
            "epoch: 1 step: 1555, loss is 0.8042201995849609\n",
            "epoch: 1 step: 1556, loss is 0.900380551815033\n",
            "epoch: 1 step: 1557, loss is 0.8010588884353638\n",
            "epoch: 1 step: 1558, loss is 0.8654620051383972\n",
            "epoch: 1 step: 1559, loss is 0.8461670279502869\n",
            "epoch: 1 step: 1560, loss is 0.9105352759361267\n",
            "epoch: 1 step: 1561, loss is 0.7694552540779114\n",
            "epoch: 1 step: 1562, loss is 0.8485837578773499\n",
            "epoch: 1 step: 1563, loss is 0.78763347864151\n",
            "epoch: 1 step: 1564, loss is 1.059761643409729\n",
            "epoch: 1 step: 1565, loss is 1.144386887550354\n",
            "epoch: 1 step: 1566, loss is 0.992952287197113\n",
            "epoch: 1 step: 1567, loss is 0.7788352370262146\n",
            "epoch: 1 step: 1568, loss is 0.9947038888931274\n",
            "epoch: 1 step: 1569, loss is 0.8524293303489685\n",
            "epoch: 1 step: 1570, loss is 1.0290088653564453\n",
            "epoch: 1 step: 1571, loss is 0.9109101295471191\n",
            "epoch: 1 step: 1572, loss is 0.8100878000259399\n",
            "epoch: 1 step: 1573, loss is 1.059119701385498\n",
            "epoch: 1 step: 1574, loss is 0.6929835677146912\n",
            "epoch: 1 step: 1575, loss is 0.9274327754974365\n",
            "epoch: 1 step: 1576, loss is 0.8401392102241516\n",
            "epoch: 1 step: 1577, loss is 1.1067532300949097\n",
            "epoch: 1 step: 1578, loss is 0.956421434879303\n",
            "epoch: 1 step: 1579, loss is 1.0181418657302856\n",
            "epoch: 1 step: 1580, loss is 0.7121513485908508\n",
            "epoch: 1 step: 1581, loss is 0.8709955811500549\n",
            "epoch: 1 step: 1582, loss is 0.9436038136482239\n",
            "epoch: 1 step: 1583, loss is 0.9727083444595337\n",
            "epoch: 1 step: 1584, loss is 1.0555684566497803\n",
            "epoch: 1 step: 1585, loss is 0.9855769276618958\n",
            "epoch: 1 step: 1586, loss is 0.9454535841941833\n",
            "epoch: 1 step: 1587, loss is 0.8132728338241577\n",
            "epoch: 1 step: 1588, loss is 0.8225785493850708\n",
            "epoch: 1 step: 1589, loss is 0.7552618980407715\n",
            "epoch: 1 step: 1590, loss is 0.9513894319534302\n",
            "epoch: 1 step: 1591, loss is 0.7742011547088623\n",
            "epoch: 1 step: 1592, loss is 0.9782562255859375\n",
            "epoch: 1 step: 1593, loss is 1.0562338829040527\n",
            "epoch: 1 step: 1594, loss is 0.8993456959724426\n",
            "epoch: 1 step: 1595, loss is 1.0189884901046753\n",
            "epoch: 1 step: 1596, loss is 0.9974656105041504\n",
            "epoch: 1 step: 1597, loss is 0.7917719483375549\n",
            "epoch: 1 step: 1598, loss is 0.861086905002594\n",
            "epoch: 1 step: 1599, loss is 0.8855956792831421\n",
            "epoch: 1 step: 1600, loss is 0.8810980916023254\n",
            "epoch: 1 step: 1601, loss is 0.8052502870559692\n",
            "epoch: 1 step: 1602, loss is 0.8975683450698853\n",
            "epoch: 1 step: 1603, loss is 0.8186367750167847\n",
            "epoch: 1 step: 1604, loss is 0.9192143082618713\n",
            "epoch: 1 step: 1605, loss is 1.0078524351119995\n",
            "epoch: 1 step: 1606, loss is 1.0764178037643433\n",
            "epoch: 1 step: 1607, loss is 0.8155490756034851\n",
            "epoch: 1 step: 1608, loss is 1.0235209465026855\n",
            "epoch: 1 step: 1609, loss is 0.8751105666160583\n",
            "epoch: 1 step: 1610, loss is 0.8836492300033569\n",
            "epoch: 1 step: 1611, loss is 0.9652366042137146\n",
            "epoch: 1 step: 1612, loss is 0.8752690553665161\n",
            "epoch: 1 step: 1613, loss is 0.959100604057312\n",
            "epoch: 1 step: 1614, loss is 0.7805690765380859\n",
            "epoch: 1 step: 1615, loss is 0.6953715085983276\n",
            "epoch: 1 step: 1616, loss is 0.871563196182251\n",
            "epoch: 1 step: 1617, loss is 0.987628161907196\n",
            "epoch: 1 step: 1618, loss is 1.146852970123291\n",
            "epoch: 1 step: 1619, loss is 0.8933311700820923\n",
            "epoch: 1 step: 1620, loss is 1.064051866531372\n",
            "epoch: 1 step: 1621, loss is 0.8149732947349548\n",
            "epoch: 1 step: 1622, loss is 0.858013391494751\n",
            "epoch: 1 step: 1623, loss is 0.9862350821495056\n",
            "epoch: 1 step: 1624, loss is 0.6673134565353394\n",
            "epoch: 1 step: 1625, loss is 0.9417979717254639\n",
            "epoch: 1 step: 1626, loss is 0.7159501910209656\n",
            "epoch: 1 step: 1627, loss is 0.8381790518760681\n",
            "epoch: 1 step: 1628, loss is 0.9379382133483887\n",
            "epoch: 1 step: 1629, loss is 0.8475379347801208\n",
            "epoch: 1 step: 1630, loss is 0.8677802085876465\n",
            "epoch: 1 step: 1631, loss is 1.011588215827942\n",
            "epoch: 1 step: 1632, loss is 0.8464688658714294\n",
            "epoch: 1 step: 1633, loss is 1.0389715433120728\n",
            "epoch: 1 step: 1634, loss is 0.7872781157493591\n",
            "epoch: 1 step: 1635, loss is 0.8048278093338013\n",
            "epoch: 1 step: 1636, loss is 0.8789891004562378\n",
            "epoch: 1 step: 1637, loss is 0.8702360987663269\n",
            "epoch: 1 step: 1638, loss is 0.7649163603782654\n",
            "epoch: 1 step: 1639, loss is 0.8462678790092468\n",
            "epoch: 1 step: 1640, loss is 0.8595672845840454\n",
            "epoch: 1 step: 1641, loss is 0.9362055659294128\n",
            "epoch: 1 step: 1642, loss is 0.822986364364624\n",
            "epoch: 1 step: 1643, loss is 0.9391117095947266\n",
            "epoch: 1 step: 1644, loss is 0.8506622910499573\n",
            "epoch: 1 step: 1645, loss is 1.0218992233276367\n",
            "epoch: 1 step: 1646, loss is 0.8357163071632385\n",
            "epoch: 1 step: 1647, loss is 0.7660617828369141\n",
            "epoch: 1 step: 1648, loss is 0.7520078420639038\n",
            "epoch: 1 step: 1649, loss is 0.9741982817649841\n",
            "epoch: 1 step: 1650, loss is 0.8505476117134094\n",
            "epoch: 1 step: 1651, loss is 0.8868845701217651\n",
            "epoch: 1 step: 1652, loss is 1.0065264701843262\n",
            "epoch: 1 step: 1653, loss is 0.8699707388877869\n",
            "epoch: 1 step: 1654, loss is 0.7390402555465698\n",
            "epoch: 1 step: 1655, loss is 0.74713134765625\n",
            "epoch: 1 step: 1656, loss is 1.0011717081069946\n",
            "epoch: 1 step: 1657, loss is 0.8864362835884094\n",
            "epoch: 1 step: 1658, loss is 0.8620970845222473\n",
            "epoch: 1 step: 1659, loss is 0.8458704352378845\n",
            "epoch: 1 step: 1660, loss is 0.7974777221679688\n",
            "epoch: 1 step: 1661, loss is 0.8985198140144348\n",
            "epoch: 1 step: 1662, loss is 0.9588959217071533\n",
            "epoch: 1 step: 1663, loss is 0.8076130151748657\n",
            "epoch: 1 step: 1664, loss is 0.8440831899642944\n",
            "epoch: 1 step: 1665, loss is 0.7656639218330383\n",
            "epoch: 1 step: 1666, loss is 0.7657838463783264\n",
            "epoch: 1 step: 1667, loss is 1.2185804843902588\n",
            "epoch: 1 step: 1668, loss is 0.7576937079429626\n",
            "epoch: 1 step: 1669, loss is 0.7941848039627075\n",
            "epoch: 1 step: 1670, loss is 0.8319877982139587\n",
            "epoch: 1 step: 1671, loss is 0.8974365592002869\n",
            "epoch: 1 step: 1672, loss is 0.8767853379249573\n",
            "epoch: 1 step: 1673, loss is 0.7587859630584717\n",
            "epoch: 1 step: 1674, loss is 0.8877069354057312\n",
            "epoch: 1 step: 1675, loss is 0.9164495468139648\n",
            "epoch: 1 step: 1676, loss is 0.898471474647522\n",
            "epoch: 1 step: 1677, loss is 0.9421244263648987\n",
            "epoch: 1 step: 1678, loss is 0.8426228761672974\n",
            "epoch: 1 step: 1679, loss is 1.0707969665527344\n",
            "epoch: 1 step: 1680, loss is 0.8215871453285217\n",
            "epoch: 1 step: 1681, loss is 0.834548830986023\n",
            "epoch: 1 step: 1682, loss is 0.9289014935493469\n",
            "epoch: 1 step: 1683, loss is 0.789454460144043\n",
            "epoch: 1 step: 1684, loss is 0.8940357565879822\n",
            "epoch: 1 step: 1685, loss is 0.8991398811340332\n",
            "epoch: 1 step: 1686, loss is 0.819437563419342\n",
            "epoch: 1 step: 1687, loss is 1.1923162937164307\n",
            "epoch: 1 step: 1688, loss is 0.7686969041824341\n",
            "epoch: 1 step: 1689, loss is 0.7264798879623413\n",
            "epoch: 1 step: 1690, loss is 0.9782938957214355\n",
            "epoch: 1 step: 1691, loss is 0.9628218412399292\n",
            "epoch: 1 step: 1692, loss is 1.1901264190673828\n",
            "epoch: 1 step: 1693, loss is 0.7721831798553467\n",
            "epoch: 1 step: 1694, loss is 0.800746738910675\n",
            "epoch: 1 step: 1695, loss is 0.8733972907066345\n",
            "epoch: 1 step: 1696, loss is 1.062199354171753\n",
            "epoch: 1 step: 1697, loss is 1.0198887586593628\n",
            "epoch: 1 step: 1698, loss is 0.9178296327590942\n",
            "epoch: 1 step: 1699, loss is 0.7756117582321167\n",
            "epoch: 1 step: 1700, loss is 0.8296656608581543\n",
            "epoch: 1 step: 1701, loss is 0.8509430289268494\n",
            "epoch: 1 step: 1702, loss is 1.0777028799057007\n",
            "epoch: 1 step: 1703, loss is 0.9954094290733337\n",
            "epoch: 1 step: 1704, loss is 0.9900354146957397\n",
            "epoch: 1 step: 1705, loss is 0.7375280261039734\n",
            "epoch: 1 step: 1706, loss is 0.8884901404380798\n",
            "epoch: 1 step: 1707, loss is 0.8726892471313477\n",
            "epoch: 1 step: 1708, loss is 0.9585054516792297\n",
            "epoch: 1 step: 1709, loss is 0.9083345532417297\n",
            "epoch: 1 step: 1710, loss is 0.8083104491233826\n",
            "epoch: 1 step: 1711, loss is 0.8530725240707397\n",
            "epoch: 1 step: 1712, loss is 0.9020379781723022\n",
            "epoch: 1 step: 1713, loss is 0.890662431716919\n",
            "epoch: 1 step: 1714, loss is 0.8317455649375916\n",
            "epoch: 1 step: 1715, loss is 1.0318273305892944\n",
            "epoch: 1 step: 1716, loss is 0.8426511883735657\n",
            "epoch: 1 step: 1717, loss is 0.7604131698608398\n",
            "epoch: 1 step: 1718, loss is 0.910379946231842\n",
            "epoch: 1 step: 1719, loss is 1.163088321685791\n",
            "epoch: 1 step: 1720, loss is 0.8678300976753235\n",
            "epoch: 1 step: 1721, loss is 0.9058584570884705\n",
            "epoch: 1 step: 1722, loss is 1.0243836641311646\n",
            "epoch: 1 step: 1723, loss is 0.9664428234100342\n",
            "epoch: 1 step: 1724, loss is 0.7591497302055359\n",
            "epoch: 1 step: 1725, loss is 1.0379799604415894\n",
            "epoch: 1 step: 1726, loss is 0.8105732202529907\n",
            "epoch: 1 step: 1727, loss is 1.0393388271331787\n",
            "epoch: 1 step: 1728, loss is 0.9331071376800537\n",
            "epoch: 1 step: 1729, loss is 0.9336444735527039\n",
            "epoch: 1 step: 1730, loss is 0.8193054795265198\n",
            "epoch: 1 step: 1731, loss is 1.08224356174469\n",
            "epoch: 1 step: 1732, loss is 1.030379056930542\n",
            "epoch: 1 step: 1733, loss is 0.8744218945503235\n",
            "epoch: 1 step: 1734, loss is 0.8799246549606323\n",
            "epoch: 1 step: 1735, loss is 0.8817786574363708\n",
            "epoch: 1 step: 1736, loss is 0.7461605072021484\n",
            "epoch: 1 step: 1737, loss is 1.0238323211669922\n",
            "epoch: 1 step: 1738, loss is 1.017089605331421\n",
            "epoch: 1 step: 1739, loss is 0.876227855682373\n",
            "epoch: 1 step: 1740, loss is 0.858399510383606\n",
            "epoch: 1 step: 1741, loss is 0.8875097036361694\n",
            "epoch: 1 step: 1742, loss is 0.9673035144805908\n",
            "epoch: 1 step: 1743, loss is 0.8089873194694519\n",
            "epoch: 1 step: 1744, loss is 0.8653820753097534\n",
            "epoch: 1 step: 1745, loss is 0.8169308304786682\n",
            "epoch: 1 step: 1746, loss is 1.035163402557373\n",
            "epoch: 1 step: 1747, loss is 0.9371026158332825\n",
            "epoch: 1 step: 1748, loss is 0.9471146464347839\n",
            "epoch: 1 step: 1749, loss is 0.9907695055007935\n",
            "epoch: 1 step: 1750, loss is 0.9483770728111267\n",
            "epoch: 1 step: 1751, loss is 0.9300317168235779\n",
            "epoch: 1 step: 1752, loss is 0.903861939907074\n",
            "epoch: 1 step: 1753, loss is 0.8126267790794373\n",
            "epoch: 1 step: 1754, loss is 0.9265578985214233\n",
            "epoch: 1 step: 1755, loss is 0.8094043135643005\n",
            "epoch: 1 step: 1756, loss is 0.8802604675292969\n",
            "epoch: 1 step: 1757, loss is 0.7500004172325134\n",
            "epoch: 1 step: 1758, loss is 0.9894112348556519\n",
            "epoch: 1 step: 1759, loss is 0.9243122339248657\n",
            "epoch: 1 step: 1760, loss is 1.0577243566513062\n",
            "epoch: 1 step: 1761, loss is 0.8875579833984375\n",
            "epoch: 1 step: 1762, loss is 0.913181483745575\n",
            "epoch: 1 step: 1763, loss is 0.8334664106369019\n",
            "epoch: 1 step: 1764, loss is 0.8704207539558411\n",
            "epoch: 1 step: 1765, loss is 0.8915874361991882\n",
            "epoch: 1 step: 1766, loss is 0.9479689598083496\n",
            "epoch: 1 step: 1767, loss is 0.946620762348175\n",
            "epoch: 1 step: 1768, loss is 0.9400249123573303\n",
            "epoch: 1 step: 1769, loss is 0.859075665473938\n",
            "epoch: 1 step: 1770, loss is 1.0391597747802734\n",
            "epoch: 1 step: 1771, loss is 0.8167458772659302\n",
            "epoch: 1 step: 1772, loss is 0.8450378179550171\n",
            "epoch: 1 step: 1773, loss is 0.8572062253952026\n",
            "epoch: 1 step: 1774, loss is 1.0264829397201538\n",
            "epoch: 1 step: 1775, loss is 0.9915645122528076\n",
            "epoch: 1 step: 1776, loss is 0.7927495241165161\n",
            "epoch: 1 step: 1777, loss is 0.8955923318862915\n",
            "epoch: 1 step: 1778, loss is 0.9774032831192017\n",
            "epoch: 1 step: 1779, loss is 0.6633743643760681\n",
            "epoch: 1 step: 1780, loss is 0.8847130537033081\n",
            "epoch: 1 step: 1781, loss is 0.7774890065193176\n",
            "epoch: 1 step: 1782, loss is 0.9076332449913025\n",
            "epoch: 1 step: 1783, loss is 1.0757050514221191\n",
            "epoch: 1 step: 1784, loss is 0.8167288899421692\n",
            "epoch: 1 step: 1785, loss is 0.7743067145347595\n",
            "epoch: 1 step: 1786, loss is 0.9236636161804199\n",
            "epoch: 1 step: 1787, loss is 0.9119866490364075\n",
            "epoch: 1 step: 1788, loss is 0.9447917938232422\n",
            "epoch: 1 step: 1789, loss is 0.744327962398529\n",
            "epoch: 1 step: 1790, loss is 0.8176560401916504\n",
            "epoch: 1 step: 1791, loss is 0.8589030504226685\n",
            "epoch: 1 step: 1792, loss is 0.6849775314331055\n",
            "epoch: 1 step: 1793, loss is 0.8017159104347229\n",
            "epoch: 1 step: 1794, loss is 0.8109959959983826\n",
            "epoch: 1 step: 1795, loss is 0.8697939515113831\n",
            "epoch: 1 step: 1796, loss is 1.0677999258041382\n",
            "epoch: 1 step: 1797, loss is 0.8415133953094482\n",
            "epoch: 1 step: 1798, loss is 0.987498939037323\n",
            "epoch: 1 step: 1799, loss is 0.9552795886993408\n",
            "epoch: 1 step: 1800, loss is 0.9635301232337952\n",
            "epoch: 1 step: 1801, loss is 1.0643025636672974\n",
            "epoch: 1 step: 1802, loss is 0.8144881725311279\n",
            "epoch: 1 step: 1803, loss is 0.7514978647232056\n",
            "epoch: 1 step: 1804, loss is 0.8456709384918213\n",
            "epoch: 1 step: 1805, loss is 0.8397250175476074\n",
            "epoch: 1 step: 1806, loss is 0.7456410527229309\n",
            "epoch: 1 step: 1807, loss is 0.8806944489479065\n",
            "epoch: 1 step: 1808, loss is 0.8782481551170349\n",
            "epoch: 1 step: 1809, loss is 0.7447940707206726\n",
            "epoch: 1 step: 1810, loss is 0.9434909224510193\n",
            "epoch: 1 step: 1811, loss is 0.8873255252838135\n",
            "epoch: 1 step: 1812, loss is 0.9626964330673218\n",
            "epoch: 1 step: 1813, loss is 0.9432010650634766\n",
            "epoch: 1 step: 1814, loss is 0.9055924415588379\n",
            "epoch: 1 step: 1815, loss is 0.7911191582679749\n",
            "epoch: 1 step: 1816, loss is 0.9035018682479858\n",
            "epoch: 1 step: 1817, loss is 0.8017856478691101\n",
            "epoch: 1 step: 1818, loss is 0.9237257838249207\n",
            "epoch: 1 step: 1819, loss is 0.7894268035888672\n",
            "epoch: 1 step: 1820, loss is 0.8454978466033936\n",
            "epoch: 1 step: 1821, loss is 0.8632972836494446\n",
            "epoch: 1 step: 1822, loss is 1.0296498537063599\n",
            "epoch: 1 step: 1823, loss is 0.9768526554107666\n",
            "epoch: 1 step: 1824, loss is 0.6959453225135803\n",
            "epoch: 1 step: 1825, loss is 1.0216002464294434\n",
            "epoch: 1 step: 1826, loss is 0.8163149356842041\n",
            "epoch: 1 step: 1827, loss is 0.9434555768966675\n",
            "epoch: 1 step: 1828, loss is 0.9341895580291748\n",
            "epoch: 1 step: 1829, loss is 0.7491405010223389\n",
            "epoch: 1 step: 1830, loss is 0.8280941843986511\n",
            "epoch: 1 step: 1831, loss is 0.9071998000144958\n",
            "epoch: 1 step: 1832, loss is 0.9025043845176697\n",
            "epoch: 1 step: 1833, loss is 1.0300225019454956\n",
            "epoch: 1 step: 1834, loss is 0.9197269678115845\n",
            "epoch: 1 step: 1835, loss is 0.8851208090782166\n",
            "epoch: 1 step: 1836, loss is 0.9493371844291687\n",
            "epoch: 1 step: 1837, loss is 0.840927004814148\n",
            "epoch: 1 step: 1838, loss is 0.9002580046653748\n",
            "epoch: 1 step: 1839, loss is 0.8991061449050903\n",
            "epoch: 1 step: 1840, loss is 0.8393856287002563\n",
            "epoch: 1 step: 1841, loss is 0.9999717473983765\n",
            "epoch: 1 step: 1842, loss is 0.7993251085281372\n",
            "epoch: 1 step: 1843, loss is 0.9596372246742249\n",
            "epoch: 1 step: 1844, loss is 1.1958774328231812\n",
            "epoch: 1 step: 1845, loss is 0.8324889540672302\n",
            "epoch: 1 step: 1846, loss is 0.874557375907898\n",
            "epoch: 1 step: 1847, loss is 1.1090128421783447\n",
            "epoch: 1 step: 1848, loss is 0.8650989532470703\n",
            "epoch: 1 step: 1849, loss is 0.8326157927513123\n",
            "epoch: 1 step: 1850, loss is 0.8759781122207642\n",
            "epoch: 1 step: 1851, loss is 0.9348308444023132\n",
            "epoch: 1 step: 1852, loss is 1.0315988063812256\n",
            "epoch: 1 step: 1853, loss is 0.7546424865722656\n",
            "epoch: 1 step: 1854, loss is 0.935583233833313\n",
            "epoch: 1 step: 1855, loss is 0.8860488533973694\n",
            "epoch: 1 step: 1856, loss is 1.0744379758834839\n",
            "epoch: 1 step: 1857, loss is 0.8286166191101074\n",
            "epoch: 1 step: 1858, loss is 0.8700097799301147\n",
            "epoch: 1 step: 1859, loss is 0.9265355467796326\n",
            "epoch: 1 step: 1860, loss is 0.8967928886413574\n",
            "epoch: 1 step: 1861, loss is 0.823013424873352\n",
            "epoch: 1 step: 1862, loss is 0.8789248466491699\n",
            "epoch: 1 step: 1863, loss is 0.9486412405967712\n",
            "epoch: 1 step: 1864, loss is 0.9919756054878235\n",
            "epoch: 1 step: 1865, loss is 0.8623464107513428\n",
            "epoch: 1 step: 1866, loss is 0.9899653196334839\n",
            "epoch: 1 step: 1867, loss is 0.8169808387756348\n",
            "epoch: 1 step: 1868, loss is 0.8197503685951233\n",
            "epoch: 1 step: 1869, loss is 0.8206015825271606\n",
            "epoch: 1 step: 1870, loss is 0.880105197429657\n",
            "epoch: 1 step: 1871, loss is 0.9771561026573181\n",
            "epoch: 1 step: 1872, loss is 0.8952270150184631\n",
            "epoch: 1 step: 1873, loss is 0.7781063318252563\n",
            "epoch: 1 step: 1874, loss is 0.9177558422088623\n",
            "epoch: 1 step: 1875, loss is 0.841575562953949\n",
            "epoch: 1 step: 1876, loss is 0.8893078565597534\n",
            "epoch: 1 step: 1877, loss is 0.8121652007102966\n",
            "epoch: 1 step: 1878, loss is 0.8162970542907715\n",
            "epoch: 1 step: 1879, loss is 0.9263455867767334\n",
            "epoch: 1 step: 1880, loss is 0.9167830944061279\n",
            "epoch: 1 step: 1881, loss is 0.9171528220176697\n",
            "epoch: 1 step: 1882, loss is 0.9917820692062378\n",
            "epoch: 1 step: 1883, loss is 0.8223941326141357\n",
            "epoch: 1 step: 1884, loss is 0.814667284488678\n",
            "epoch: 1 step: 1885, loss is 0.8700846433639526\n",
            "epoch: 1 step: 1886, loss is 0.9073594808578491\n",
            "epoch: 1 step: 1887, loss is 0.8201709389686584\n",
            "epoch: 1 step: 1888, loss is 0.9121654033660889\n",
            "epoch: 1 step: 1889, loss is 0.7908852100372314\n",
            "epoch: 1 step: 1890, loss is 0.774904727935791\n",
            "epoch: 1 step: 1891, loss is 0.7873554825782776\n",
            "epoch: 1 step: 1892, loss is 0.9332363605499268\n",
            "epoch: 1 step: 1893, loss is 0.9773064851760864\n",
            "epoch: 1 step: 1894, loss is 0.957363486289978\n",
            "epoch: 1 step: 1895, loss is 0.8049387335777283\n",
            "epoch: 1 step: 1896, loss is 0.8811116218566895\n",
            "epoch: 1 step: 1897, loss is 0.9082962274551392\n",
            "epoch: 1 step: 1898, loss is 1.065551996231079\n",
            "epoch: 1 step: 1899, loss is 0.8333377242088318\n",
            "epoch: 1 step: 1900, loss is 0.9298418164253235\n",
            "epoch: 1 step: 1901, loss is 0.9275999665260315\n",
            "epoch: 1 step: 1902, loss is 1.0953329801559448\n",
            "epoch: 1 step: 1903, loss is 1.0176888704299927\n",
            "epoch: 1 step: 1904, loss is 0.9006279706954956\n",
            "epoch: 1 step: 1905, loss is 0.8773382306098938\n",
            "epoch: 1 step: 1906, loss is 0.835494875907898\n",
            "epoch: 1 step: 1907, loss is 0.7434056997299194\n",
            "epoch: 1 step: 1908, loss is 0.734885573387146\n",
            "epoch: 1 step: 1909, loss is 0.8215761184692383\n",
            "epoch: 1 step: 1910, loss is 0.9625949263572693\n",
            "epoch: 1 step: 1911, loss is 0.8232091665267944\n",
            "epoch: 1 step: 1912, loss is 0.8782963752746582\n",
            "epoch: 1 step: 1913, loss is 0.7448480129241943\n",
            "epoch: 1 step: 1914, loss is 0.9995178580284119\n",
            "epoch: 1 step: 1915, loss is 0.8975312113761902\n",
            "epoch: 1 step: 1916, loss is 0.860265851020813\n",
            "epoch: 1 step: 1917, loss is 0.8644592761993408\n",
            "epoch: 1 step: 1918, loss is 0.8818507790565491\n",
            "epoch: 1 step: 1919, loss is 0.9773448705673218\n",
            "epoch: 1 step: 1920, loss is 0.8579112887382507\n",
            "epoch: 1 step: 1921, loss is 0.8553504347801208\n",
            "epoch: 1 step: 1922, loss is 0.9408026337623596\n",
            "epoch: 1 step: 1923, loss is 0.8583136796951294\n",
            "epoch: 1 step: 1924, loss is 0.910538911819458\n",
            "epoch: 1 step: 1925, loss is 0.9480398893356323\n",
            "epoch: 1 step: 1926, loss is 0.862405002117157\n",
            "epoch: 1 step: 1927, loss is 0.9053018093109131\n",
            "epoch: 1 step: 1928, loss is 0.6842585802078247\n",
            "epoch: 1 step: 1929, loss is 0.7552164196968079\n",
            "epoch: 1 step: 1930, loss is 0.809069037437439\n",
            "epoch: 1 step: 1931, loss is 0.9171406626701355\n",
            "epoch: 1 step: 1932, loss is 0.8141310214996338\n",
            "epoch: 1 step: 1933, loss is 0.9616843461990356\n",
            "epoch: 1 step: 1934, loss is 0.8976898193359375\n",
            "epoch: 1 step: 1935, loss is 0.7033700346946716\n",
            "epoch: 1 step: 1936, loss is 0.8195930123329163\n",
            "epoch: 1 step: 1937, loss is 0.938563346862793\n",
            "epoch: 1 step: 1938, loss is 0.9038137197494507\n",
            "epoch: 1 step: 1939, loss is 0.9653047919273376\n",
            "epoch: 1 step: 1940, loss is 0.8469656705856323\n",
            "epoch: 1 step: 1941, loss is 0.9370102882385254\n",
            "epoch: 1 step: 1942, loss is 1.0502675771713257\n",
            "epoch: 1 step: 1943, loss is 0.7115642428398132\n",
            "epoch: 1 step: 1944, loss is 0.9490314722061157\n",
            "epoch: 1 step: 1945, loss is 1.1881630420684814\n",
            "epoch: 1 step: 1946, loss is 0.8237747550010681\n",
            "epoch: 1 step: 1947, loss is 0.9932082295417786\n",
            "epoch: 1 step: 1948, loss is 0.7885499596595764\n",
            "epoch: 1 step: 1949, loss is 0.938442587852478\n",
            "epoch: 1 step: 1950, loss is 0.8125430941581726\n",
            "epoch: 1 step: 1951, loss is 0.7748720645904541\n",
            "epoch: 1 step: 1952, loss is 0.8214864134788513\n",
            "epoch: 1 step: 1953, loss is 0.8481113314628601\n",
            "epoch: 1 step: 1954, loss is 1.1113481521606445\n",
            "epoch: 1 step: 1955, loss is 0.8351647257804871\n",
            "epoch: 1 step: 1956, loss is 0.6774236559867859\n",
            "epoch: 1 step: 1957, loss is 0.7845156788825989\n",
            "epoch: 1 step: 1958, loss is 0.8862022757530212\n",
            "epoch: 1 step: 1959, loss is 0.877842366695404\n",
            "epoch: 1 step: 1960, loss is 1.1598026752471924\n",
            "epoch: 1 step: 1961, loss is 0.8153837323188782\n",
            "epoch: 1 step: 1962, loss is 0.8677582144737244\n",
            "epoch: 1 step: 1963, loss is 0.8200809955596924\n",
            "epoch: 1 step: 1964, loss is 0.7849379777908325\n",
            "epoch: 1 step: 1965, loss is 0.793710470199585\n",
            "epoch: 1 step: 1966, loss is 0.8790155053138733\n",
            "epoch: 1 step: 1967, loss is 0.925361692905426\n",
            "epoch: 1 step: 1968, loss is 0.9309292435646057\n",
            "epoch: 1 step: 1969, loss is 1.1470645666122437\n",
            "epoch: 1 step: 1970, loss is 1.1635684967041016\n",
            "epoch: 1 step: 1971, loss is 0.7759629487991333\n",
            "epoch: 1 step: 1972, loss is 0.9298378229141235\n",
            "epoch: 1 step: 1973, loss is 0.9114149212837219\n",
            "epoch: 1 step: 1974, loss is 0.9362922310829163\n",
            "epoch: 1 step: 1975, loss is 1.0616246461868286\n",
            "epoch: 1 step: 1976, loss is 0.9497592449188232\n",
            "epoch: 1 step: 1977, loss is 0.8641309142112732\n",
            "epoch: 1 step: 1978, loss is 0.842117428779602\n",
            "epoch: 1 step: 1979, loss is 0.863524317741394\n",
            "epoch: 1 step: 1980, loss is 1.1839966773986816\n",
            "epoch: 1 step: 1981, loss is 0.9089865684509277\n",
            "epoch: 1 step: 1982, loss is 0.7072968482971191\n",
            "epoch: 1 step: 1983, loss is 0.994594395160675\n",
            "epoch: 1 step: 1984, loss is 0.7780453562736511\n",
            "epoch: 1 step: 1985, loss is 0.773210346698761\n",
            "epoch: 1 step: 1986, loss is 0.7435097098350525\n",
            "epoch: 1 step: 1987, loss is 0.9638049006462097\n",
            "epoch: 1 step: 1988, loss is 0.8451179265975952\n",
            "epoch: 1 step: 1989, loss is 0.9428002834320068\n",
            "epoch: 1 step: 1990, loss is 0.903995156288147\n",
            "epoch: 1 step: 1991, loss is 0.7349064946174622\n",
            "epoch: 1 step: 1992, loss is 0.9277159571647644\n",
            "epoch: 1 step: 1993, loss is 0.704902172088623\n",
            "epoch: 1 step: 1994, loss is 0.7944913506507874\n",
            "epoch: 1 step: 1995, loss is 0.9352884888648987\n",
            "epoch: 1 step: 1996, loss is 0.8635169267654419\n",
            "epoch: 1 step: 1997, loss is 0.9921026825904846\n",
            "epoch: 1 step: 1998, loss is 0.9096337556838989\n",
            "epoch: 1 step: 1999, loss is 0.8830716609954834\n",
            "epoch: 1 step: 2000, loss is 0.8563776016235352\n",
            "epoch: 1 step: 2001, loss is 0.8429107666015625\n",
            "epoch: 1 step: 2002, loss is 0.913834273815155\n",
            "epoch: 1 step: 2003, loss is 0.987117350101471\n",
            "epoch: 1 step: 2004, loss is 0.8907689452171326\n",
            "epoch: 1 step: 2005, loss is 1.0580683946609497\n",
            "epoch: 1 step: 2006, loss is 0.8521573543548584\n",
            "epoch: 1 step: 2007, loss is 0.9321287870407104\n",
            "epoch: 1 step: 2008, loss is 0.9328181743621826\n",
            "epoch: 1 step: 2009, loss is 1.1001828908920288\n",
            "epoch: 1 step: 2010, loss is 0.7905958890914917\n",
            "epoch: 1 step: 2011, loss is 0.8024756908416748\n",
            "epoch: 1 step: 2012, loss is 0.8741752505302429\n",
            "epoch: 1 step: 2013, loss is 0.9740794897079468\n",
            "epoch: 1 step: 2014, loss is 0.9955583214759827\n",
            "epoch: 1 step: 2015, loss is 0.8558895587921143\n",
            "epoch: 1 step: 2016, loss is 0.7977137565612793\n",
            "epoch: 1 step: 2017, loss is 0.8844844102859497\n",
            "epoch: 1 step: 2018, loss is 1.0106029510498047\n",
            "epoch: 1 step: 2019, loss is 0.8370761275291443\n",
            "epoch: 1 step: 2020, loss is 0.7767091989517212\n",
            "epoch: 1 step: 2021, loss is 1.0115946531295776\n",
            "epoch: 1 step: 2022, loss is 0.8733780384063721\n",
            "epoch: 1 step: 2023, loss is 0.7254204154014587\n",
            "epoch: 1 step: 2024, loss is 0.7532906532287598\n",
            "epoch: 1 step: 2025, loss is 0.8489614725112915\n",
            "epoch: 1 step: 2026, loss is 0.9035630226135254\n",
            "epoch: 1 step: 2027, loss is 0.7123984694480896\n",
            "epoch: 1 step: 2028, loss is 1.0942093133926392\n",
            "epoch: 1 step: 2029, loss is 0.8670570850372314\n",
            "epoch: 1 step: 2030, loss is 0.9282795190811157\n",
            "epoch: 1 step: 2031, loss is 1.0554959774017334\n",
            "epoch: 1 step: 2032, loss is 0.8578616976737976\n",
            "epoch: 1 step: 2033, loss is 0.9758975505828857\n",
            "epoch: 1 step: 2034, loss is 0.894969642162323\n",
            "epoch: 1 step: 2035, loss is 1.010802149772644\n",
            "epoch: 1 step: 2036, loss is 0.7479599714279175\n",
            "epoch: 1 step: 2037, loss is 0.8073858618736267\n",
            "epoch: 1 step: 2038, loss is 0.9593696594238281\n",
            "epoch: 1 step: 2039, loss is 0.8629297018051147\n",
            "epoch: 1 step: 2040, loss is 0.7797136902809143\n",
            "epoch: 1 step: 2041, loss is 0.8511937260627747\n",
            "epoch: 1 step: 2042, loss is 0.9372509121894836\n",
            "epoch: 1 step: 2043, loss is 0.8405269384384155\n",
            "epoch: 1 step: 2044, loss is 0.9500298500061035\n",
            "epoch: 1 step: 2045, loss is 0.7550608515739441\n",
            "epoch: 1 step: 2046, loss is 0.9411221742630005\n",
            "epoch: 1 step: 2047, loss is 0.9105589985847473\n",
            "epoch: 1 step: 2048, loss is 0.9900354743003845\n",
            "epoch: 1 step: 2049, loss is 1.0122791528701782\n",
            "epoch: 1 step: 2050, loss is 1.1573609113693237\n",
            "epoch: 1 step: 2051, loss is 0.883133053779602\n",
            "epoch: 1 step: 2052, loss is 0.8430571556091309\n",
            "epoch: 1 step: 2053, loss is 0.8260513544082642\n",
            "epoch: 1 step: 2054, loss is 0.9076963663101196\n",
            "epoch: 1 step: 2055, loss is 0.7810347080230713\n",
            "epoch: 1 step: 2056, loss is 0.8641980886459351\n",
            "epoch: 1 step: 2057, loss is 0.8631051182746887\n",
            "epoch: 1 step: 2058, loss is 0.9248796105384827\n",
            "epoch: 1 step: 2059, loss is 0.8804631233215332\n",
            "epoch: 1 step: 2060, loss is 0.9342449307441711\n",
            "epoch: 1 step: 2061, loss is 0.8553659319877625\n",
            "epoch: 1 step: 2062, loss is 0.7525495290756226\n",
            "epoch: 1 step: 2063, loss is 0.9042395353317261\n",
            "epoch: 1 step: 2064, loss is 0.8444502353668213\n",
            "epoch: 1 step: 2065, loss is 0.9327598214149475\n",
            "epoch: 1 step: 2066, loss is 0.8589738607406616\n",
            "epoch: 1 step: 2067, loss is 0.8158113956451416\n",
            "epoch: 1 step: 2068, loss is 0.6790164113044739\n",
            "epoch: 1 step: 2069, loss is 0.8247269988059998\n",
            "epoch: 1 step: 2070, loss is 0.6870248913764954\n",
            "epoch: 1 step: 2071, loss is 0.84943026304245\n",
            "epoch: 1 step: 2072, loss is 0.9307554960250854\n",
            "epoch: 1 step: 2073, loss is 0.8916807770729065\n",
            "epoch: 1 step: 2074, loss is 0.8653705716133118\n",
            "epoch: 1 step: 2075, loss is 0.8245389461517334\n",
            "epoch: 1 step: 2076, loss is 0.7297233939170837\n",
            "epoch: 1 step: 2077, loss is 0.958551287651062\n",
            "epoch: 1 step: 2078, loss is 0.917909562587738\n",
            "epoch: 1 step: 2079, loss is 0.9519286155700684\n",
            "epoch: 1 step: 2080, loss is 0.8453305959701538\n",
            "epoch: 1 step: 2081, loss is 1.1956779956817627\n",
            "epoch: 1 step: 2082, loss is 0.8590139746665955\n",
            "epoch: 1 step: 2083, loss is 1.066880464553833\n",
            "epoch: 1 step: 2084, loss is 0.8521553874015808\n",
            "epoch: 1 step: 2085, loss is 0.9691910743713379\n",
            "epoch: 1 step: 2086, loss is 0.8869646787643433\n",
            "epoch: 1 step: 2087, loss is 0.9952409267425537\n",
            "epoch: 1 step: 2088, loss is 0.9925278425216675\n",
            "epoch: 1 step: 2089, loss is 0.8013606071472168\n",
            "epoch: 1 step: 2090, loss is 0.8706051707267761\n",
            "epoch: 1 step: 2091, loss is 0.6734232306480408\n",
            "epoch: 1 step: 2092, loss is 0.9893966913223267\n",
            "epoch: 1 step: 2093, loss is 0.800336480140686\n",
            "epoch: 1 step: 2094, loss is 0.9219430088996887\n",
            "epoch: 1 step: 2095, loss is 0.8921069502830505\n",
            "epoch: 1 step: 2096, loss is 1.0040191411972046\n",
            "epoch: 1 step: 2097, loss is 0.9373523592948914\n",
            "epoch: 1 step: 2098, loss is 1.1024360656738281\n",
            "epoch: 1 step: 2099, loss is 0.9792787432670593\n",
            "epoch: 1 step: 2100, loss is 0.7557277679443359\n",
            "epoch: 1 step: 2101, loss is 0.8924602270126343\n",
            "epoch: 1 step: 2102, loss is 0.7362589836120605\n",
            "epoch: 1 step: 2103, loss is 0.9666405916213989\n",
            "epoch: 1 step: 2104, loss is 0.8655768632888794\n",
            "epoch: 1 step: 2105, loss is 0.7247005701065063\n",
            "epoch: 1 step: 2106, loss is 1.1145728826522827\n",
            "epoch: 1 step: 2107, loss is 0.9037521481513977\n",
            "epoch: 1 step: 2108, loss is 0.6697220802307129\n",
            "epoch: 1 step: 2109, loss is 0.6785683631896973\n",
            "epoch: 1 step: 2110, loss is 0.8034338355064392\n",
            "epoch: 1 step: 2111, loss is 0.9093397259712219\n",
            "epoch: 1 step: 2112, loss is 0.9413074851036072\n",
            "epoch: 1 step: 2113, loss is 0.6986899375915527\n",
            "epoch: 1 step: 2114, loss is 0.8992122411727905\n",
            "epoch: 1 step: 2115, loss is 0.8880447745323181\n",
            "epoch: 1 step: 2116, loss is 0.9761566519737244\n",
            "epoch: 1 step: 2117, loss is 0.7636581063270569\n",
            "epoch: 1 step: 2118, loss is 0.8182966709136963\n",
            "epoch: 1 step: 2119, loss is 0.8007551431655884\n",
            "epoch: 1 step: 2120, loss is 0.9290013313293457\n",
            "epoch: 1 step: 2121, loss is 0.8714491128921509\n",
            "epoch: 1 step: 2122, loss is 0.9634679555892944\n",
            "epoch: 1 step: 2123, loss is 0.954780101776123\n",
            "epoch: 1 step: 2124, loss is 0.9065781831741333\n",
            "epoch: 1 step: 2125, loss is 0.8189629912376404\n",
            "epoch: 1 step: 2126, loss is 1.0156500339508057\n",
            "epoch: 1 step: 2127, loss is 0.8753806352615356\n",
            "epoch: 1 step: 2128, loss is 1.0261814594268799\n",
            "epoch: 1 step: 2129, loss is 0.8471353054046631\n",
            "epoch: 1 step: 2130, loss is 0.8942943811416626\n",
            "epoch: 1 step: 2131, loss is 0.9242672324180603\n",
            "epoch: 1 step: 2132, loss is 0.963649570941925\n",
            "epoch: 1 step: 2133, loss is 0.7584665417671204\n",
            "epoch: 1 step: 2134, loss is 0.8686067461967468\n",
            "epoch: 1 step: 2135, loss is 0.687161922454834\n",
            "epoch: 1 step: 2136, loss is 0.8772796988487244\n",
            "epoch: 1 step: 2137, loss is 0.8674682974815369\n",
            "epoch: 1 step: 2138, loss is 0.8312364220619202\n",
            "epoch: 1 step: 2139, loss is 0.7442740797996521\n",
            "epoch: 1 step: 2140, loss is 0.9832697510719299\n",
            "epoch: 1 step: 2141, loss is 0.843817949295044\n",
            "epoch: 1 step: 2142, loss is 0.8889670968055725\n",
            "epoch: 1 step: 2143, loss is 0.8764268755912781\n",
            "epoch: 1 step: 2144, loss is 1.051587462425232\n",
            "epoch: 1 step: 2145, loss is 0.8280290365219116\n",
            "epoch: 1 step: 2146, loss is 0.8311839699745178\n",
            "epoch: 1 step: 2147, loss is 0.8253171443939209\n",
            "epoch: 1 step: 2148, loss is 0.9535958170890808\n",
            "epoch: 1 step: 2149, loss is 1.0035655498504639\n",
            "epoch: 1 step: 2150, loss is 1.136987328529358\n",
            "epoch: 1 step: 2151, loss is 0.9307327270507812\n",
            "epoch: 1 step: 2152, loss is 0.8861353993415833\n",
            "epoch: 1 step: 2153, loss is 1.0721923112869263\n",
            "epoch: 1 step: 2154, loss is 0.8940861821174622\n",
            "epoch: 1 step: 2155, loss is 0.9214606881141663\n",
            "epoch: 1 step: 2156, loss is 0.8149979710578918\n",
            "epoch: 1 step: 2157, loss is 0.9805831909179688\n",
            "epoch: 1 step: 2158, loss is 0.7902377843856812\n",
            "epoch: 1 step: 2159, loss is 1.003727674484253\n",
            "epoch: 1 step: 2160, loss is 0.9756723642349243\n",
            "epoch: 1 step: 2161, loss is 0.8945818543434143\n",
            "epoch: 1 step: 2162, loss is 0.9971076846122742\n",
            "epoch: 1 step: 2163, loss is 0.9533507227897644\n",
            "epoch: 1 step: 2164, loss is 1.0699937343597412\n",
            "epoch: 1 step: 2165, loss is 0.9954192638397217\n",
            "epoch: 1 step: 2166, loss is 0.7607768774032593\n",
            "epoch: 1 step: 2167, loss is 0.8384207487106323\n",
            "epoch: 1 step: 2168, loss is 0.86043381690979\n",
            "epoch: 1 step: 2169, loss is 0.7828752398490906\n",
            "epoch: 1 step: 2170, loss is 0.8864406943321228\n",
            "epoch: 1 step: 2171, loss is 1.0272468328475952\n",
            "epoch: 1 step: 2172, loss is 0.808009684085846\n",
            "epoch: 1 step: 2173, loss is 0.8033637404441833\n",
            "epoch: 1 step: 2174, loss is 0.7542946934700012\n",
            "epoch: 1 step: 2175, loss is 0.8798957467079163\n",
            "epoch: 1 step: 2176, loss is 0.8283157348632812\n",
            "epoch: 1 step: 2177, loss is 0.7446019649505615\n",
            "epoch: 1 step: 2178, loss is 0.7706497311592102\n",
            "epoch: 1 step: 2179, loss is 0.8756642937660217\n",
            "epoch: 1 step: 2180, loss is 0.8549833297729492\n",
            "epoch: 1 step: 2181, loss is 0.8705318570137024\n",
            "epoch: 1 step: 2182, loss is 0.9368194341659546\n",
            "epoch: 1 step: 2183, loss is 0.854474663734436\n",
            "epoch: 1 step: 2184, loss is 0.6982256174087524\n",
            "epoch: 1 step: 2185, loss is 1.001451015472412\n",
            "epoch: 1 step: 2186, loss is 0.8732194900512695\n",
            "epoch: 1 step: 2187, loss is 0.8680610060691833\n",
            "epoch: 1 step: 2188, loss is 0.8630139827728271\n",
            "epoch: 1 step: 2189, loss is 0.8923355340957642\n",
            "epoch: 1 step: 2190, loss is 0.9250622391700745\n",
            "epoch: 1 step: 2191, loss is 0.9337738752365112\n",
            "epoch: 1 step: 2192, loss is 0.9977124333381653\n",
            "epoch: 1 step: 2193, loss is 0.9325169324874878\n",
            "epoch: 1 step: 2194, loss is 0.8591234087944031\n",
            "epoch: 1 step: 2195, loss is 0.8512429594993591\n",
            "epoch: 1 step: 2196, loss is 1.0651848316192627\n",
            "epoch: 1 step: 2197, loss is 0.8743985891342163\n",
            "epoch: 1 step: 2198, loss is 0.9435015916824341\n",
            "epoch: 1 step: 2199, loss is 0.9055686593055725\n",
            "epoch: 1 step: 2200, loss is 0.8654046058654785\n",
            "epoch: 1 step: 2201, loss is 0.7584933638572693\n",
            "epoch: 1 step: 2202, loss is 0.9420098066329956\n",
            "epoch: 1 step: 2203, loss is 0.7675666213035583\n",
            "epoch: 1 step: 2204, loss is 1.0733546018600464\n",
            "epoch: 1 step: 2205, loss is 0.9194119572639465\n",
            "epoch: 1 step: 2206, loss is 0.8608479499816895\n",
            "epoch: 1 step: 2207, loss is 0.8073174357414246\n",
            "epoch: 1 step: 2208, loss is 0.8091968297958374\n",
            "epoch: 1 step: 2209, loss is 0.7963310480117798\n",
            "epoch: 1 step: 2210, loss is 0.8283636569976807\n",
            "epoch: 1 step: 2211, loss is 1.026845932006836\n",
            "epoch: 1 step: 2212, loss is 0.7748631238937378\n",
            "epoch: 1 step: 2213, loss is 1.012966275215149\n",
            "epoch: 1 step: 2214, loss is 0.7784854769706726\n",
            "epoch: 1 step: 2215, loss is 0.8437902927398682\n",
            "epoch: 1 step: 2216, loss is 0.8478071689605713\n",
            "epoch: 1 step: 2217, loss is 0.9392149448394775\n",
            "epoch: 1 step: 2218, loss is 0.8420417904853821\n",
            "epoch: 1 step: 2219, loss is 0.9482957124710083\n",
            "epoch: 1 step: 2220, loss is 0.8150034546852112\n",
            "epoch: 1 step: 2221, loss is 0.9122920632362366\n",
            "epoch: 1 step: 2222, loss is 0.9483715891838074\n",
            "epoch: 1 step: 2223, loss is 0.793373703956604\n",
            "epoch: 1 step: 2224, loss is 0.9135912656784058\n",
            "epoch: 1 step: 2225, loss is 0.9859387278556824\n",
            "epoch: 1 step: 2226, loss is 0.8496962785720825\n",
            "epoch: 1 step: 2227, loss is 1.1327632665634155\n",
            "epoch: 1 step: 2228, loss is 0.9789739847183228\n",
            "epoch: 1 step: 2229, loss is 0.7547183632850647\n",
            "epoch: 1 step: 2230, loss is 0.9050989151000977\n",
            "epoch: 1 step: 2231, loss is 0.9145210385322571\n",
            "epoch: 1 step: 2232, loss is 0.9875786304473877\n",
            "epoch: 1 step: 2233, loss is 1.1627209186553955\n",
            "epoch: 1 step: 2234, loss is 0.7568361163139343\n",
            "epoch: 1 step: 2235, loss is 0.8305508494377136\n",
            "epoch: 1 step: 2236, loss is 0.7899379730224609\n",
            "epoch: 1 step: 2237, loss is 0.819962739944458\n",
            "epoch: 1 step: 2238, loss is 0.7591665983200073\n",
            "epoch: 1 step: 2239, loss is 0.7742464542388916\n",
            "epoch: 1 step: 2240, loss is 0.8945354223251343\n",
            "epoch: 1 step: 2241, loss is 0.7592795491218567\n",
            "epoch: 1 step: 2242, loss is 0.8411608338356018\n",
            "epoch: 1 step: 2243, loss is 0.847949743270874\n",
            "epoch: 1 step: 2244, loss is 0.7972181439399719\n",
            "epoch: 1 step: 2245, loss is 0.8015124201774597\n",
            "epoch: 1 step: 2246, loss is 0.9392425417900085\n",
            "epoch: 1 step: 2247, loss is 1.0603083372116089\n",
            "epoch: 1 step: 2248, loss is 0.948306679725647\n",
            "epoch: 1 step: 2249, loss is 0.8234570622444153\n",
            "epoch: 1 step: 2250, loss is 0.8643416166305542\n",
            "epoch: 1 step: 2251, loss is 0.925818681716919\n",
            "epoch: 1 step: 2252, loss is 0.9530947208404541\n",
            "epoch: 1 step: 2253, loss is 0.7972174882888794\n",
            "epoch: 1 step: 2254, loss is 1.0853084325790405\n",
            "epoch: 1 step: 2255, loss is 0.929449737071991\n",
            "epoch: 1 step: 2256, loss is 0.9547300338745117\n",
            "epoch: 1 step: 2257, loss is 0.6057013869285583\n",
            "epoch: 1 step: 2258, loss is 0.8029192090034485\n",
            "epoch: 1 step: 2259, loss is 0.9771437048912048\n",
            "epoch: 1 step: 2260, loss is 1.0411211252212524\n",
            "epoch: 1 step: 2261, loss is 1.0751506090164185\n",
            "epoch: 1 step: 2262, loss is 0.8401105999946594\n",
            "epoch: 1 step: 2263, loss is 0.829335629940033\n",
            "epoch: 1 step: 2264, loss is 1.0718904733657837\n",
            "epoch: 1 step: 2265, loss is 1.1028848886489868\n",
            "epoch: 1 step: 2266, loss is 0.9095144867897034\n",
            "epoch: 1 step: 2267, loss is 0.7311232686042786\n",
            "epoch: 1 step: 2268, loss is 1.0242252349853516\n",
            "epoch: 1 step: 2269, loss is 0.9827547669410706\n",
            "epoch: 1 step: 2270, loss is 0.8747923970222473\n",
            "epoch: 1 step: 2271, loss is 0.870284914970398\n",
            "epoch: 1 step: 2272, loss is 0.9033727049827576\n",
            "epoch: 1 step: 2273, loss is 0.7150920629501343\n",
            "epoch: 1 step: 2274, loss is 1.0566895008087158\n",
            "epoch: 1 step: 2275, loss is 0.964951753616333\n",
            "epoch: 1 step: 2276, loss is 0.7833558917045593\n",
            "epoch: 1 step: 2277, loss is 0.9840404987335205\n",
            "epoch: 1 step: 2278, loss is 1.0231941938400269\n",
            "epoch: 1 step: 2279, loss is 0.8740379810333252\n",
            "epoch: 1 step: 2280, loss is 0.7608488202095032\n",
            "epoch: 1 step: 2281, loss is 0.8448302745819092\n",
            "epoch: 1 step: 2282, loss is 0.9926085472106934\n",
            "epoch: 1 step: 2283, loss is 0.8693716526031494\n",
            "epoch: 1 step: 2284, loss is 0.8861933350563049\n",
            "epoch: 1 step: 2285, loss is 0.9100439548492432\n",
            "epoch: 1 step: 2286, loss is 0.955721914768219\n",
            "epoch: 1 step: 2287, loss is 0.8430584669113159\n",
            "epoch: 1 step: 2288, loss is 0.8286526203155518\n",
            "epoch: 1 step: 2289, loss is 0.9540735483169556\n",
            "epoch: 1 step: 2290, loss is 0.8352904915809631\n",
            "epoch: 1 step: 2291, loss is 0.8529409766197205\n",
            "epoch: 1 step: 2292, loss is 0.7875764966011047\n",
            "epoch: 1 step: 2293, loss is 0.9937708973884583\n",
            "epoch: 1 step: 2294, loss is 0.8513966202735901\n",
            "epoch: 1 step: 2295, loss is 0.9521704316139221\n",
            "epoch: 1 step: 2296, loss is 0.9633375406265259\n",
            "epoch: 1 step: 2297, loss is 0.8260921239852905\n",
            "epoch: 1 step: 2298, loss is 0.8080270290374756\n",
            "epoch: 1 step: 2299, loss is 0.8368188142776489\n",
            "epoch: 1 step: 2300, loss is 0.9284529089927673\n",
            "epoch: 1 step: 2301, loss is 0.8590919375419617\n",
            "epoch: 1 step: 2302, loss is 0.841398298740387\n",
            "epoch: 1 step: 2303, loss is 0.9831185340881348\n",
            "epoch: 1 step: 2304, loss is 0.8393001556396484\n",
            "epoch: 1 step: 2305, loss is 0.7203131318092346\n",
            "epoch: 1 step: 2306, loss is 0.9304517507553101\n",
            "epoch: 1 step: 2307, loss is 0.9657116532325745\n",
            "epoch: 1 step: 2308, loss is 0.9112816452980042\n",
            "epoch: 1 step: 2309, loss is 0.8622720837593079\n",
            "epoch: 1 step: 2310, loss is 0.8686915040016174\n",
            "epoch: 1 step: 2311, loss is 1.0356760025024414\n",
            "epoch: 1 step: 2312, loss is 0.900819718837738\n",
            "epoch: 1 step: 2313, loss is 0.8699793815612793\n",
            "epoch: 1 step: 2314, loss is 0.8470817804336548\n",
            "epoch: 1 step: 2315, loss is 0.9709057807922363\n",
            "epoch: 1 step: 2316, loss is 0.9026209712028503\n",
            "epoch: 1 step: 2317, loss is 0.9295051097869873\n",
            "epoch: 1 step: 2318, loss is 0.9769513010978699\n",
            "epoch: 1 step: 2319, loss is 0.9537049531936646\n",
            "epoch: 1 step: 2320, loss is 1.20413339138031\n",
            "epoch: 1 step: 2321, loss is 0.8875130414962769\n",
            "epoch: 1 step: 2322, loss is 0.9668002128601074\n",
            "epoch: 1 step: 2323, loss is 1.1364595890045166\n",
            "epoch: 1 step: 2324, loss is 0.7711721062660217\n",
            "epoch: 1 step: 2325, loss is 0.8665142059326172\n",
            "epoch: 1 step: 2326, loss is 0.8384234309196472\n",
            "epoch: 1 step: 2327, loss is 0.9170384407043457\n",
            "epoch: 1 step: 2328, loss is 0.9513932466506958\n",
            "epoch: 1 step: 2329, loss is 0.7951553463935852\n",
            "epoch: 1 step: 2330, loss is 0.9310921430587769\n",
            "epoch: 1 step: 2331, loss is 0.9099851250648499\n",
            "epoch: 1 step: 2332, loss is 0.8417891263961792\n",
            "epoch: 1 step: 2333, loss is 0.9182996153831482\n",
            "epoch: 1 step: 2334, loss is 0.9523136019706726\n",
            "epoch: 1 step: 2335, loss is 0.8457047939300537\n",
            "epoch: 1 step: 2336, loss is 0.8477040529251099\n",
            "epoch: 1 step: 2337, loss is 0.6613375544548035\n",
            "epoch: 1 step: 2338, loss is 0.9307611584663391\n",
            "epoch: 1 step: 2339, loss is 0.7509037256240845\n",
            "epoch: 1 step: 2340, loss is 0.8669152855873108\n",
            "epoch: 1 step: 2341, loss is 0.8349576592445374\n",
            "epoch: 1 step: 2342, loss is 0.8550423383712769\n",
            "epoch: 1 step: 2343, loss is 0.9674662351608276\n",
            "epoch: 1 step: 2344, loss is 1.0454813241958618\n",
            "epoch: 1 step: 2345, loss is 0.9465250968933105\n",
            "epoch: 1 step: 2346, loss is 0.8742635250091553\n",
            "epoch: 1 step: 2347, loss is 1.008744239807129\n",
            "epoch: 1 step: 2348, loss is 0.8989143371582031\n",
            "epoch: 1 step: 2349, loss is 0.8927212357521057\n",
            "epoch: 1 step: 2350, loss is 0.887881338596344\n",
            "epoch: 1 step: 2351, loss is 0.8134387135505676\n",
            "epoch: 1 step: 2352, loss is 0.8983451128005981\n",
            "epoch: 1 step: 2353, loss is 0.7486552000045776\n",
            "epoch: 1 step: 2354, loss is 1.0802788734436035\n",
            "epoch: 1 step: 2355, loss is 0.905411422252655\n",
            "epoch: 1 step: 2356, loss is 0.8089839220046997\n",
            "epoch: 1 step: 2357, loss is 0.8619861006736755\n",
            "epoch: 1 step: 2358, loss is 0.8207984566688538\n",
            "epoch: 1 step: 2359, loss is 0.7950236201286316\n",
            "epoch: 1 step: 2360, loss is 0.9396430253982544\n",
            "epoch: 1 step: 2361, loss is 0.867796778678894\n",
            "epoch: 1 step: 2362, loss is 1.031198263168335\n",
            "epoch: 1 step: 2363, loss is 0.9611285924911499\n",
            "epoch: 1 step: 2364, loss is 0.7344841957092285\n",
            "epoch: 1 step: 2365, loss is 0.9035120606422424\n",
            "epoch: 1 step: 2366, loss is 0.7215346693992615\n",
            "epoch: 1 step: 2367, loss is 1.0363754034042358\n",
            "epoch: 1 step: 2368, loss is 0.8150177597999573\n",
            "epoch: 1 step: 2369, loss is 0.9456760287284851\n",
            "epoch: 1 step: 2370, loss is 0.8591147661209106\n",
            "epoch: 1 step: 2371, loss is 1.0097626447677612\n",
            "epoch: 1 step: 2372, loss is 0.9796697497367859\n",
            "epoch: 1 step: 2373, loss is 0.8142101764678955\n",
            "epoch: 1 step: 2374, loss is 0.8519566655158997\n",
            "epoch: 1 step: 2375, loss is 0.802904486656189\n",
            "epoch: 1 step: 2376, loss is 0.9754316806793213\n",
            "epoch: 1 step: 2377, loss is 0.8914212584495544\n",
            "epoch: 1 step: 2378, loss is 0.9688138961791992\n",
            "epoch: 1 step: 2379, loss is 0.8589147329330444\n",
            "epoch: 1 step: 2380, loss is 0.9385766983032227\n",
            "epoch: 1 step: 2381, loss is 0.7576690912246704\n",
            "epoch: 1 step: 2382, loss is 0.8881464600563049\n",
            "epoch: 1 step: 2383, loss is 0.8656973838806152\n",
            "epoch: 1 step: 2384, loss is 1.0014970302581787\n",
            "epoch: 1 step: 2385, loss is 0.8608323335647583\n",
            "epoch: 1 step: 2386, loss is 0.8153557181358337\n",
            "epoch: 1 step: 2387, loss is 0.8041197657585144\n",
            "epoch: 1 step: 2388, loss is 0.9046423435211182\n",
            "epoch: 1 step: 2389, loss is 0.8743656277656555\n",
            "epoch: 1 step: 2390, loss is 0.9627530574798584\n",
            "epoch: 1 step: 2391, loss is 0.8178050518035889\n",
            "epoch: 1 step: 2392, loss is 0.8792684674263\n",
            "epoch: 1 step: 2393, loss is 0.7856603264808655\n",
            "epoch: 1 step: 2394, loss is 0.9562466740608215\n",
            "epoch: 1 step: 2395, loss is 0.7331836223602295\n",
            "epoch: 1 step: 2396, loss is 0.8753065466880798\n",
            "epoch: 1 step: 2397, loss is 0.7990608215332031\n",
            "epoch: 1 step: 2398, loss is 0.8529478311538696\n",
            "epoch: 1 step: 2399, loss is 0.9646557569503784\n",
            "epoch: 1 step: 2400, loss is 0.8012779951095581\n",
            "epoch: 1 step: 2401, loss is 0.8530262112617493\n",
            "epoch: 1 step: 2402, loss is 1.0576967000961304\n",
            "epoch: 1 step: 2403, loss is 0.7573573589324951\n",
            "epoch: 1 step: 2404, loss is 1.1371691226959229\n",
            "epoch: 1 step: 2405, loss is 0.6027880311012268\n",
            "epoch: 1 step: 2406, loss is 0.9656121730804443\n",
            "epoch: 1 step: 2407, loss is 1.0803691148757935\n",
            "epoch: 1 step: 2408, loss is 0.865674614906311\n",
            "epoch: 1 step: 2409, loss is 0.7641772627830505\n",
            "epoch: 1 step: 2410, loss is 0.8123188614845276\n",
            "epoch: 1 step: 2411, loss is 0.965786337852478\n",
            "epoch: 1 step: 2412, loss is 0.8087457418441772\n",
            "epoch: 1 step: 2413, loss is 1.120246410369873\n",
            "epoch: 1 step: 2414, loss is 0.76679527759552\n",
            "epoch: 1 step: 2415, loss is 0.8650711178779602\n",
            "epoch: 1 step: 2416, loss is 1.0622143745422363\n",
            "epoch: 1 step: 2417, loss is 0.7575556039810181\n",
            "epoch: 1 step: 2418, loss is 0.8648231029510498\n",
            "epoch: 1 step: 2419, loss is 0.7800690531730652\n",
            "epoch: 1 step: 2420, loss is 1.0715376138687134\n",
            "epoch: 1 step: 2421, loss is 1.134498119354248\n",
            "epoch: 1 step: 2422, loss is 1.027984857559204\n",
            "epoch: 1 step: 2423, loss is 0.7281333804130554\n",
            "epoch: 1 step: 2424, loss is 0.9457612037658691\n",
            "epoch: 1 step: 2425, loss is 0.9948630928993225\n",
            "epoch: 1 step: 2426, loss is 0.7558972835540771\n",
            "epoch: 1 step: 2427, loss is 0.851740837097168\n",
            "epoch: 1 step: 2428, loss is 0.7675667405128479\n",
            "epoch: 1 step: 2429, loss is 0.8035421967506409\n",
            "epoch: 1 step: 2430, loss is 0.7252810001373291\n",
            "epoch: 1 step: 2431, loss is 1.096840262413025\n",
            "epoch: 1 step: 2432, loss is 0.9774350523948669\n",
            "epoch: 1 step: 2433, loss is 0.8695365786552429\n",
            "epoch: 1 step: 2434, loss is 0.6855766177177429\n",
            "epoch: 1 step: 2435, loss is 0.9184431433677673\n",
            "epoch: 1 step: 2436, loss is 1.0091954469680786\n",
            "epoch: 1 step: 2437, loss is 0.9915392994880676\n",
            "epoch: 1 step: 2438, loss is 0.9472442269325256\n",
            "epoch: 1 step: 2439, loss is 0.967371940612793\n",
            "epoch: 1 step: 2440, loss is 0.8986488580703735\n",
            "epoch: 1 step: 2441, loss is 0.7658423185348511\n",
            "epoch: 1 step: 2442, loss is 0.9535075426101685\n",
            "epoch: 1 step: 2443, loss is 0.8635144829750061\n",
            "epoch: 1 step: 2444, loss is 0.8368982672691345\n",
            "epoch: 1 step: 2445, loss is 0.7000002861022949\n",
            "epoch: 1 step: 2446, loss is 0.9905970692634583\n",
            "epoch: 1 step: 2447, loss is 0.7932312488555908\n",
            "epoch: 1 step: 2448, loss is 0.8920660614967346\n",
            "epoch: 1 step: 2449, loss is 1.1623002290725708\n",
            "epoch: 1 step: 2450, loss is 0.7852802276611328\n",
            "epoch: 1 step: 2451, loss is 0.6823524236679077\n",
            "epoch: 1 step: 2452, loss is 0.7592711448669434\n",
            "epoch: 1 step: 2453, loss is 0.7014581561088562\n",
            "epoch: 1 step: 2454, loss is 0.8131638765335083\n",
            "epoch: 1 step: 2455, loss is 0.7453588247299194\n",
            "epoch: 1 step: 2456, loss is 0.7682953476905823\n",
            "epoch: 1 step: 2457, loss is 0.7803986668586731\n",
            "epoch: 1 step: 2458, loss is 1.051475167274475\n",
            "epoch: 1 step: 2459, loss is 0.88816237449646\n",
            "epoch: 1 step: 2460, loss is 0.7102816700935364\n",
            "epoch: 1 step: 2461, loss is 0.7964070439338684\n",
            "epoch: 1 step: 2462, loss is 1.046397089958191\n",
            "epoch: 1 step: 2463, loss is 0.8317247629165649\n",
            "epoch: 1 step: 2464, loss is 0.8705215454101562\n",
            "epoch: 1 step: 2465, loss is 0.8450390100479126\n",
            "epoch: 1 step: 2466, loss is 0.92645663022995\n",
            "epoch: 1 step: 2467, loss is 0.768917977809906\n",
            "epoch: 1 step: 2468, loss is 0.9699314832687378\n",
            "epoch: 1 step: 2469, loss is 0.9415473937988281\n",
            "epoch: 1 step: 2470, loss is 0.9129477739334106\n",
            "epoch: 1 step: 2471, loss is 0.9427186250686646\n",
            "epoch: 1 step: 2472, loss is 0.787055492401123\n",
            "epoch: 1 step: 2473, loss is 0.8863842487335205\n",
            "epoch: 1 step: 2474, loss is 0.8026208281517029\n",
            "epoch: 1 step: 2475, loss is 0.8414957523345947\n",
            "epoch: 1 step: 2476, loss is 0.7980125546455383\n",
            "epoch: 1 step: 2477, loss is 0.6634793877601624\n",
            "epoch: 1 step: 2478, loss is 0.9736284017562866\n",
            "epoch: 1 step: 2479, loss is 1.0692697763442993\n",
            "epoch: 1 step: 2480, loss is 1.0972845554351807\n",
            "epoch: 1 step: 2481, loss is 0.8224281072616577\n",
            "epoch: 1 step: 2482, loss is 0.8853270411491394\n",
            "epoch: 1 step: 2483, loss is 0.9614236354827881\n",
            "epoch: 1 step: 2484, loss is 0.7862185835838318\n",
            "epoch: 1 step: 2485, loss is 0.8654205203056335\n",
            "epoch: 1 step: 2486, loss is 1.0315606594085693\n",
            "epoch: 1 step: 2487, loss is 0.9595202207565308\n",
            "epoch: 1 step: 2488, loss is 0.8714835047721863\n",
            "epoch: 1 step: 2489, loss is 0.898952066898346\n",
            "epoch: 1 step: 2490, loss is 0.7066659331321716\n",
            "epoch: 1 step: 2491, loss is 0.8192778825759888\n",
            "epoch: 1 step: 2492, loss is 0.8790431022644043\n",
            "epoch: 1 step: 2493, loss is 0.8395591378211975\n",
            "epoch: 1 step: 2494, loss is 0.8263155817985535\n",
            "epoch: 1 step: 2495, loss is 0.8509740233421326\n",
            "epoch: 1 step: 2496, loss is 0.9667138457298279\n",
            "epoch: 1 step: 2497, loss is 0.7848618030548096\n",
            "epoch: 1 step: 2498, loss is 1.0501394271850586\n",
            "epoch: 1 step: 2499, loss is 0.8546241521835327\n",
            "epoch: 1 step: 2500, loss is 0.8854463696479797\n",
            "epoch: 1 step: 2501, loss is 1.0342669486999512\n",
            "epoch: 1 step: 2502, loss is 0.9690710306167603\n",
            "epoch: 1 step: 2503, loss is 0.8143921494483948\n",
            "epoch: 1 step: 2504, loss is 0.9710955023765564\n",
            "epoch: 1 step: 2505, loss is 0.8545244932174683\n",
            "epoch: 1 step: 2506, loss is 0.989994466304779\n",
            "epoch: 1 step: 2507, loss is 0.9504193067550659\n",
            "epoch: 1 step: 2508, loss is 0.9510110020637512\n",
            "epoch: 1 step: 2509, loss is 0.9147500395774841\n",
            "epoch: 1 step: 2510, loss is 0.8921746611595154\n",
            "epoch: 1 step: 2511, loss is 0.9079092741012573\n",
            "epoch: 1 step: 2512, loss is 0.8569175601005554\n",
            "epoch: 1 step: 2513, loss is 0.8873156309127808\n",
            "epoch: 1 step: 2514, loss is 0.9301778078079224\n",
            "epoch: 1 step: 2515, loss is 1.02809476852417\n",
            "epoch: 1 step: 2516, loss is 0.8918449878692627\n",
            "epoch: 1 step: 2517, loss is 0.8816840648651123\n",
            "epoch: 1 step: 2518, loss is 0.9274985194206238\n",
            "epoch: 1 step: 2519, loss is 0.7776989340782166\n",
            "epoch: 1 step: 2520, loss is 0.9104748368263245\n",
            "epoch: 1 step: 2521, loss is 0.7593174576759338\n",
            "epoch: 1 step: 2522, loss is 0.7542949914932251\n",
            "epoch: 1 step: 2523, loss is 0.7742415070533752\n",
            "epoch: 1 step: 2524, loss is 0.8540751934051514\n",
            "epoch: 1 step: 2525, loss is 0.9952588081359863\n",
            "epoch: 1 step: 2526, loss is 0.8532578349113464\n",
            "epoch: 1 step: 2527, loss is 0.7691118717193604\n",
            "epoch: 1 step: 2528, loss is 0.9811376929283142\n",
            "epoch: 1 step: 2529, loss is 0.8406574726104736\n",
            "epoch: 1 step: 2530, loss is 0.9634467363357544\n",
            "epoch: 1 step: 2531, loss is 0.8593122363090515\n",
            "epoch: 1 step: 2532, loss is 0.8859846591949463\n",
            "epoch: 1 step: 2533, loss is 0.803080141544342\n",
            "epoch: 1 step: 2534, loss is 1.0260249376296997\n",
            "epoch: 1 step: 2535, loss is 0.9340260028839111\n",
            "epoch: 1 step: 2536, loss is 0.8736675381660461\n",
            "epoch: 1 step: 2537, loss is 1.004123568534851\n",
            "epoch: 1 step: 2538, loss is 0.8498179912567139\n",
            "epoch: 1 step: 2539, loss is 0.8372789025306702\n",
            "epoch: 1 step: 2540, loss is 0.9892434477806091\n",
            "epoch: 1 step: 2541, loss is 0.8081381320953369\n",
            "epoch: 1 step: 2542, loss is 0.8093912601470947\n",
            "epoch: 1 step: 2543, loss is 0.9478864073753357\n",
            "epoch: 1 step: 2544, loss is 0.8798010349273682\n",
            "epoch: 1 step: 2545, loss is 0.7298030257225037\n",
            "epoch: 1 step: 2546, loss is 0.7517378330230713\n",
            "epoch: 1 step: 2547, loss is 0.9505682587623596\n",
            "epoch: 1 step: 2548, loss is 0.8543638586997986\n",
            "epoch: 1 step: 2549, loss is 0.781886100769043\n",
            "epoch: 1 step: 2550, loss is 0.8735970854759216\n",
            "epoch: 1 step: 2551, loss is 0.8025481700897217\n",
            "epoch: 1 step: 2552, loss is 0.7901242971420288\n",
            "epoch: 1 step: 2553, loss is 0.8948129415512085\n",
            "epoch: 1 step: 2554, loss is 1.0252302885055542\n",
            "epoch: 1 step: 2555, loss is 0.916614830493927\n",
            "epoch: 1 step: 2556, loss is 0.7652230262756348\n",
            "epoch: 1 step: 2557, loss is 0.7555142045021057\n",
            "epoch: 1 step: 2558, loss is 0.8216463327407837\n",
            "epoch: 1 step: 2559, loss is 0.8095991015434265\n",
            "epoch: 1 step: 2560, loss is 0.9388390183448792\n",
            "epoch: 1 step: 2561, loss is 0.9897788763046265\n",
            "epoch: 1 step: 2562, loss is 0.9015596508979797\n",
            "epoch: 1 step: 2563, loss is 0.9173142313957214\n",
            "epoch: 1 step: 2564, loss is 0.852670431137085\n",
            "epoch: 1 step: 2565, loss is 1.0845979452133179\n",
            "epoch: 1 step: 2566, loss is 0.8891690373420715\n",
            "epoch: 1 step: 2567, loss is 0.9114961624145508\n",
            "epoch: 1 step: 2568, loss is 0.7088936567306519\n",
            "epoch: 1 step: 2569, loss is 0.7113279700279236\n",
            "epoch: 1 step: 2570, loss is 0.7660688757896423\n",
            "epoch: 1 step: 2571, loss is 0.984812319278717\n",
            "epoch: 1 step: 2572, loss is 0.9126618504524231\n",
            "epoch: 1 step: 2573, loss is 0.6255521774291992\n",
            "epoch: 1 step: 2574, loss is 0.7739761471748352\n",
            "epoch: 1 step: 2575, loss is 0.8654813766479492\n",
            "epoch: 1 step: 2576, loss is 0.8357940912246704\n",
            "epoch: 1 step: 2577, loss is 0.9286234974861145\n",
            "epoch: 1 step: 2578, loss is 0.7737823128700256\n",
            "epoch: 1 step: 2579, loss is 0.8918207883834839\n",
            "epoch: 1 step: 2580, loss is 1.0200117826461792\n",
            "epoch: 1 step: 2581, loss is 0.9013127088546753\n",
            "epoch: 1 step: 2582, loss is 0.8992101550102234\n",
            "epoch: 1 step: 2583, loss is 0.908135712146759\n",
            "epoch: 1 step: 2584, loss is 0.8422603011131287\n",
            "epoch: 1 step: 2585, loss is 0.9910187125205994\n",
            "epoch: 1 step: 2586, loss is 0.8802902698516846\n",
            "epoch: 1 step: 2587, loss is 0.8876417279243469\n",
            "epoch: 1 step: 2588, loss is 0.8188967704772949\n",
            "epoch: 1 step: 2589, loss is 0.9349644184112549\n",
            "epoch: 1 step: 2590, loss is 0.9360542893409729\n",
            "epoch: 1 step: 2591, loss is 0.777661919593811\n",
            "epoch: 1 step: 2592, loss is 0.8394864201545715\n",
            "epoch: 1 step: 2593, loss is 0.8211794495582581\n",
            "epoch: 1 step: 2594, loss is 0.8823583722114563\n",
            "epoch: 1 step: 2595, loss is 0.9941776394844055\n",
            "epoch: 1 step: 2596, loss is 1.046436071395874\n",
            "epoch: 1 step: 2597, loss is 0.8056060671806335\n",
            "epoch: 1 step: 2598, loss is 0.9046703577041626\n",
            "epoch: 1 step: 2599, loss is 0.9655044078826904\n",
            "epoch: 1 step: 2600, loss is 0.9838435649871826\n",
            "epoch: 1 step: 2601, loss is 0.8822659850120544\n",
            "epoch: 1 step: 2602, loss is 0.8812269568443298\n",
            "epoch: 1 step: 2603, loss is 0.9402331709861755\n",
            "epoch: 1 step: 2604, loss is 0.8484741449356079\n",
            "epoch: 1 step: 2605, loss is 0.8997446298599243\n",
            "epoch: 1 step: 2606, loss is 0.9204505085945129\n",
            "epoch: 1 step: 2607, loss is 0.8744097948074341\n",
            "epoch: 1 step: 2608, loss is 0.7914420962333679\n",
            "epoch: 1 step: 2609, loss is 1.2001773118972778\n",
            "epoch: 1 step: 2610, loss is 0.80410236120224\n",
            "epoch: 1 step: 2611, loss is 0.8574695587158203\n",
            "epoch: 1 step: 2612, loss is 0.834065318107605\n",
            "epoch: 1 step: 2613, loss is 0.9310089945793152\n",
            "epoch: 1 step: 2614, loss is 0.8977550268173218\n",
            "epoch: 1 step: 2615, loss is 0.9645594954490662\n",
            "epoch: 1 step: 2616, loss is 0.7349601984024048\n",
            "epoch: 1 step: 2617, loss is 0.8307837247848511\n",
            "epoch: 1 step: 2618, loss is 1.0339151620864868\n",
            "epoch: 1 step: 2619, loss is 0.8814948797225952\n",
            "epoch: 1 step: 2620, loss is 0.9112609028816223\n",
            "epoch: 1 step: 2621, loss is 0.7942679524421692\n",
            "epoch: 1 step: 2622, loss is 0.9415016770362854\n",
            "epoch: 1 step: 2623, loss is 0.8268369436264038\n",
            "epoch: 1 step: 2624, loss is 0.7855377793312073\n",
            "epoch: 1 step: 2625, loss is 0.8574258685112\n",
            "epoch: 1 step: 2626, loss is 0.7420631051063538\n",
            "epoch: 1 step: 2627, loss is 0.6734108328819275\n",
            "epoch: 1 step: 2628, loss is 0.9520682096481323\n",
            "epoch: 1 step: 2629, loss is 1.0687016248703003\n",
            "epoch: 1 step: 2630, loss is 0.9296376705169678\n",
            "epoch: 1 step: 2631, loss is 0.9820207357406616\n",
            "epoch: 1 step: 2632, loss is 0.962295413017273\n",
            "epoch: 1 step: 2633, loss is 1.0584988594055176\n",
            "epoch: 1 step: 2634, loss is 0.8337846994400024\n",
            "epoch: 1 step: 2635, loss is 0.9166703820228577\n",
            "epoch: 1 step: 2636, loss is 0.7955099940299988\n",
            "epoch: 1 step: 2637, loss is 0.9383695721626282\n",
            "epoch: 1 step: 2638, loss is 1.0291213989257812\n",
            "epoch: 1 step: 2639, loss is 0.9079163670539856\n",
            "epoch: 1 step: 2640, loss is 0.7785099148750305\n",
            "epoch: 1 step: 2641, loss is 0.8018881678581238\n",
            "epoch: 1 step: 2642, loss is 0.7980285286903381\n",
            "epoch: 1 step: 2643, loss is 0.7203326225280762\n",
            "epoch: 1 step: 2644, loss is 0.8573922514915466\n",
            "epoch: 1 step: 2645, loss is 0.8247409462928772\n",
            "epoch: 1 step: 2646, loss is 0.9949422478675842\n",
            "epoch: 1 step: 2647, loss is 0.8686575889587402\n",
            "epoch: 1 step: 2648, loss is 0.9237626194953918\n",
            "epoch: 1 step: 2649, loss is 0.7323459386825562\n",
            "epoch: 1 step: 2650, loss is 1.0442519187927246\n",
            "epoch: 1 step: 2651, loss is 0.9780551791191101\n",
            "epoch: 1 step: 2652, loss is 0.8278735876083374\n",
            "epoch: 1 step: 2653, loss is 0.9205880761146545\n",
            "epoch: 1 step: 2654, loss is 0.9548386335372925\n",
            "epoch: 1 step: 2655, loss is 0.9025560021400452\n",
            "epoch: 1 step: 2656, loss is 1.021247386932373\n",
            "epoch: 1 step: 2657, loss is 0.916723906993866\n",
            "epoch: 1 step: 2658, loss is 0.8527832627296448\n",
            "epoch: 1 step: 2659, loss is 0.9508066773414612\n",
            "epoch: 1 step: 2660, loss is 0.9428067207336426\n",
            "epoch: 1 step: 2661, loss is 0.7434749603271484\n",
            "epoch: 1 step: 2662, loss is 0.8261212110519409\n",
            "epoch: 1 step: 2663, loss is 1.0174883604049683\n",
            "epoch: 1 step: 2664, loss is 0.8484843373298645\n",
            "epoch: 1 step: 2665, loss is 0.7365272641181946\n",
            "epoch: 1 step: 2666, loss is 1.2432136535644531\n",
            "epoch: 1 step: 2667, loss is 0.9266895055770874\n",
            "epoch: 1 step: 2668, loss is 1.0622777938842773\n",
            "epoch: 1 step: 2669, loss is 0.8249175548553467\n",
            "epoch: 1 step: 2670, loss is 0.9376853108406067\n",
            "epoch: 1 step: 2671, loss is 0.8779250383377075\n",
            "epoch: 1 step: 2672, loss is 0.8576414585113525\n",
            "epoch: 1 step: 2673, loss is 0.7489718198776245\n",
            "epoch: 1 step: 2674, loss is 0.9013009071350098\n",
            "epoch: 1 step: 2675, loss is 1.0173715353012085\n",
            "epoch: 1 step: 2676, loss is 0.7869685292243958\n",
            "epoch: 1 step: 2677, loss is 0.9187993407249451\n",
            "epoch: 1 step: 2678, loss is 0.7765877842903137\n",
            "epoch: 1 step: 2679, loss is 0.8692718148231506\n",
            "epoch: 1 step: 2680, loss is 0.8181664943695068\n",
            "epoch: 1 step: 2681, loss is 0.771146833896637\n",
            "epoch: 1 step: 2682, loss is 0.7980127334594727\n",
            "epoch: 1 step: 2683, loss is 0.9478450417518616\n",
            "epoch: 1 step: 2684, loss is 0.9082297086715698\n",
            "epoch: 1 step: 2685, loss is 1.0943278074264526\n",
            "epoch: 1 step: 2686, loss is 0.8892757296562195\n",
            "epoch: 1 step: 2687, loss is 0.8856728672981262\n",
            "epoch: 1 step: 2688, loss is 0.7816189527511597\n",
            "epoch: 1 step: 2689, loss is 0.9726539850234985\n",
            "epoch: 1 step: 2690, loss is 0.9719526767730713\n",
            "epoch: 1 step: 2691, loss is 0.7757260203361511\n",
            "epoch: 1 step: 2692, loss is 0.9406853318214417\n",
            "epoch: 1 step: 2693, loss is 0.8657082915306091\n",
            "epoch: 1 step: 2694, loss is 0.9543063640594482\n",
            "epoch: 1 step: 2695, loss is 0.9289026856422424\n",
            "epoch: 1 step: 2696, loss is 0.9472159147262573\n",
            "epoch: 1 step: 2697, loss is 0.9275017380714417\n",
            "epoch: 1 step: 2698, loss is 1.071144700050354\n",
            "epoch: 1 step: 2699, loss is 1.0056625604629517\n",
            "epoch: 1 step: 2700, loss is 0.9480560421943665\n",
            "epoch: 1 step: 2701, loss is 0.9588339328765869\n",
            "epoch: 1 step: 2702, loss is 0.7548545598983765\n",
            "epoch: 1 step: 2703, loss is 0.8083174824714661\n",
            "epoch: 1 step: 2704, loss is 0.7934499979019165\n",
            "epoch: 1 step: 2705, loss is 0.782400369644165\n",
            "epoch: 1 step: 2706, loss is 0.867728590965271\n",
            "epoch: 1 step: 2707, loss is 0.875973641872406\n",
            "epoch: 1 step: 2708, loss is 0.90464186668396\n",
            "epoch: 1 step: 2709, loss is 0.6068366169929504\n",
            "epoch: 1 step: 2710, loss is 0.9451143145561218\n",
            "epoch: 1 step: 2711, loss is 1.0409752130508423\n",
            "epoch: 1 step: 2712, loss is 0.8910512924194336\n",
            "epoch: 1 step: 2713, loss is 1.0008374452590942\n",
            "epoch: 1 step: 2714, loss is 0.7138219475746155\n",
            "epoch: 1 step: 2715, loss is 1.0214496850967407\n",
            "epoch: 1 step: 2716, loss is 0.9915038347244263\n",
            "epoch: 1 step: 2717, loss is 0.7900471687316895\n",
            "epoch: 1 step: 2718, loss is 0.8439326286315918\n",
            "epoch: 1 step: 2719, loss is 0.9647834897041321\n",
            "epoch: 1 step: 2720, loss is 0.8138589859008789\n",
            "epoch: 1 step: 2721, loss is 0.9506988525390625\n",
            "epoch: 1 step: 2722, loss is 0.8273133635520935\n",
            "epoch: 1 step: 2723, loss is 0.9309810996055603\n",
            "epoch: 1 step: 2724, loss is 0.8762098550796509\n",
            "epoch: 1 step: 2725, loss is 0.8702473044395447\n",
            "epoch: 1 step: 2726, loss is 0.9303247332572937\n",
            "epoch: 1 step: 2727, loss is 0.91801917552948\n",
            "epoch: 1 step: 2728, loss is 0.948478639125824\n",
            "epoch: 1 step: 2729, loss is 0.9811548590660095\n",
            "epoch: 1 step: 2730, loss is 0.8555371761322021\n",
            "epoch: 1 step: 2731, loss is 0.8515023589134216\n",
            "epoch: 1 step: 2732, loss is 0.858001708984375\n",
            "epoch: 1 step: 2733, loss is 0.8732102513313293\n",
            "epoch: 1 step: 2734, loss is 0.9165911674499512\n",
            "epoch: 1 step: 2735, loss is 1.0764325857162476\n",
            "epoch: 1 step: 2736, loss is 0.8385117053985596\n",
            "epoch: 1 step: 2737, loss is 0.9554375410079956\n",
            "epoch: 1 step: 2738, loss is 0.9523035287857056\n",
            "epoch: 1 step: 2739, loss is 0.9006261229515076\n",
            "epoch: 1 step: 2740, loss is 0.9408940076828003\n",
            "epoch: 1 step: 2741, loss is 0.9649812579154968\n",
            "epoch: 1 step: 2742, loss is 1.0100383758544922\n",
            "epoch: 1 step: 2743, loss is 0.9041334390640259\n",
            "epoch: 1 step: 2744, loss is 0.7740830183029175\n",
            "epoch: 1 step: 2745, loss is 0.8855130672454834\n",
            "epoch: 1 step: 2746, loss is 1.0016250610351562\n",
            "epoch: 1 step: 2747, loss is 0.8488641977310181\n",
            "epoch: 1 step: 2748, loss is 0.8995150327682495\n",
            "epoch: 1 step: 2749, loss is 0.8038985133171082\n",
            "epoch: 1 step: 2750, loss is 0.8169668912887573\n",
            "epoch: 1 step: 2751, loss is 0.9926835298538208\n",
            "epoch: 1 step: 2752, loss is 1.0105042457580566\n",
            "epoch: 1 step: 2753, loss is 1.00649094581604\n",
            "epoch: 1 step: 2754, loss is 0.7655709981918335\n",
            "epoch: 1 step: 2755, loss is 1.155701994895935\n",
            "epoch: 1 step: 2756, loss is 0.8820028305053711\n",
            "epoch: 1 step: 2757, loss is 0.888624906539917\n",
            "epoch: 1 step: 2758, loss is 0.8755154609680176\n",
            "epoch: 1 step: 2759, loss is 0.8673008680343628\n",
            "epoch: 1 step: 2760, loss is 0.9031177759170532\n",
            "epoch: 1 step: 2761, loss is 0.804497241973877\n",
            "epoch: 1 step: 2762, loss is 0.8495381474494934\n",
            "epoch: 1 step: 2763, loss is 0.8449327945709229\n",
            "epoch: 1 step: 2764, loss is 0.689691960811615\n",
            "epoch: 1 step: 2765, loss is 0.8297837972640991\n",
            "epoch: 1 step: 2766, loss is 1.1360361576080322\n",
            "epoch: 1 step: 2767, loss is 0.9756027460098267\n",
            "epoch: 1 step: 2768, loss is 1.1281459331512451\n",
            "epoch: 1 step: 2769, loss is 0.7861983776092529\n",
            "epoch: 1 step: 2770, loss is 0.7828750610351562\n",
            "epoch: 1 step: 2771, loss is 0.9118704795837402\n",
            "epoch: 1 step: 2772, loss is 0.8764100670814514\n",
            "epoch: 1 step: 2773, loss is 0.776253879070282\n",
            "epoch: 1 step: 2774, loss is 0.7822384834289551\n",
            "epoch: 1 step: 2775, loss is 0.9280900359153748\n",
            "epoch: 1 step: 2776, loss is 0.8591883182525635\n",
            "epoch: 1 step: 2777, loss is 1.1081767082214355\n",
            "epoch: 1 step: 2778, loss is 0.9288278818130493\n",
            "epoch: 1 step: 2779, loss is 0.8418636918067932\n",
            "epoch: 1 step: 2780, loss is 0.9396321773529053\n",
            "epoch: 1 step: 2781, loss is 0.8824425339698792\n",
            "epoch: 1 step: 2782, loss is 0.9874446392059326\n",
            "epoch: 1 step: 2783, loss is 0.9624810218811035\n",
            "epoch: 1 step: 2784, loss is 0.8685077428817749\n",
            "epoch: 1 step: 2785, loss is 0.9922081828117371\n",
            "epoch: 1 step: 2786, loss is 0.8725765943527222\n",
            "epoch: 1 step: 2787, loss is 0.9620686173439026\n",
            "epoch: 1 step: 2788, loss is 0.8681060671806335\n",
            "epoch: 1 step: 2789, loss is 1.015636920928955\n",
            "epoch: 1 step: 2790, loss is 0.920492947101593\n",
            "epoch: 1 step: 2791, loss is 0.8175942897796631\n",
            "epoch: 1 step: 2792, loss is 0.7309722304344177\n",
            "epoch: 1 step: 2793, loss is 0.7562320232391357\n",
            "epoch: 1 step: 2794, loss is 0.7445647120475769\n",
            "epoch: 1 step: 2795, loss is 0.9669966697692871\n",
            "epoch: 1 step: 2796, loss is 0.9932781457901001\n",
            "epoch: 1 step: 2797, loss is 0.8841187953948975\n",
            "epoch: 1 step: 2798, loss is 0.8118840456008911\n",
            "epoch: 1 step: 2799, loss is 0.7244412899017334\n",
            "epoch: 1 step: 2800, loss is 1.0492513179779053\n",
            "epoch: 1 step: 2801, loss is 0.9586952924728394\n",
            "epoch: 1 step: 2802, loss is 0.8459534645080566\n",
            "epoch: 1 step: 2803, loss is 0.9234148859977722\n",
            "epoch: 1 step: 2804, loss is 0.8997147083282471\n",
            "epoch: 1 step: 2805, loss is 1.0204359292984009\n",
            "epoch: 1 step: 2806, loss is 0.9905996322631836\n",
            "epoch: 1 step: 2807, loss is 0.8198849558830261\n",
            "epoch: 1 step: 2808, loss is 0.8122130036354065\n",
            "epoch: 1 step: 2809, loss is 0.9808410406112671\n",
            "epoch: 1 step: 2810, loss is 0.8496633172035217\n",
            "epoch: 1 step: 2811, loss is 0.801063597202301\n",
            "epoch: 1 step: 2812, loss is 1.0957412719726562\n",
            "epoch: 1 step: 2813, loss is 0.7525549530982971\n",
            "epoch: 1 step: 2814, loss is 0.7328975200653076\n",
            "epoch: 1 step: 2815, loss is 0.834259569644928\n",
            "epoch: 1 step: 2816, loss is 0.8931390047073364\n",
            "epoch: 1 step: 2817, loss is 0.7008628249168396\n",
            "epoch: 1 step: 2818, loss is 1.0173585414886475\n",
            "epoch: 1 step: 2819, loss is 0.9023416042327881\n",
            "epoch: 1 step: 2820, loss is 0.9918487071990967\n",
            "epoch: 1 step: 2821, loss is 0.8011653423309326\n",
            "epoch: 1 step: 2822, loss is 0.7878629565238953\n",
            "epoch: 1 step: 2823, loss is 0.776208221912384\n",
            "epoch: 1 step: 2824, loss is 1.0207717418670654\n",
            "epoch: 1 step: 2825, loss is 0.9419232606887817\n",
            "epoch: 1 step: 2826, loss is 0.9958502054214478\n",
            "epoch: 1 step: 2827, loss is 0.93357253074646\n",
            "epoch: 1 step: 2828, loss is 1.0449469089508057\n",
            "epoch: 1 step: 2829, loss is 0.8070030808448792\n",
            "epoch: 1 step: 2830, loss is 0.908731997013092\n",
            "epoch: 1 step: 2831, loss is 0.8802222609519958\n",
            "epoch: 1 step: 2832, loss is 0.7901530861854553\n",
            "epoch: 1 step: 2833, loss is 0.9130340814590454\n",
            "epoch: 1 step: 2834, loss is 1.0325170755386353\n",
            "epoch: 1 step: 2835, loss is 1.1186366081237793\n",
            "epoch: 1 step: 2836, loss is 0.9169317483901978\n",
            "epoch: 1 step: 2837, loss is 0.8667079210281372\n",
            "epoch: 1 step: 2838, loss is 0.8668922185897827\n",
            "epoch: 1 step: 2839, loss is 0.8210513591766357\n",
            "epoch: 1 step: 2840, loss is 0.7926239967346191\n",
            "epoch: 1 step: 2841, loss is 1.0205740928649902\n",
            "epoch: 1 step: 2842, loss is 0.7397133111953735\n",
            "epoch: 1 step: 2843, loss is 0.9563748836517334\n",
            "epoch: 1 step: 2844, loss is 0.9171772599220276\n",
            "epoch: 1 step: 2845, loss is 0.8061387538909912\n",
            "epoch: 1 step: 2846, loss is 0.9649194478988647\n",
            "epoch: 1 step: 2847, loss is 1.051160454750061\n",
            "epoch: 1 step: 2848, loss is 0.8557155132293701\n",
            "epoch: 1 step: 2849, loss is 0.9752535223960876\n",
            "epoch: 1 step: 2850, loss is 0.8303086161613464\n",
            "epoch: 1 step: 2851, loss is 0.7511167526245117\n",
            "epoch: 1 step: 2852, loss is 0.834226667881012\n",
            "epoch: 1 step: 2853, loss is 0.929961621761322\n",
            "epoch: 1 step: 2854, loss is 0.9588886499404907\n",
            "epoch: 1 step: 2855, loss is 0.7776239514350891\n",
            "epoch: 1 step: 2856, loss is 0.8089590072631836\n",
            "epoch: 1 step: 2857, loss is 1.0921599864959717\n",
            "epoch: 1 step: 2858, loss is 0.8366807699203491\n",
            "epoch: 1 step: 2859, loss is 0.7671942710876465\n",
            "epoch: 1 step: 2860, loss is 1.1258333921432495\n",
            "epoch: 1 step: 2861, loss is 0.8764727115631104\n",
            "epoch: 1 step: 2862, loss is 0.8417937159538269\n",
            "epoch: 1 step: 2863, loss is 0.8706372380256653\n",
            "epoch: 1 step: 2864, loss is 0.7509192228317261\n",
            "epoch: 1 step: 2865, loss is 0.9357157945632935\n",
            "epoch: 1 step: 2866, loss is 0.9765411019325256\n",
            "epoch: 1 step: 2867, loss is 0.8546361923217773\n",
            "epoch: 1 step: 2868, loss is 0.879000186920166\n",
            "epoch: 1 step: 2869, loss is 0.9144582152366638\n",
            "epoch: 1 step: 2870, loss is 0.7760827541351318\n",
            "epoch: 1 step: 2871, loss is 0.965640127658844\n",
            "epoch: 1 step: 2872, loss is 0.6765318512916565\n",
            "epoch: 1 step: 2873, loss is 0.9499346613883972\n",
            "epoch: 1 step: 2874, loss is 0.7824787497520447\n",
            "epoch: 1 step: 2875, loss is 0.7907423377037048\n",
            "epoch: 1 step: 2876, loss is 0.9452962279319763\n",
            "epoch: 1 step: 2877, loss is 0.8470344543457031\n",
            "epoch: 1 step: 2878, loss is 0.8942394852638245\n",
            "epoch: 1 step: 2879, loss is 0.8668038845062256\n",
            "epoch: 1 step: 2880, loss is 0.7679713368415833\n",
            "epoch: 1 step: 2881, loss is 0.9170113205909729\n",
            "epoch: 1 step: 2882, loss is 0.893200159072876\n",
            "epoch: 1 step: 2883, loss is 1.0835750102996826\n",
            "epoch: 1 step: 2884, loss is 0.8994825482368469\n",
            "epoch: 1 step: 2885, loss is 1.1014912128448486\n",
            "epoch: 1 step: 2886, loss is 0.9001892805099487\n",
            "epoch: 1 step: 2887, loss is 1.042108416557312\n",
            "epoch: 1 step: 2888, loss is 0.9337023496627808\n",
            "epoch: 1 step: 2889, loss is 0.8636108040809631\n",
            "epoch: 1 step: 2890, loss is 0.803869366645813\n",
            "epoch: 1 step: 2891, loss is 0.6817016005516052\n",
            "epoch: 1 step: 2892, loss is 0.992737889289856\n",
            "epoch: 1 step: 2893, loss is 0.8020945191383362\n",
            "epoch: 1 step: 2894, loss is 0.9590722918510437\n",
            "epoch: 1 step: 2895, loss is 0.8951866030693054\n",
            "epoch: 1 step: 2896, loss is 0.9142325520515442\n",
            "epoch: 1 step: 2897, loss is 0.9850554466247559\n",
            "epoch: 1 step: 2898, loss is 0.9179366230964661\n",
            "epoch: 1 step: 2899, loss is 1.1221593618392944\n",
            "epoch: 1 step: 2900, loss is 0.9542747139930725\n",
            "epoch: 1 step: 2901, loss is 0.8666461706161499\n",
            "epoch: 1 step: 2902, loss is 0.7689593434333801\n",
            "epoch: 1 step: 2903, loss is 0.7730015516281128\n",
            "epoch: 1 step: 2904, loss is 1.10458242893219\n",
            "epoch: 1 step: 2905, loss is 0.8852640390396118\n",
            "epoch: 1 step: 2906, loss is 0.9380767345428467\n",
            "epoch: 1 step: 2907, loss is 0.8273259401321411\n",
            "epoch: 1 step: 2908, loss is 1.087951421737671\n",
            "epoch: 1 step: 2909, loss is 0.8669357895851135\n",
            "epoch: 1 step: 2910, loss is 0.872484028339386\n",
            "epoch: 1 step: 2911, loss is 0.9192456007003784\n",
            "epoch: 1 step: 2912, loss is 0.8436104655265808\n",
            "epoch: 1 step: 2913, loss is 0.8490992784500122\n",
            "epoch: 1 step: 2914, loss is 0.9091583490371704\n",
            "epoch: 1 step: 2915, loss is 0.9738068580627441\n",
            "epoch: 1 step: 2916, loss is 0.9878576993942261\n",
            "epoch: 1 step: 2917, loss is 0.924510657787323\n",
            "epoch: 1 step: 2918, loss is 0.8944846987724304\n",
            "epoch: 1 step: 2919, loss is 0.8286025524139404\n",
            "epoch: 1 step: 2920, loss is 0.9608528017997742\n",
            "epoch: 1 step: 2921, loss is 0.7125463485717773\n",
            "epoch: 1 step: 2922, loss is 0.767092764377594\n",
            "epoch: 1 step: 2923, loss is 0.7886480689048767\n",
            "epoch: 1 step: 2924, loss is 0.8000701665878296\n",
            "epoch: 1 step: 2925, loss is 1.0981369018554688\n",
            "epoch: 1 step: 2926, loss is 0.8694689869880676\n",
            "epoch: 1 step: 2927, loss is 0.8239132761955261\n",
            "epoch: 1 step: 2928, loss is 0.7820026874542236\n",
            "epoch: 1 step: 2929, loss is 0.9918859004974365\n",
            "epoch: 1 step: 2930, loss is 1.0335276126861572\n",
            "epoch: 1 step: 2931, loss is 1.0608773231506348\n",
            "epoch: 1 step: 2932, loss is 0.8392384648323059\n",
            "epoch: 1 step: 2933, loss is 0.9308950304985046\n",
            "epoch: 1 step: 2934, loss is 0.8050956726074219\n",
            "epoch: 1 step: 2935, loss is 0.926791250705719\n",
            "epoch: 1 step: 2936, loss is 0.9403439164161682\n",
            "epoch: 1 step: 2937, loss is 0.706516444683075\n",
            "epoch: 1 step: 2938, loss is 0.8937016129493713\n",
            "epoch: 1 step: 2939, loss is 1.2607308626174927\n",
            "epoch: 1 step: 2940, loss is 0.8327763080596924\n",
            "epoch: 1 step: 2941, loss is 0.8734294176101685\n",
            "epoch: 1 step: 2942, loss is 0.8847163915634155\n",
            "epoch: 1 step: 2943, loss is 0.9945582747459412\n",
            "epoch: 1 step: 2944, loss is 0.8312429785728455\n",
            "epoch: 1 step: 2945, loss is 0.8567167520523071\n",
            "epoch: 1 step: 2946, loss is 1.031668782234192\n",
            "epoch: 1 step: 2947, loss is 1.0104243755340576\n",
            "epoch: 1 step: 2948, loss is 0.9809989333152771\n",
            "epoch: 1 step: 2949, loss is 1.0488591194152832\n",
            "epoch: 1 step: 2950, loss is 0.7685020565986633\n",
            "epoch: 1 step: 2951, loss is 1.033929467201233\n",
            "epoch: 1 step: 2952, loss is 0.7966051697731018\n",
            "epoch: 1 step: 2953, loss is 1.1253458261489868\n",
            "epoch: 1 step: 2954, loss is 0.9298369884490967\n",
            "epoch: 1 step: 2955, loss is 1.03389573097229\n",
            "epoch: 1 step: 2956, loss is 0.7827274799346924\n",
            "epoch: 1 step: 2957, loss is 0.8635767698287964\n",
            "epoch: 1 step: 2958, loss is 1.1273142099380493\n",
            "epoch: 1 step: 2959, loss is 0.8365164399147034\n",
            "epoch: 1 step: 2960, loss is 0.8761963844299316\n",
            "epoch: 1 step: 2961, loss is 0.9038248658180237\n",
            "epoch: 1 step: 2962, loss is 1.1159504652023315\n",
            "epoch: 1 step: 2963, loss is 0.9533262848854065\n",
            "epoch: 1 step: 2964, loss is 0.8566034436225891\n",
            "epoch: 1 step: 2965, loss is 0.9584034085273743\n",
            "epoch: 1 step: 2966, loss is 0.9996637105941772\n",
            "epoch: 1 step: 2967, loss is 0.7610146999359131\n",
            "epoch: 1 step: 2968, loss is 1.0358303785324097\n",
            "epoch: 1 step: 2969, loss is 1.075034499168396\n",
            "epoch: 1 step: 2970, loss is 0.855164647102356\n",
            "epoch: 1 step: 2971, loss is 0.7936791181564331\n",
            "epoch: 1 step: 2972, loss is 0.9252342581748962\n",
            "epoch: 1 step: 2973, loss is 0.9204171895980835\n",
            "epoch: 1 step: 2974, loss is 0.8238660097122192\n",
            "epoch: 1 step: 2975, loss is 0.8269355297088623\n",
            "epoch: 1 step: 2976, loss is 0.8853771090507507\n",
            "epoch: 1 step: 2977, loss is 0.8691450953483582\n",
            "epoch: 1 step: 2978, loss is 0.8903122544288635\n",
            "epoch: 1 step: 2979, loss is 0.7705276608467102\n",
            "epoch: 1 step: 2980, loss is 0.9854381084442139\n",
            "epoch: 1 step: 2981, loss is 0.8202767372131348\n",
            "epoch: 1 step: 2982, loss is 0.9534746408462524\n",
            "epoch: 1 step: 2983, loss is 0.955552875995636\n",
            "epoch: 1 step: 2984, loss is 0.7990089058876038\n",
            "epoch: 1 step: 2985, loss is 1.0533910989761353\n",
            "epoch: 1 step: 2986, loss is 1.1009244918823242\n",
            "epoch: 1 step: 2987, loss is 0.9283849596977234\n",
            "epoch: 1 step: 2988, loss is 0.7526752352714539\n",
            "epoch: 1 step: 2989, loss is 0.8379433751106262\n",
            "epoch: 1 step: 2990, loss is 0.8646908402442932\n",
            "epoch: 1 step: 2991, loss is 0.8855884671211243\n",
            "epoch: 1 step: 2992, loss is 0.8684564828872681\n",
            "epoch: 1 step: 2993, loss is 0.9610214829444885\n",
            "epoch: 1 step: 2994, loss is 0.9516611099243164\n",
            "epoch: 1 step: 2995, loss is 0.9356390833854675\n",
            "epoch: 1 step: 2996, loss is 0.9228264689445496\n",
            "epoch: 1 step: 2997, loss is 0.8305862545967102\n",
            "epoch: 1 step: 2998, loss is 0.8792204260826111\n",
            "epoch: 1 step: 2999, loss is 0.9034717082977295\n",
            "epoch: 1 step: 3000, loss is 0.856966495513916\n",
            "epoch: 1 step: 3001, loss is 0.7568689584732056\n",
            "epoch: 1 step: 3002, loss is 0.7585909962654114\n",
            "epoch: 1 step: 3003, loss is 0.9239916205406189\n",
            "epoch: 1 step: 3004, loss is 1.0241596698760986\n",
            "epoch: 1 step: 3005, loss is 0.7887510061264038\n",
            "epoch: 1 step: 3006, loss is 0.9595350623130798\n",
            "epoch: 1 step: 3007, loss is 0.6727859973907471\n",
            "epoch: 1 step: 3008, loss is 0.8426745533943176\n",
            "epoch: 1 step: 3009, loss is 0.8473613858222961\n",
            "epoch: 1 step: 3010, loss is 0.8240900635719299\n",
            "epoch: 1 step: 3011, loss is 0.9927241206169128\n",
            "epoch: 1 step: 3012, loss is 0.8717656135559082\n",
            "epoch: 1 step: 3013, loss is 1.0511819124221802\n",
            "epoch: 1 step: 3014, loss is 0.7719948887825012\n",
            "epoch: 1 step: 3015, loss is 0.8895552754402161\n",
            "epoch: 1 step: 3016, loss is 0.8851100206375122\n",
            "epoch: 1 step: 3017, loss is 0.9881075620651245\n",
            "epoch: 1 step: 3018, loss is 1.0064700841903687\n",
            "epoch: 1 step: 3019, loss is 0.8657519221305847\n",
            "epoch: 1 step: 3020, loss is 0.8519635200500488\n",
            "epoch: 1 step: 3021, loss is 0.8565117120742798\n",
            "epoch: 1 step: 3022, loss is 0.7655596733093262\n",
            "epoch: 1 step: 3023, loss is 0.9300627112388611\n",
            "epoch: 1 step: 3024, loss is 0.8401246070861816\n",
            "epoch: 1 step: 3025, loss is 0.887362003326416\n",
            "epoch: 1 step: 3026, loss is 0.8415561318397522\n",
            "epoch: 1 step: 3027, loss is 0.7813954949378967\n",
            "epoch: 1 step: 3028, loss is 0.6847420334815979\n",
            "epoch: 1 step: 3029, loss is 0.9768702387809753\n",
            "epoch: 1 step: 3030, loss is 0.9350088238716125\n",
            "epoch: 1 step: 3031, loss is 0.9394528269767761\n",
            "epoch: 1 step: 3032, loss is 0.8068495392799377\n",
            "epoch: 1 step: 3033, loss is 0.9095239043235779\n",
            "epoch: 1 step: 3034, loss is 1.022578239440918\n",
            "epoch: 1 step: 3035, loss is 0.8243138790130615\n",
            "epoch: 1 step: 3036, loss is 0.894412100315094\n",
            "epoch: 1 step: 3037, loss is 1.0771145820617676\n",
            "epoch: 1 step: 3038, loss is 1.0875288248062134\n",
            "epoch: 1 step: 3039, loss is 0.8397974967956543\n",
            "epoch: 1 step: 3040, loss is 0.8825918436050415\n",
            "epoch: 1 step: 3041, loss is 1.1063299179077148\n",
            "epoch: 1 step: 3042, loss is 0.9410043954849243\n",
            "epoch: 1 step: 3043, loss is 0.9220795035362244\n",
            "epoch: 1 step: 3044, loss is 0.9576023817062378\n",
            "epoch: 1 step: 3045, loss is 0.9562507271766663\n",
            "epoch: 1 step: 3046, loss is 1.067117691040039\n",
            "epoch: 1 step: 3047, loss is 1.0591027736663818\n",
            "epoch: 1 step: 3048, loss is 0.9467911124229431\n",
            "epoch: 1 step: 3049, loss is 0.7486380934715271\n",
            "epoch: 1 step: 3050, loss is 1.0180132389068604\n",
            "epoch: 1 step: 3051, loss is 1.0669175386428833\n",
            "epoch: 1 step: 3052, loss is 1.0630204677581787\n",
            "epoch: 1 step: 3053, loss is 1.070915937423706\n",
            "epoch: 1 step: 3054, loss is 0.965848445892334\n",
            "epoch: 1 step: 3055, loss is 0.8754446506500244\n",
            "epoch: 1 step: 3056, loss is 0.8164281249046326\n",
            "epoch: 1 step: 3057, loss is 0.871143102645874\n",
            "epoch: 1 step: 3058, loss is 0.874868631362915\n",
            "epoch: 1 step: 3059, loss is 0.8553361296653748\n",
            "epoch: 1 step: 3060, loss is 0.8781787753105164\n",
            "epoch: 1 step: 3061, loss is 0.8076958060264587\n",
            "epoch: 1 step: 3062, loss is 0.8475037813186646\n",
            "epoch: 1 step: 3063, loss is 1.0412132740020752\n",
            "epoch: 1 step: 3064, loss is 1.0413897037506104\n",
            "epoch: 1 step: 3065, loss is 0.8557683825492859\n",
            "epoch: 1 step: 3066, loss is 0.870713472366333\n",
            "epoch: 1 step: 3067, loss is 0.9062808752059937\n",
            "epoch: 1 step: 3068, loss is 0.871222972869873\n",
            "epoch: 1 step: 3069, loss is 0.9504097700119019\n",
            "epoch: 1 step: 3070, loss is 0.924318790435791\n",
            "epoch: 1 step: 3071, loss is 0.8234288692474365\n",
            "epoch: 1 step: 3072, loss is 0.7856208086013794\n",
            "epoch: 1 step: 3073, loss is 0.9700225591659546\n",
            "epoch: 1 step: 3074, loss is 0.7643786072731018\n",
            "epoch: 1 step: 3075, loss is 0.9513098001480103\n",
            "epoch: 1 step: 3076, loss is 0.8208828568458557\n",
            "epoch: 1 step: 3077, loss is 1.0149822235107422\n",
            "epoch: 1 step: 3078, loss is 0.87569659948349\n",
            "epoch: 1 step: 3079, loss is 0.8725075125694275\n",
            "epoch: 1 step: 3080, loss is 1.0151209831237793\n",
            "epoch: 1 step: 3081, loss is 0.7598033547401428\n",
            "epoch: 1 step: 3082, loss is 0.8377012014389038\n",
            "epoch: 1 step: 3083, loss is 0.8681968450546265\n",
            "epoch: 1 step: 3084, loss is 0.9379152059555054\n",
            "epoch: 1 step: 3085, loss is 0.941144585609436\n",
            "epoch: 1 step: 3086, loss is 0.9536880850791931\n",
            "epoch: 1 step: 3087, loss is 0.8663928508758545\n",
            "epoch: 1 step: 3088, loss is 0.9527977705001831\n",
            "epoch: 1 step: 3089, loss is 1.0285078287124634\n",
            "epoch: 1 step: 3090, loss is 0.9327225089073181\n",
            "epoch: 1 step: 3091, loss is 1.0376111268997192\n",
            "epoch: 1 step: 3092, loss is 0.9761757850646973\n",
            "epoch: 1 step: 3093, loss is 1.097164511680603\n",
            "epoch: 1 step: 3094, loss is 0.8388599157333374\n",
            "epoch: 1 step: 3095, loss is 0.8544580340385437\n",
            "epoch: 1 step: 3096, loss is 0.9099867343902588\n",
            "epoch: 1 step: 3097, loss is 0.9587345719337463\n",
            "epoch: 1 step: 3098, loss is 0.9740432500839233\n",
            "epoch: 1 step: 3099, loss is 0.8562432527542114\n",
            "epoch: 1 step: 3100, loss is 0.8054972887039185\n",
            "epoch: 1 step: 3101, loss is 0.9385406374931335\n",
            "epoch: 1 step: 3102, loss is 0.8920573592185974\n",
            "epoch: 1 step: 3103, loss is 0.8180537819862366\n",
            "epoch: 1 step: 3104, loss is 0.8643920421600342\n",
            "epoch: 1 step: 3105, loss is 0.7725152373313904\n",
            "epoch: 1 step: 3106, loss is 0.9867445826530457\n",
            "epoch: 1 step: 3107, loss is 0.904090940952301\n",
            "epoch: 1 step: 3108, loss is 0.9059001803398132\n",
            "epoch: 1 step: 3109, loss is 0.9968658089637756\n",
            "epoch: 1 step: 3110, loss is 0.8304545283317566\n",
            "epoch: 1 step: 3111, loss is 0.7808699607849121\n",
            "epoch: 1 step: 3112, loss is 0.8640503883361816\n",
            "epoch: 1 step: 3113, loss is 0.8168177008628845\n",
            "epoch: 1 step: 3114, loss is 0.9687473177909851\n",
            "epoch: 1 step: 3115, loss is 0.8957291841506958\n",
            "epoch: 1 step: 3116, loss is 0.9205520749092102\n",
            "epoch: 1 step: 3117, loss is 0.8711411952972412\n",
            "epoch: 1 step: 3118, loss is 0.9260280132293701\n",
            "epoch: 1 step: 3119, loss is 1.0325602293014526\n",
            "epoch: 1 step: 3120, loss is 0.9978683590888977\n",
            "epoch: 1 step: 3121, loss is 0.9933927655220032\n",
            "epoch: 1 step: 3122, loss is 0.8139068484306335\n",
            "epoch: 1 step: 3123, loss is 0.7707359194755554\n",
            "epoch: 1 step: 3124, loss is 0.992180347442627\n",
            "epoch: 1 step: 3125, loss is 0.9422685503959656\n",
            "epoch: 1 step: 3126, loss is 0.75145423412323\n",
            "epoch: 1 step: 3127, loss is 0.8130770921707153\n",
            "epoch: 1 step: 3128, loss is 0.853959858417511\n",
            "epoch: 1 step: 3129, loss is 1.0099139213562012\n",
            "epoch: 1 step: 3130, loss is 0.9551278352737427\n",
            "epoch: 1 step: 3131, loss is 0.8122431039810181\n",
            "epoch: 1 step: 3132, loss is 0.9539768099784851\n",
            "epoch: 1 step: 3133, loss is 0.9893126487731934\n",
            "epoch: 1 step: 3134, loss is 0.9072471261024475\n",
            "epoch: 1 step: 3135, loss is 0.7434186935424805\n",
            "epoch: 1 step: 3136, loss is 1.020419955253601\n",
            "epoch: 1 step: 3137, loss is 1.2230348587036133\n",
            "epoch: 1 step: 3138, loss is 0.9040083289146423\n",
            "epoch: 1 step: 3139, loss is 0.7433677911758423\n",
            "epoch: 1 step: 3140, loss is 0.7774598002433777\n",
            "epoch: 1 step: 3141, loss is 0.9696672558784485\n",
            "epoch: 1 step: 3142, loss is 0.9437647461891174\n",
            "epoch: 1 step: 3143, loss is 0.934657096862793\n",
            "epoch: 1 step: 3144, loss is 0.7138539552688599\n",
            "epoch: 1 step: 3145, loss is 0.8304147720336914\n",
            "epoch: 1 step: 3146, loss is 0.942061722278595\n",
            "epoch: 1 step: 3147, loss is 0.985197901725769\n",
            "epoch: 1 step: 3148, loss is 0.8002623319625854\n",
            "epoch: 1 step: 3149, loss is 0.7676341533660889\n",
            "epoch: 1 step: 3150, loss is 0.8982322216033936\n",
            "epoch: 1 step: 3151, loss is 0.9541909694671631\n",
            "epoch: 1 step: 3152, loss is 0.8283442854881287\n",
            "epoch: 1 step: 3153, loss is 0.962314248085022\n",
            "epoch: 1 step: 3154, loss is 1.0658373832702637\n",
            "epoch: 1 step: 3155, loss is 0.914721667766571\n",
            "epoch: 1 step: 3156, loss is 0.7893208265304565\n",
            "epoch: 1 step: 3157, loss is 1.229902744293213\n",
            "epoch: 1 step: 3158, loss is 0.823738157749176\n",
            "epoch: 1 step: 3159, loss is 0.7615455389022827\n",
            "epoch: 1 step: 3160, loss is 1.0067675113677979\n",
            "epoch: 1 step: 3161, loss is 0.9210718274116516\n",
            "epoch: 1 step: 3162, loss is 0.988290548324585\n",
            "epoch: 1 step: 3163, loss is 0.856866180896759\n",
            "epoch: 1 step: 3164, loss is 0.7357683181762695\n",
            "epoch: 1 step: 3165, loss is 0.891976535320282\n",
            "epoch: 1 step: 3166, loss is 0.9064655900001526\n",
            "epoch: 1 step: 3167, loss is 0.788745105266571\n",
            "epoch: 1 step: 3168, loss is 0.7900825142860413\n",
            "epoch: 1 step: 3169, loss is 0.7537028789520264\n",
            "epoch: 1 step: 3170, loss is 0.8093768954277039\n",
            "epoch: 1 step: 3171, loss is 1.0685904026031494\n",
            "epoch: 1 step: 3172, loss is 0.8430434465408325\n",
            "epoch: 1 step: 3173, loss is 0.8773300051689148\n",
            "epoch: 1 step: 3174, loss is 0.907194197177887\n",
            "epoch: 1 step: 3175, loss is 0.8813661336898804\n",
            "epoch: 1 step: 3176, loss is 0.8593846559524536\n",
            "epoch: 1 step: 3177, loss is 1.021875262260437\n",
            "epoch: 1 step: 3178, loss is 0.9216639399528503\n",
            "epoch: 1 step: 3179, loss is 0.9758212566375732\n",
            "epoch: 1 step: 3180, loss is 0.9762115478515625\n",
            "epoch: 1 step: 3181, loss is 0.8855834007263184\n",
            "epoch: 1 step: 3182, loss is 1.0766072273254395\n",
            "epoch: 1 step: 3183, loss is 0.7178207635879517\n",
            "epoch: 1 step: 3184, loss is 0.8216999769210815\n",
            "epoch: 1 step: 3185, loss is 0.7631371021270752\n",
            "epoch: 1 step: 3186, loss is 0.8856614232063293\n",
            "epoch: 1 step: 3187, loss is 0.8679301142692566\n",
            "epoch: 1 step: 3188, loss is 0.805543839931488\n",
            "epoch: 1 step: 3189, loss is 0.9996948838233948\n",
            "epoch: 1 step: 3190, loss is 0.8125883936882019\n",
            "epoch: 1 step: 3191, loss is 1.054304838180542\n",
            "epoch: 1 step: 3192, loss is 0.9251313209533691\n",
            "epoch: 1 step: 3193, loss is 0.9318907260894775\n",
            "epoch: 1 step: 3194, loss is 0.7879613637924194\n",
            "epoch: 1 step: 3195, loss is 0.9059008359909058\n",
            "epoch: 1 step: 3196, loss is 0.9351444244384766\n",
            "epoch: 1 step: 3197, loss is 0.7956005334854126\n",
            "epoch: 1 step: 3198, loss is 0.7369390726089478\n",
            "epoch: 1 step: 3199, loss is 0.9947842359542847\n",
            "epoch: 1 step: 3200, loss is 1.0006954669952393\n",
            "epoch: 1 step: 3201, loss is 0.9302878379821777\n",
            "epoch: 1 step: 3202, loss is 0.8606246709823608\n",
            "epoch: 1 step: 3203, loss is 0.9059870839118958\n",
            "epoch: 1 step: 3204, loss is 0.9294586777687073\n",
            "epoch: 1 step: 3205, loss is 0.8562365770339966\n",
            "epoch: 1 step: 3206, loss is 0.7432945370674133\n",
            "epoch: 1 step: 3207, loss is 1.0396069288253784\n",
            "epoch: 1 step: 3208, loss is 0.7705920934677124\n",
            "epoch: 1 step: 3209, loss is 1.0852841138839722\n",
            "epoch: 1 step: 3210, loss is 0.9141600131988525\n",
            "epoch: 1 step: 3211, loss is 0.9700435996055603\n",
            "epoch: 1 step: 3212, loss is 0.8827639222145081\n",
            "epoch: 1 step: 3213, loss is 1.0238178968429565\n",
            "epoch: 1 step: 3214, loss is 0.7829290628433228\n",
            "epoch: 1 step: 3215, loss is 1.0493377447128296\n",
            "epoch: 1 step: 3216, loss is 0.8098862767219543\n",
            "epoch: 1 step: 3217, loss is 0.8657032251358032\n",
            "epoch: 1 step: 3218, loss is 0.6789552569389343\n",
            "epoch: 1 step: 3219, loss is 0.8410894870758057\n",
            "epoch: 1 step: 3220, loss is 0.789846658706665\n",
            "epoch: 1 step: 3221, loss is 0.8801581263542175\n",
            "epoch: 1 step: 3222, loss is 0.9683454036712646\n",
            "epoch: 1 step: 3223, loss is 0.8895868062973022\n",
            "epoch: 1 step: 3224, loss is 0.813542366027832\n",
            "epoch: 1 step: 3225, loss is 0.9326757788658142\n",
            "epoch: 1 step: 3226, loss is 0.9227662086486816\n",
            "epoch: 1 step: 3227, loss is 0.8853302597999573\n",
            "epoch: 1 step: 3228, loss is 0.7295863628387451\n",
            "epoch: 1 step: 3229, loss is 0.7660955190658569\n",
            "epoch: 1 step: 3230, loss is 0.8193891644477844\n",
            "epoch: 1 step: 3231, loss is 0.8877105116844177\n",
            "epoch: 1 step: 3232, loss is 0.9796196818351746\n",
            "epoch: 1 step: 3233, loss is 0.9326220750808716\n",
            "epoch: 1 step: 3234, loss is 0.858972430229187\n",
            "epoch: 1 step: 3235, loss is 0.8315762877464294\n",
            "epoch: 1 step: 3236, loss is 0.981955885887146\n",
            "epoch: 1 step: 3237, loss is 0.7757620215415955\n",
            "epoch: 1 step: 3238, loss is 0.9328063130378723\n",
            "epoch: 1 step: 3239, loss is 0.8382177948951721\n",
            "epoch: 1 step: 3240, loss is 0.8219653367996216\n",
            "epoch: 1 step: 3241, loss is 0.7077330350875854\n",
            "epoch: 1 step: 3242, loss is 0.8449603319168091\n",
            "epoch: 1 step: 3243, loss is 0.9695465564727783\n",
            "epoch: 1 step: 3244, loss is 0.7016485333442688\n",
            "epoch: 1 step: 3245, loss is 0.9052257537841797\n",
            "epoch: 1 step: 3246, loss is 0.8963033556938171\n",
            "epoch: 1 step: 3247, loss is 0.790310263633728\n",
            "epoch: 1 step: 3248, loss is 0.9114393591880798\n",
            "epoch: 1 step: 3249, loss is 0.9346050024032593\n",
            "epoch: 1 step: 3250, loss is 0.8137267827987671\n",
            "epoch: 1 step: 3251, loss is 1.0959111452102661\n",
            "epoch: 1 step: 3252, loss is 0.7887371778488159\n",
            "epoch: 1 step: 3253, loss is 0.9316608905792236\n",
            "epoch: 1 step: 3254, loss is 0.9000393152236938\n",
            "epoch: 1 step: 3255, loss is 0.9057247042655945\n",
            "epoch: 1 step: 3256, loss is 0.7900629043579102\n",
            "epoch: 1 step: 3257, loss is 0.9086087346076965\n",
            "epoch: 1 step: 3258, loss is 0.8983718752861023\n",
            "epoch: 1 step: 3259, loss is 0.8217957019805908\n",
            "epoch: 1 step: 3260, loss is 1.003421664237976\n",
            "epoch: 1 step: 3261, loss is 0.9419446587562561\n",
            "epoch: 1 step: 3262, loss is 0.8417283296585083\n",
            "epoch: 1 step: 3263, loss is 0.9462054967880249\n",
            "epoch: 1 step: 3264, loss is 0.8622748255729675\n",
            "epoch: 1 step: 3265, loss is 0.919073760509491\n",
            "epoch: 1 step: 3266, loss is 0.7908881902694702\n",
            "epoch: 1 step: 3267, loss is 0.9065327048301697\n",
            "epoch: 1 step: 3268, loss is 0.905590832233429\n",
            "epoch: 1 step: 3269, loss is 0.8987942934036255\n",
            "epoch: 1 step: 3270, loss is 0.7741633057594299\n",
            "epoch: 1 step: 3271, loss is 0.8977843523025513\n",
            "epoch: 1 step: 3272, loss is 0.7545314431190491\n",
            "epoch: 1 step: 3273, loss is 0.901577353477478\n",
            "epoch: 1 step: 3274, loss is 0.9703613519668579\n",
            "epoch: 1 step: 3275, loss is 0.8043458461761475\n",
            "epoch: 1 step: 3276, loss is 0.9110171794891357\n",
            "epoch: 1 step: 3277, loss is 0.8949740529060364\n",
            "epoch: 1 step: 3278, loss is 1.1697742938995361\n",
            "epoch: 1 step: 3279, loss is 0.8882813453674316\n",
            "epoch: 1 step: 3280, loss is 0.8707784414291382\n",
            "epoch: 1 step: 3281, loss is 0.8354171514511108\n",
            "epoch: 1 step: 3282, loss is 1.0161052942276\n",
            "epoch: 1 step: 3283, loss is 0.8672142028808594\n",
            "epoch: 1 step: 3284, loss is 0.99256831407547\n",
            "epoch: 1 step: 3285, loss is 0.7800901532173157\n",
            "epoch: 1 step: 3286, loss is 0.7841628193855286\n",
            "epoch: 1 step: 3287, loss is 0.7334061861038208\n",
            "epoch: 1 step: 3288, loss is 0.8710306882858276\n",
            "epoch: 1 step: 3289, loss is 0.986179769039154\n",
            "epoch: 1 step: 3290, loss is 0.8775042295455933\n",
            "epoch: 1 step: 3291, loss is 0.8722600936889648\n",
            "epoch: 1 step: 3292, loss is 0.8601418733596802\n",
            "epoch: 1 step: 3293, loss is 0.9512954950332642\n",
            "epoch: 1 step: 3294, loss is 1.039818525314331\n",
            "epoch: 1 step: 3295, loss is 0.8448022603988647\n",
            "epoch: 1 step: 3296, loss is 0.7092447876930237\n",
            "epoch: 1 step: 3297, loss is 0.87850022315979\n",
            "epoch: 1 step: 3298, loss is 0.858768105506897\n",
            "epoch: 1 step: 3299, loss is 0.8694307208061218\n",
            "epoch: 1 step: 3300, loss is 0.8513235449790955\n",
            "epoch: 1 step: 3301, loss is 0.7812551259994507\n",
            "epoch: 1 step: 3302, loss is 0.9449952244758606\n",
            "epoch: 1 step: 3303, loss is 1.0041699409484863\n",
            "epoch: 1 step: 3304, loss is 0.8889662623405457\n",
            "epoch: 1 step: 3305, loss is 0.9463707208633423\n",
            "epoch: 1 step: 3306, loss is 0.9761416912078857\n",
            "epoch: 1 step: 3307, loss is 0.8372414708137512\n",
            "epoch: 1 step: 3308, loss is 0.664159893989563\n",
            "epoch: 1 step: 3309, loss is 0.8055781126022339\n",
            "epoch: 1 step: 3310, loss is 0.8673012852668762\n",
            "epoch: 1 step: 3311, loss is 0.9731748104095459\n",
            "epoch: 1 step: 3312, loss is 0.8529457449913025\n",
            "epoch: 1 step: 3313, loss is 0.715594470500946\n",
            "epoch: 1 step: 3314, loss is 0.8144814968109131\n",
            "epoch: 1 step: 3315, loss is 0.8323633670806885\n",
            "epoch: 1 step: 3316, loss is 0.8675331473350525\n",
            "epoch: 1 step: 3317, loss is 0.8148302435874939\n",
            "epoch: 1 step: 3318, loss is 1.2038201093673706\n",
            "epoch: 1 step: 3319, loss is 0.8998329639434814\n",
            "epoch: 1 step: 3320, loss is 0.8965110778808594\n",
            "epoch: 1 step: 3321, loss is 0.9106745719909668\n",
            "epoch: 1 step: 3322, loss is 0.9013864994049072\n",
            "epoch: 1 step: 3323, loss is 0.9367905855178833\n",
            "epoch: 1 step: 3324, loss is 0.9253278970718384\n",
            "epoch: 1 step: 3325, loss is 0.7801618576049805\n",
            "epoch: 1 step: 3326, loss is 0.85777348279953\n",
            "epoch: 1 step: 3327, loss is 0.8814451098442078\n",
            "epoch: 1 step: 3328, loss is 0.9540706276893616\n",
            "epoch: 1 step: 3329, loss is 0.9265199303627014\n",
            "epoch: 1 step: 3330, loss is 0.9362322688102722\n",
            "epoch: 1 step: 3331, loss is 0.8764481544494629\n",
            "epoch: 1 step: 3332, loss is 0.8910919427871704\n",
            "epoch: 1 step: 3333, loss is 0.8616724610328674\n",
            "epoch: 1 step: 3334, loss is 0.9685355424880981\n",
            "epoch: 1 step: 3335, loss is 0.7572417259216309\n",
            "epoch: 1 step: 3336, loss is 0.7728641629219055\n",
            "epoch: 1 step: 3337, loss is 0.8201490044593811\n",
            "epoch: 1 step: 3338, loss is 0.9159460663795471\n",
            "epoch: 1 step: 3339, loss is 0.9626905918121338\n",
            "epoch: 1 step: 3340, loss is 0.8376336097717285\n",
            "epoch: 1 step: 3341, loss is 1.0299361944198608\n",
            "epoch: 1 step: 3342, loss is 0.8790100812911987\n",
            "epoch: 1 step: 3343, loss is 0.9676423072814941\n",
            "epoch: 1 step: 3344, loss is 0.7970293760299683\n",
            "epoch: 1 step: 3345, loss is 0.9452962875366211\n",
            "epoch: 1 step: 3346, loss is 0.8403000831604004\n",
            "epoch: 1 step: 3347, loss is 0.8959320187568665\n",
            "epoch: 1 step: 3348, loss is 0.9640179872512817\n",
            "epoch: 1 step: 3349, loss is 0.9951820969581604\n",
            "epoch: 1 step: 3350, loss is 0.8183291554450989\n",
            "epoch: 1 step: 3351, loss is 0.8964396715164185\n",
            "epoch: 1 step: 3352, loss is 0.8574209809303284\n",
            "epoch: 1 step: 3353, loss is 0.7530205249786377\n",
            "epoch: 1 step: 3354, loss is 0.8980929255485535\n",
            "epoch: 1 step: 3355, loss is 1.1643120050430298\n",
            "epoch: 1 step: 3356, loss is 0.9917214512825012\n",
            "epoch: 1 step: 3357, loss is 0.9793593287467957\n",
            "epoch: 1 step: 3358, loss is 0.7175160050392151\n",
            "epoch: 1 step: 3359, loss is 0.7951129078865051\n",
            "epoch: 1 step: 3360, loss is 1.0164059400558472\n",
            "epoch: 1 step: 3361, loss is 0.9208143949508667\n",
            "epoch: 1 step: 3362, loss is 0.8052086234092712\n",
            "epoch: 1 step: 3363, loss is 0.829171895980835\n",
            "epoch: 1 step: 3364, loss is 0.8020418882369995\n",
            "epoch: 1 step: 3365, loss is 0.9521699547767639\n",
            "epoch: 1 step: 3366, loss is 0.8497341871261597\n",
            "epoch: 1 step: 3367, loss is 0.9322507381439209\n",
            "epoch: 1 step: 3368, loss is 0.8129109144210815\n",
            "epoch: 1 step: 3369, loss is 0.7840901017189026\n",
            "epoch: 1 step: 3370, loss is 0.9230527281761169\n",
            "epoch: 1 step: 3371, loss is 0.9052345752716064\n",
            "epoch: 1 step: 3372, loss is 1.010451078414917\n",
            "epoch: 1 step: 3373, loss is 0.9278067946434021\n",
            "epoch: 1 step: 3374, loss is 0.8416649103164673\n",
            "epoch: 1 step: 3375, loss is 0.8667197823524475\n",
            "epoch: 1 step: 3376, loss is 0.7013915777206421\n",
            "epoch: 1 step: 3377, loss is 0.9823035001754761\n",
            "epoch: 1 step: 3378, loss is 0.8188393115997314\n",
            "epoch: 1 step: 3379, loss is 0.9197067618370056\n",
            "epoch: 1 step: 3380, loss is 0.9995432496070862\n",
            "epoch: 1 step: 3381, loss is 1.1123595237731934\n",
            "epoch: 1 step: 3382, loss is 0.9502013325691223\n",
            "epoch: 1 step: 3383, loss is 0.9765397310256958\n",
            "epoch: 1 step: 3384, loss is 0.8843705058097839\n",
            "epoch: 1 step: 3385, loss is 0.8873963952064514\n",
            "epoch: 1 step: 3386, loss is 1.003103256225586\n",
            "epoch: 1 step: 3387, loss is 0.7699514031410217\n",
            "epoch: 1 step: 3388, loss is 0.7714594602584839\n",
            "epoch: 1 step: 3389, loss is 0.7568930983543396\n",
            "epoch: 1 step: 3390, loss is 1.0305111408233643\n",
            "epoch: 1 step: 3391, loss is 0.8785218596458435\n",
            "epoch: 1 step: 3392, loss is 0.6886458992958069\n",
            "epoch: 1 step: 3393, loss is 0.9198764562606812\n",
            "epoch: 1 step: 3394, loss is 0.891597330570221\n",
            "epoch: 1 step: 3395, loss is 0.7777804732322693\n",
            "epoch: 1 step: 3396, loss is 1.023733139038086\n",
            "epoch: 1 step: 3397, loss is 1.0642739534378052\n",
            "epoch: 1 step: 3398, loss is 0.6936753392219543\n",
            "epoch: 1 step: 3399, loss is 1.0711981058120728\n",
            "epoch: 1 step: 3400, loss is 0.8124697208404541\n",
            "epoch: 1 step: 3401, loss is 0.7199504971504211\n",
            "epoch: 1 step: 3402, loss is 0.9740576148033142\n",
            "epoch: 1 step: 3403, loss is 0.7572363615036011\n",
            "epoch: 1 step: 3404, loss is 0.9759556651115417\n",
            "epoch: 1 step: 3405, loss is 1.0626012086868286\n",
            "epoch: 1 step: 3406, loss is 0.7681447267532349\n",
            "epoch: 1 step: 3407, loss is 0.9396930932998657\n",
            "epoch: 1 step: 3408, loss is 1.0947171449661255\n",
            "epoch: 1 step: 3409, loss is 1.02799654006958\n",
            "epoch: 1 step: 3410, loss is 0.7993419170379639\n",
            "epoch: 1 step: 3411, loss is 0.7425642609596252\n",
            "epoch: 1 step: 3412, loss is 0.7965743541717529\n",
            "epoch: 1 step: 3413, loss is 0.9524508714675903\n",
            "epoch: 1 step: 3414, loss is 0.8794264197349548\n",
            "epoch: 1 step: 3415, loss is 0.9572322368621826\n",
            "epoch: 1 step: 3416, loss is 0.807204008102417\n",
            "epoch: 1 step: 3417, loss is 0.8053277730941772\n",
            "epoch: 1 step: 3418, loss is 0.9178206324577332\n",
            "epoch: 1 step: 3419, loss is 0.9662188291549683\n",
            "epoch: 1 step: 3420, loss is 0.9369630813598633\n",
            "epoch: 1 step: 3421, loss is 0.8742102980613708\n",
            "epoch: 1 step: 3422, loss is 0.9529018402099609\n",
            "epoch: 1 step: 3423, loss is 0.9886220097541809\n",
            "epoch: 1 step: 3424, loss is 0.7890917062759399\n",
            "epoch: 1 step: 3425, loss is 1.1700671911239624\n",
            "epoch: 1 step: 3426, loss is 0.7448120713233948\n",
            "epoch: 1 step: 3427, loss is 0.805975615978241\n",
            "epoch: 1 step: 3428, loss is 0.9180527925491333\n",
            "epoch: 1 step: 3429, loss is 0.8338835835456848\n",
            "epoch: 1 step: 3430, loss is 0.9340218901634216\n",
            "epoch: 1 step: 3431, loss is 0.6959821581840515\n",
            "epoch: 1 step: 3432, loss is 0.751924455165863\n",
            "epoch: 1 step: 3433, loss is 0.9082912802696228\n",
            "epoch: 1 step: 3434, loss is 1.0081300735473633\n",
            "epoch: 1 step: 3435, loss is 0.8342795968055725\n",
            "epoch: 1 step: 3436, loss is 0.9610421657562256\n",
            "epoch: 1 step: 3437, loss is 0.9019961953163147\n",
            "epoch: 1 step: 3438, loss is 0.7637401223182678\n",
            "epoch: 1 step: 3439, loss is 0.8434776663780212\n",
            "epoch: 1 step: 3440, loss is 0.9033520817756653\n",
            "epoch: 1 step: 3441, loss is 0.9472889304161072\n",
            "epoch: 1 step: 3442, loss is 1.1142665147781372\n",
            "epoch: 1 step: 3443, loss is 0.8076522946357727\n",
            "epoch: 1 step: 3444, loss is 0.9271990656852722\n",
            "epoch: 1 step: 3445, loss is 0.898115873336792\n",
            "epoch: 1 step: 3446, loss is 0.8079202175140381\n",
            "epoch: 1 step: 3447, loss is 0.9795382022857666\n",
            "epoch: 1 step: 3448, loss is 0.8764981031417847\n",
            "epoch: 1 step: 3449, loss is 1.0154184103012085\n",
            "epoch: 1 step: 3450, loss is 1.0917421579360962\n",
            "epoch: 1 step: 3451, loss is 0.8597943782806396\n",
            "epoch: 1 step: 3452, loss is 0.9632778167724609\n",
            "epoch: 1 step: 3453, loss is 0.805968701839447\n",
            "epoch: 1 step: 3454, loss is 0.9628405570983887\n",
            "epoch: 1 step: 3455, loss is 0.7467881441116333\n",
            "epoch: 1 step: 3456, loss is 0.9737156629562378\n",
            "epoch: 1 step: 3457, loss is 0.849919319152832\n",
            "epoch: 1 step: 3458, loss is 0.8507052063941956\n",
            "epoch: 1 step: 3459, loss is 0.9676668047904968\n",
            "epoch: 1 step: 3460, loss is 0.8713930249214172\n",
            "epoch: 1 step: 3461, loss is 0.8281848430633545\n",
            "epoch: 1 step: 3462, loss is 0.9964118003845215\n",
            "epoch: 1 step: 3463, loss is 0.7441790699958801\n",
            "epoch: 1 step: 3464, loss is 0.8375638723373413\n",
            "epoch: 1 step: 3465, loss is 0.786077082157135\n",
            "epoch: 1 step: 3466, loss is 0.685712993144989\n",
            "epoch: 1 step: 3467, loss is 0.8135402202606201\n",
            "epoch: 1 step: 3468, loss is 0.9504625201225281\n",
            "epoch: 1 step: 3469, loss is 0.7399292588233948\n",
            "epoch: 1 step: 3470, loss is 0.96941077709198\n",
            "epoch: 1 step: 3471, loss is 0.8057724237442017\n",
            "epoch: 1 step: 3472, loss is 0.96737140417099\n",
            "epoch: 1 step: 3473, loss is 0.8161681294441223\n",
            "epoch: 1 step: 3474, loss is 0.873358428478241\n",
            "epoch: 1 step: 3475, loss is 0.7933890223503113\n",
            "epoch: 1 step: 3476, loss is 0.8453691601753235\n",
            "epoch: 1 step: 3477, loss is 0.7794146537780762\n",
            "epoch: 1 step: 3478, loss is 0.8500744104385376\n",
            "epoch: 1 step: 3479, loss is 1.0869053602218628\n",
            "epoch: 1 step: 3480, loss is 0.99494868516922\n",
            "epoch: 1 step: 3481, loss is 0.9566565155982971\n",
            "epoch: 1 step: 3482, loss is 0.8281978964805603\n",
            "epoch: 1 step: 3483, loss is 0.9346877932548523\n",
            "epoch: 1 step: 3484, loss is 1.1743826866149902\n",
            "epoch: 1 step: 3485, loss is 0.6446467041969299\n",
            "epoch: 1 step: 3486, loss is 0.7675491571426392\n",
            "epoch: 1 step: 3487, loss is 0.8723790645599365\n",
            "epoch: 1 step: 3488, loss is 0.7199397087097168\n",
            "epoch: 1 step: 3489, loss is 0.8630696535110474\n",
            "epoch: 1 step: 3490, loss is 0.7735428214073181\n",
            "epoch: 1 step: 3491, loss is 0.9551875591278076\n",
            "epoch: 1 step: 3492, loss is 0.9355092644691467\n",
            "epoch: 1 step: 3493, loss is 0.8098336458206177\n",
            "epoch: 1 step: 3494, loss is 0.8271750807762146\n",
            "epoch: 1 step: 3495, loss is 0.8396800756454468\n",
            "epoch: 1 step: 3496, loss is 0.6587068438529968\n",
            "epoch: 1 step: 3497, loss is 0.8261131048202515\n",
            "epoch: 1 step: 3498, loss is 0.9140530824661255\n",
            "epoch: 1 step: 3499, loss is 0.7320122122764587\n",
            "epoch: 1 step: 3500, loss is 0.8998274207115173\n",
            "epoch: 1 step: 3501, loss is 0.9325965642929077\n",
            "epoch: 1 step: 3502, loss is 0.7674128413200378\n",
            "epoch: 1 step: 3503, loss is 1.0336722135543823\n",
            "epoch: 1 step: 3504, loss is 0.9707309603691101\n",
            "epoch: 1 step: 3505, loss is 1.0692986249923706\n",
            "epoch: 1 step: 3506, loss is 0.6435205340385437\n",
            "epoch: 1 step: 3507, loss is 0.6868797540664673\n",
            "epoch: 1 step: 3508, loss is 0.9075489044189453\n",
            "epoch: 1 step: 3509, loss is 0.8428002595901489\n",
            "epoch: 1 step: 3510, loss is 0.8768947124481201\n",
            "epoch: 1 step: 3511, loss is 0.97449791431427\n",
            "epoch: 1 step: 3512, loss is 0.9700284004211426\n",
            "epoch: 1 step: 3513, loss is 0.9962490200996399\n",
            "epoch: 1 step: 3514, loss is 0.7796885967254639\n",
            "epoch: 1 step: 3515, loss is 0.876648485660553\n",
            "epoch: 1 step: 3516, loss is 0.7490362524986267\n",
            "epoch: 1 step: 3517, loss is 0.8773500323295593\n",
            "epoch: 1 step: 3518, loss is 0.8528650999069214\n",
            "epoch: 1 step: 3519, loss is 0.8182500600814819\n",
            "epoch: 1 step: 3520, loss is 0.7329719066619873\n",
            "epoch: 1 step: 3521, loss is 1.104156494140625\n",
            "epoch: 1 step: 3522, loss is 0.9708526134490967\n",
            "epoch: 1 step: 3523, loss is 0.9009122252464294\n",
            "epoch: 1 step: 3524, loss is 0.9219239354133606\n",
            "epoch: 1 step: 3525, loss is 0.8187684416770935\n",
            "epoch: 1 step: 3526, loss is 1.0155651569366455\n",
            "epoch: 1 step: 3527, loss is 0.9179545044898987\n",
            "epoch: 1 step: 3528, loss is 0.8275967240333557\n",
            "epoch: 1 step: 3529, loss is 0.9877162575721741\n",
            "epoch: 1 step: 3530, loss is 1.0116688013076782\n",
            "epoch: 1 step: 3531, loss is 0.8814573884010315\n",
            "epoch: 1 step: 3532, loss is 0.9335412383079529\n",
            "epoch: 1 step: 3533, loss is 0.8761002421379089\n",
            "epoch: 1 step: 3534, loss is 1.014590859413147\n",
            "epoch: 1 step: 3535, loss is 0.904114842414856\n",
            "epoch: 1 step: 3536, loss is 0.9046109318733215\n",
            "epoch: 1 step: 3537, loss is 0.9759705662727356\n",
            "epoch: 1 step: 3538, loss is 0.9115880727767944\n",
            "epoch: 1 step: 3539, loss is 1.076857328414917\n",
            "epoch: 1 step: 3540, loss is 0.8622751832008362\n",
            "epoch: 1 step: 3541, loss is 0.9365026354789734\n",
            "epoch: 1 step: 3542, loss is 0.9299181699752808\n",
            "epoch: 1 step: 3543, loss is 0.9814454317092896\n",
            "epoch: 1 step: 3544, loss is 0.6392176151275635\n",
            "epoch: 1 step: 3545, loss is 0.8363243341445923\n",
            "epoch: 1 step: 3546, loss is 0.7720673084259033\n",
            "epoch: 1 step: 3547, loss is 0.9671550393104553\n",
            "epoch: 1 step: 3548, loss is 0.8507392406463623\n",
            "epoch: 1 step: 3549, loss is 0.7878199815750122\n",
            "epoch: 1 step: 3550, loss is 0.8432074785232544\n",
            "epoch: 1 step: 3551, loss is 0.9237558841705322\n",
            "epoch: 1 step: 3552, loss is 1.0721626281738281\n",
            "epoch: 1 step: 3553, loss is 0.9793917536735535\n",
            "epoch: 1 step: 3554, loss is 0.8815160393714905\n",
            "epoch: 1 step: 3555, loss is 0.983819305896759\n",
            "epoch: 1 step: 3556, loss is 0.827742874622345\n",
            "epoch: 1 step: 3557, loss is 0.827959418296814\n",
            "epoch: 1 step: 3558, loss is 0.8528420329093933\n",
            "epoch: 1 step: 3559, loss is 0.7523463368415833\n",
            "epoch: 1 step: 3560, loss is 0.895747184753418\n",
            "epoch: 1 step: 3561, loss is 1.0584568977355957\n",
            "epoch: 1 step: 3562, loss is 0.850495457649231\n",
            "epoch: 1 step: 3563, loss is 0.8070487976074219\n",
            "epoch: 1 step: 3564, loss is 0.9675073623657227\n",
            "epoch: 1 step: 3565, loss is 1.0339009761810303\n",
            "epoch: 1 step: 3566, loss is 0.8113147616386414\n",
            "epoch: 1 step: 3567, loss is 0.927486002445221\n",
            "epoch: 1 step: 3568, loss is 0.9823928475379944\n",
            "epoch: 1 step: 3569, loss is 0.9506914615631104\n",
            "epoch: 1 step: 3570, loss is 0.8052693009376526\n",
            "epoch: 1 step: 3571, loss is 1.0362266302108765\n",
            "epoch: 1 step: 3572, loss is 0.822050929069519\n",
            "epoch: 1 step: 3573, loss is 0.876228928565979\n",
            "epoch: 1 step: 3574, loss is 0.9976192116737366\n",
            "epoch: 1 step: 3575, loss is 0.856590986251831\n",
            "epoch: 1 step: 3576, loss is 0.8949944972991943\n",
            "epoch: 1 step: 3577, loss is 0.9245507121086121\n",
            "epoch: 1 step: 3578, loss is 0.7946929931640625\n",
            "epoch: 1 step: 3579, loss is 0.7764315009117126\n",
            "epoch: 1 step: 3580, loss is 0.9050607085227966\n",
            "epoch: 1 step: 3581, loss is 0.9917281270027161\n",
            "epoch: 1 step: 3582, loss is 0.8384032249450684\n",
            "epoch: 1 step: 3583, loss is 0.9019907712936401\n",
            "epoch: 1 step: 3584, loss is 0.9889460802078247\n",
            "epoch: 1 step: 3585, loss is 0.7704142928123474\n",
            "epoch: 1 step: 3586, loss is 0.8449861407279968\n",
            "epoch: 1 step: 3587, loss is 0.9285445213317871\n",
            "epoch: 1 step: 3588, loss is 0.862544059753418\n",
            "epoch: 1 step: 3589, loss is 0.8954467177391052\n",
            "epoch: 1 step: 3590, loss is 0.9791795015335083\n",
            "epoch: 1 step: 3591, loss is 0.8377159237861633\n",
            "epoch: 1 step: 3592, loss is 0.8416295647621155\n",
            "epoch: 1 step: 3593, loss is 0.9457635879516602\n",
            "epoch: 1 step: 3594, loss is 0.8712635040283203\n",
            "epoch: 1 step: 3595, loss is 0.9047397971153259\n",
            "epoch: 1 step: 3596, loss is 0.9079262018203735\n",
            "epoch: 1 step: 3597, loss is 0.8618483543395996\n",
            "epoch: 1 step: 3598, loss is 0.9319570064544678\n",
            "epoch: 1 step: 3599, loss is 0.8618500828742981\n",
            "epoch: 1 step: 3600, loss is 0.8019928336143494\n",
            "epoch: 1 step: 3601, loss is 0.8040236830711365\n",
            "epoch: 1 step: 3602, loss is 1.0236948728561401\n",
            "epoch: 1 step: 3603, loss is 0.8094250559806824\n",
            "epoch: 1 step: 3604, loss is 0.8310797214508057\n",
            "epoch: 1 step: 3605, loss is 0.9839227199554443\n",
            "epoch: 1 step: 3606, loss is 0.860541582107544\n",
            "epoch: 1 step: 3607, loss is 0.9550499320030212\n",
            "epoch: 1 step: 3608, loss is 0.9050009250640869\n",
            "epoch: 1 step: 3609, loss is 0.9540443420410156\n",
            "epoch: 1 step: 3610, loss is 0.987185537815094\n",
            "epoch: 1 step: 3611, loss is 0.8365164399147034\n",
            "epoch: 1 step: 3612, loss is 0.7241995930671692\n",
            "epoch: 1 step: 3613, loss is 0.8915831446647644\n",
            "epoch: 1 step: 3614, loss is 0.9144922494888306\n",
            "epoch: 1 step: 3615, loss is 0.820249080657959\n",
            "epoch: 1 step: 3616, loss is 1.1062958240509033\n",
            "epoch: 1 step: 3617, loss is 0.9031141996383667\n",
            "epoch: 1 step: 3618, loss is 0.8808068633079529\n",
            "epoch: 1 step: 3619, loss is 0.9404222965240479\n",
            "epoch: 1 step: 3620, loss is 0.8495601415634155\n",
            "epoch: 1 step: 3621, loss is 0.8885565996170044\n",
            "epoch: 1 step: 3622, loss is 0.9003186225891113\n",
            "epoch: 1 step: 3623, loss is 0.8881543874740601\n",
            "epoch: 1 step: 3624, loss is 0.9516531229019165\n",
            "epoch: 1 step: 3625, loss is 0.8042072653770447\n",
            "epoch: 1 step: 3626, loss is 0.7299237847328186\n",
            "epoch: 1 step: 3627, loss is 0.9713417887687683\n",
            "epoch: 1 step: 3628, loss is 0.8701578378677368\n",
            "epoch: 1 step: 3629, loss is 0.8921775817871094\n",
            "epoch: 1 step: 3630, loss is 0.8333859443664551\n",
            "epoch: 1 step: 3631, loss is 0.9190071225166321\n",
            "epoch: 1 step: 3632, loss is 0.7296784520149231\n",
            "epoch: 1 step: 3633, loss is 0.9461849927902222\n",
            "epoch: 1 step: 3634, loss is 0.8538529872894287\n",
            "epoch: 1 step: 3635, loss is 1.0375547409057617\n",
            "epoch: 1 step: 3636, loss is 0.6647166609764099\n",
            "epoch: 1 step: 3637, loss is 0.9293007254600525\n",
            "epoch: 1 step: 3638, loss is 0.8112943768501282\n",
            "epoch: 1 step: 3639, loss is 0.9372937083244324\n",
            "epoch: 1 step: 3640, loss is 0.918498694896698\n",
            "epoch: 1 step: 3641, loss is 0.9600090980529785\n",
            "epoch: 1 step: 3642, loss is 1.0984078645706177\n",
            "epoch: 1 step: 3643, loss is 0.8298608660697937\n",
            "epoch: 1 step: 3644, loss is 0.8794524073600769\n",
            "epoch: 1 step: 3645, loss is 0.8395998477935791\n",
            "epoch: 1 step: 3646, loss is 0.8619031310081482\n",
            "epoch: 1 step: 3647, loss is 0.7765945196151733\n",
            "epoch: 1 step: 3648, loss is 0.9807055592536926\n",
            "epoch: 1 step: 3649, loss is 0.7136748433113098\n",
            "epoch: 1 step: 3650, loss is 1.0017824172973633\n",
            "epoch: 1 step: 3651, loss is 0.7548816800117493\n",
            "epoch: 1 step: 3652, loss is 0.9312024712562561\n",
            "epoch: 1 step: 3653, loss is 0.823922872543335\n",
            "epoch: 1 step: 3654, loss is 1.1597570180892944\n",
            "epoch: 1 step: 3655, loss is 0.876943051815033\n",
            "epoch: 1 step: 3656, loss is 0.9457263350486755\n",
            "epoch: 1 step: 3657, loss is 0.8203678131103516\n",
            "epoch: 1 step: 3658, loss is 1.0615164041519165\n",
            "epoch: 1 step: 3659, loss is 0.830207109451294\n",
            "epoch: 1 step: 3660, loss is 0.9561241269111633\n",
            "epoch: 1 step: 3661, loss is 0.8858941197395325\n",
            "epoch: 1 step: 3662, loss is 0.697717010974884\n",
            "epoch: 1 step: 3663, loss is 0.9900947213172913\n",
            "epoch: 1 step: 3664, loss is 0.9459980726242065\n",
            "epoch: 1 step: 3665, loss is 1.1451421976089478\n",
            "epoch: 1 step: 3666, loss is 0.8126726746559143\n",
            "epoch: 1 step: 3667, loss is 0.9240684509277344\n",
            "epoch: 1 step: 3668, loss is 1.038116693496704\n",
            "epoch: 1 step: 3669, loss is 0.8674495816230774\n",
            "epoch: 1 step: 3670, loss is 0.9765694737434387\n",
            "epoch: 1 step: 3671, loss is 0.9723830819129944\n",
            "epoch: 1 step: 3672, loss is 0.7873525619506836\n",
            "epoch: 1 step: 3673, loss is 0.8103489279747009\n",
            "epoch: 1 step: 3674, loss is 0.8787792325019836\n",
            "epoch: 1 step: 3675, loss is 0.652852475643158\n",
            "epoch: 1 step: 3676, loss is 1.0591663122177124\n",
            "epoch: 1 step: 3677, loss is 0.93628990650177\n",
            "epoch: 1 step: 3678, loss is 0.8751528263092041\n",
            "epoch: 1 step: 3679, loss is 0.934619128704071\n",
            "epoch: 1 step: 3680, loss is 0.7631268501281738\n",
            "epoch: 1 step: 3681, loss is 0.7841428518295288\n",
            "epoch: 1 step: 3682, loss is 0.9106943011283875\n",
            "epoch: 1 step: 3683, loss is 0.8654502034187317\n",
            "epoch: 1 step: 3684, loss is 0.9017384052276611\n",
            "epoch: 1 step: 3685, loss is 1.1089686155319214\n",
            "epoch: 1 step: 3686, loss is 0.8048162460327148\n",
            "epoch: 1 step: 3687, loss is 0.8810713291168213\n",
            "epoch: 1 step: 3688, loss is 0.911795437335968\n",
            "epoch: 1 step: 3689, loss is 0.7722267508506775\n",
            "epoch: 1 step: 3690, loss is 0.8738796710968018\n",
            "epoch: 1 step: 3691, loss is 0.8636220097541809\n",
            "epoch: 1 step: 3692, loss is 0.9356105327606201\n",
            "epoch: 1 step: 3693, loss is 0.7037844061851501\n",
            "epoch: 1 step: 3694, loss is 0.9270568490028381\n",
            "epoch: 1 step: 3695, loss is 0.9511354565620422\n",
            "epoch: 1 step: 3696, loss is 0.8056256771087646\n",
            "epoch: 1 step: 3697, loss is 0.7523319125175476\n",
            "epoch: 1 step: 3698, loss is 1.010934591293335\n",
            "epoch: 1 step: 3699, loss is 0.8291440010070801\n",
            "epoch: 1 step: 3700, loss is 0.923517107963562\n",
            "epoch: 1 step: 3701, loss is 0.7878777384757996\n",
            "epoch: 1 step: 3702, loss is 0.8418031334877014\n",
            "epoch: 1 step: 3703, loss is 0.8766207695007324\n",
            "epoch: 1 step: 3704, loss is 0.837257981300354\n",
            "epoch: 1 step: 3705, loss is 0.8458122611045837\n",
            "epoch: 1 step: 3706, loss is 0.8989285826683044\n",
            "epoch: 1 step: 3707, loss is 0.7514449954032898\n",
            "epoch: 1 step: 3708, loss is 0.8703245520591736\n",
            "epoch: 1 step: 3709, loss is 0.9879884123802185\n",
            "epoch: 1 step: 3710, loss is 1.0230791568756104\n",
            "epoch: 1 step: 3711, loss is 0.9596227407455444\n",
            "epoch: 1 step: 3712, loss is 0.7946150302886963\n",
            "epoch: 1 step: 3713, loss is 0.8799849152565002\n",
            "epoch: 1 step: 3714, loss is 0.7809194922447205\n",
            "epoch: 1 step: 3715, loss is 1.0081968307495117\n",
            "epoch: 1 step: 3716, loss is 0.7090337872505188\n",
            "epoch: 1 step: 3717, loss is 0.968920111656189\n",
            "epoch: 1 step: 3718, loss is 0.7647660374641418\n",
            "epoch: 1 step: 3719, loss is 0.7743750810623169\n",
            "epoch: 1 step: 3720, loss is 0.92584228515625\n",
            "epoch: 1 step: 3721, loss is 0.6481556296348572\n",
            "epoch: 1 step: 3722, loss is 0.9817847013473511\n",
            "epoch: 1 step: 3723, loss is 0.9634020328521729\n",
            "epoch: 1 step: 3724, loss is 0.9438149929046631\n",
            "epoch: 1 step: 3725, loss is 0.8012897968292236\n",
            "epoch: 1 step: 3726, loss is 0.924471378326416\n",
            "epoch: 1 step: 3727, loss is 0.87632817029953\n",
            "epoch: 1 step: 3728, loss is 0.9445706605911255\n",
            "epoch: 1 step: 3729, loss is 1.0664476156234741\n",
            "epoch: 1 step: 3730, loss is 0.8889071345329285\n",
            "epoch: 1 step: 3731, loss is 0.7878052592277527\n",
            "epoch: 1 step: 3732, loss is 0.8454142808914185\n",
            "epoch: 1 step: 3733, loss is 1.107362985610962\n",
            "epoch: 1 step: 3734, loss is 0.8336485028266907\n",
            "epoch: 1 step: 3735, loss is 0.8242615461349487\n",
            "epoch: 1 step: 3736, loss is 0.8062651753425598\n",
            "epoch: 1 step: 3737, loss is 0.802598774433136\n",
            "epoch: 1 step: 3738, loss is 0.9845126271247864\n",
            "epoch: 1 step: 3739, loss is 0.8046219348907471\n",
            "epoch: 1 step: 3740, loss is 1.0168532133102417\n",
            "epoch: 1 step: 3741, loss is 0.9121770262718201\n",
            "epoch: 1 step: 3742, loss is 0.9495248794555664\n",
            "epoch: 1 step: 3743, loss is 1.0298631191253662\n",
            "epoch: 1 step: 3744, loss is 0.9397040605545044\n",
            "epoch: 1 step: 3745, loss is 1.0057271718978882\n",
            "epoch: 1 step: 3746, loss is 0.8719282150268555\n",
            "epoch: 1 step: 3747, loss is 0.8611894845962524\n",
            "epoch: 1 step: 3748, loss is 0.8607470393180847\n",
            "epoch: 1 step: 3749, loss is 0.9803060293197632\n",
            "epoch: 1 step: 3750, loss is 0.9964218139648438\n",
            "epoch: 1 step: 3751, loss is 1.0098001956939697\n",
            "epoch: 1 step: 3752, loss is 0.9744060635566711\n",
            "epoch: 1 step: 3753, loss is 0.8383432030677795\n",
            "epoch: 1 step: 3754, loss is 0.7301689386367798\n",
            "epoch: 1 step: 3755, loss is 0.8896485567092896\n",
            "epoch: 1 step: 3756, loss is 0.9562927484512329\n",
            "epoch: 1 step: 3757, loss is 0.9062505960464478\n",
            "epoch: 1 step: 3758, loss is 0.7210384011268616\n",
            "epoch: 1 step: 3759, loss is 0.7266289591789246\n",
            "epoch: 1 step: 3760, loss is 1.0743046998977661\n",
            "epoch: 1 step: 3761, loss is 0.7206409573554993\n",
            "epoch: 1 step: 3762, loss is 0.8887616395950317\n",
            "epoch: 1 step: 3763, loss is 0.7920467853546143\n",
            "epoch: 1 step: 3764, loss is 0.8703648447990417\n",
            "epoch: 1 step: 3765, loss is 0.8995941281318665\n",
            "epoch: 1 step: 3766, loss is 0.822016716003418\n",
            "epoch: 1 step: 3767, loss is 1.0108553171157837\n",
            "epoch: 1 step: 3768, loss is 0.7464407086372375\n",
            "epoch: 1 step: 3769, loss is 0.8728212714195251\n",
            "epoch: 1 step: 3770, loss is 0.7548615336418152\n",
            "epoch: 1 step: 3771, loss is 0.8643003702163696\n",
            "epoch: 1 step: 3772, loss is 0.8611840605735779\n",
            "epoch: 1 step: 3773, loss is 0.7513781785964966\n",
            "epoch: 1 step: 3774, loss is 0.9991816282272339\n",
            "epoch: 1 step: 3775, loss is 1.0245968103408813\n",
            "epoch: 1 step: 3776, loss is 0.9914982318878174\n",
            "epoch: 1 step: 3777, loss is 1.1057451963424683\n",
            "epoch: 1 step: 3778, loss is 0.9687369465827942\n",
            "epoch: 1 step: 3779, loss is 0.9050174355506897\n",
            "epoch: 1 step: 3780, loss is 0.9403313398361206\n",
            "epoch: 1 step: 3781, loss is 0.9315738677978516\n",
            "epoch: 1 step: 3782, loss is 0.855151355266571\n",
            "epoch: 1 step: 3783, loss is 0.8849438428878784\n",
            "epoch: 1 step: 3784, loss is 0.8933079242706299\n",
            "epoch: 1 step: 3785, loss is 0.6877415776252747\n",
            "epoch: 1 step: 3786, loss is 0.8389073014259338\n",
            "epoch: 1 step: 3787, loss is 0.8394920825958252\n",
            "epoch: 1 step: 3788, loss is 0.6831156015396118\n",
            "epoch: 1 step: 3789, loss is 0.902147114276886\n",
            "epoch: 1 step: 3790, loss is 0.8983851075172424\n",
            "epoch: 1 step: 3791, loss is 0.9195860028266907\n",
            "epoch: 1 step: 3792, loss is 0.7745909094810486\n",
            "epoch: 1 step: 3793, loss is 0.8387550711631775\n",
            "epoch: 1 step: 3794, loss is 0.894167423248291\n",
            "epoch: 1 step: 3795, loss is 0.8789981007575989\n",
            "epoch: 1 step: 3796, loss is 0.7316778302192688\n",
            "epoch: 1 step: 3797, loss is 0.8152642250061035\n",
            "epoch: 1 step: 3798, loss is 0.904573380947113\n",
            "epoch: 1 step: 3799, loss is 1.0607675313949585\n",
            "epoch: 1 step: 3800, loss is 0.8200978636741638\n",
            "epoch: 1 step: 3801, loss is 0.895638108253479\n",
            "epoch: 1 step: 3802, loss is 0.8457155823707581\n",
            "epoch: 1 step: 3803, loss is 0.9428738951683044\n",
            "epoch: 1 step: 3804, loss is 0.9825538992881775\n",
            "epoch: 1 step: 3805, loss is 0.8872412443161011\n",
            "epoch: 1 step: 3806, loss is 0.8724930286407471\n",
            "epoch: 1 step: 3807, loss is 0.8876417875289917\n",
            "epoch: 1 step: 3808, loss is 0.7895367741584778\n",
            "epoch: 1 step: 3809, loss is 0.9856674671173096\n",
            "epoch: 1 step: 3810, loss is 0.8636449575424194\n",
            "epoch: 1 step: 3811, loss is 0.6947514414787292\n",
            "epoch: 1 step: 3812, loss is 0.776192843914032\n",
            "epoch: 1 step: 3813, loss is 0.6732427477836609\n",
            "epoch: 1 step: 3814, loss is 1.245548129081726\n",
            "epoch: 1 step: 3815, loss is 0.7447925209999084\n",
            "epoch: 1 step: 3816, loss is 1.0026955604553223\n",
            "epoch: 1 step: 3817, loss is 0.8421456813812256\n",
            "epoch: 1 step: 3818, loss is 0.9702730178833008\n",
            "epoch: 1 step: 3819, loss is 0.924566388130188\n",
            "epoch: 1 step: 3820, loss is 0.9562702775001526\n",
            "epoch: 1 step: 3821, loss is 0.9224051833152771\n",
            "epoch: 1 step: 3822, loss is 0.8712407350540161\n",
            "epoch: 1 step: 3823, loss is 0.8625147342681885\n",
            "epoch: 1 step: 3824, loss is 0.8006064891815186\n",
            "epoch: 1 step: 3825, loss is 0.9435655474662781\n",
            "epoch: 1 step: 3826, loss is 0.8336775302886963\n",
            "epoch: 1 step: 3827, loss is 0.8137727975845337\n",
            "epoch: 1 step: 3828, loss is 0.6638843417167664\n",
            "epoch: 1 step: 3829, loss is 0.7320608496665955\n",
            "epoch: 1 step: 3830, loss is 1.0863127708435059\n",
            "epoch: 1 step: 3831, loss is 0.734462320804596\n",
            "epoch: 1 step: 3832, loss is 0.9112274050712585\n",
            "epoch: 1 step: 3833, loss is 0.7188899517059326\n",
            "epoch: 1 step: 3834, loss is 0.8135213851928711\n",
            "epoch: 1 step: 3835, loss is 1.032043218612671\n",
            "epoch: 1 step: 3836, loss is 0.9356118440628052\n",
            "epoch: 1 step: 3837, loss is 0.9285695552825928\n",
            "epoch: 1 step: 3838, loss is 0.953082263469696\n",
            "epoch: 1 step: 3839, loss is 0.8472850322723389\n",
            "epoch: 1 step: 3840, loss is 0.7810109853744507\n",
            "epoch: 1 step: 3841, loss is 0.8026444911956787\n",
            "epoch: 1 step: 3842, loss is 0.9259249567985535\n",
            "epoch: 1 step: 3843, loss is 0.9271414279937744\n",
            "epoch: 1 step: 3844, loss is 0.7173858880996704\n",
            "epoch: 1 step: 3845, loss is 0.9815881848335266\n",
            "epoch: 1 step: 3846, loss is 0.861683189868927\n",
            "epoch: 1 step: 3847, loss is 0.9955666065216064\n",
            "epoch: 1 step: 3848, loss is 0.9705300331115723\n",
            "epoch: 1 step: 3849, loss is 0.9465882182121277\n",
            "epoch: 1 step: 3850, loss is 0.7839502692222595\n",
            "epoch: 1 step: 3851, loss is 0.953091561794281\n",
            "epoch: 1 step: 3852, loss is 0.9102873802185059\n",
            "epoch: 1 step: 3853, loss is 0.933525025844574\n",
            "epoch: 1 step: 3854, loss is 1.0003752708435059\n",
            "epoch: 1 step: 3855, loss is 0.7840301990509033\n",
            "epoch: 1 step: 3856, loss is 0.6475304365158081\n",
            "epoch: 1 step: 3857, loss is 0.9988164901733398\n",
            "epoch: 1 step: 3858, loss is 0.8664599061012268\n",
            "epoch: 1 step: 3859, loss is 1.0581191778182983\n",
            "epoch: 1 step: 3860, loss is 0.8054707646369934\n",
            "epoch: 1 step: 3861, loss is 1.0121548175811768\n",
            "epoch: 1 step: 3862, loss is 0.7780753970146179\n",
            "epoch: 1 step: 3863, loss is 0.9376686215400696\n",
            "epoch: 1 step: 3864, loss is 0.8888012766838074\n",
            "epoch: 1 step: 3865, loss is 0.8073344826698303\n",
            "epoch: 1 step: 3866, loss is 0.9012987613677979\n",
            "epoch: 1 step: 3867, loss is 1.1038038730621338\n",
            "epoch: 1 step: 3868, loss is 0.8934751749038696\n",
            "epoch: 1 step: 3869, loss is 0.8349156379699707\n",
            "epoch: 1 step: 3870, loss is 0.8128089904785156\n",
            "epoch: 1 step: 3871, loss is 0.9086979627609253\n",
            "epoch: 1 step: 3872, loss is 0.9701622128486633\n",
            "epoch: 1 step: 3873, loss is 0.7319581508636475\n",
            "epoch: 1 step: 3874, loss is 1.1244310140609741\n",
            "epoch: 1 step: 3875, loss is 0.9172825813293457\n",
            "epoch: 1 step: 3876, loss is 0.9713355898857117\n",
            "epoch: 1 step: 3877, loss is 0.9191375374794006\n",
            "epoch: 1 step: 3878, loss is 0.8330956101417542\n",
            "epoch: 1 step: 3879, loss is 0.8860998749732971\n",
            "epoch: 1 step: 3880, loss is 0.8582678437232971\n",
            "epoch: 1 step: 3881, loss is 0.8957846164703369\n",
            "epoch: 1 step: 3882, loss is 0.9410087466239929\n",
            "epoch: 1 step: 3883, loss is 0.7017754912376404\n",
            "epoch: 1 step: 3884, loss is 0.8953691720962524\n",
            "epoch: 1 step: 3885, loss is 0.9123029708862305\n",
            "epoch: 1 step: 3886, loss is 0.8689118027687073\n",
            "epoch: 1 step: 3887, loss is 0.8527526259422302\n",
            "epoch: 1 step: 3888, loss is 0.7167631387710571\n",
            "epoch: 1 step: 3889, loss is 0.9092994928359985\n",
            "epoch: 1 step: 3890, loss is 0.8046919107437134\n",
            "epoch: 1 step: 3891, loss is 0.8336743712425232\n",
            "epoch: 1 step: 3892, loss is 0.941586434841156\n",
            "epoch: 1 step: 3893, loss is 1.0117652416229248\n",
            "epoch: 1 step: 3894, loss is 0.9328274726867676\n",
            "epoch: 1 step: 3895, loss is 1.0398443937301636\n",
            "epoch: 1 step: 3896, loss is 0.8067058324813843\n",
            "epoch: 1 step: 3897, loss is 0.7265931367874146\n",
            "epoch: 1 step: 3898, loss is 0.8863778710365295\n",
            "epoch: 1 step: 3899, loss is 0.9062449932098389\n",
            "epoch: 1 step: 3900, loss is 0.8056551218032837\n",
            "epoch: 1 step: 3901, loss is 0.9557645320892334\n",
            "epoch: 1 step: 3902, loss is 1.0282231569290161\n",
            "epoch: 1 step: 3903, loss is 0.783604621887207\n",
            "epoch: 1 step: 3904, loss is 0.9359883069992065\n",
            "epoch: 1 step: 3905, loss is 0.7164103984832764\n",
            "epoch: 1 step: 3906, loss is 0.8464251756668091\n",
            "epoch: 1 step: 3907, loss is 0.9055343866348267\n",
            "epoch: 1 step: 3908, loss is 0.9010168313980103\n",
            "epoch: 1 step: 3909, loss is 1.0190458297729492\n",
            "epoch: 1 step: 3910, loss is 1.0949091911315918\n",
            "epoch: 1 step: 3911, loss is 0.8200594782829285\n",
            "epoch: 1 step: 3912, loss is 0.877575695514679\n",
            "epoch: 1 step: 3913, loss is 0.7621004581451416\n",
            "epoch: 1 step: 3914, loss is 0.8224767446517944\n",
            "epoch: 1 step: 3915, loss is 1.1545716524124146\n",
            "epoch: 1 step: 3916, loss is 0.9044227004051208\n",
            "epoch: 1 step: 3917, loss is 0.7587069272994995\n",
            "epoch: 1 step: 3918, loss is 1.0806889533996582\n",
            "epoch: 1 step: 3919, loss is 1.0935989618301392\n",
            "epoch: 1 step: 3920, loss is 0.8788816928863525\n",
            "epoch: 1 step: 3921, loss is 0.9631016254425049\n",
            "epoch: 1 step: 3922, loss is 0.7435757517814636\n",
            "epoch: 1 step: 3923, loss is 1.0254062414169312\n",
            "epoch: 1 step: 3924, loss is 0.9064534306526184\n",
            "epoch: 1 step: 3925, loss is 0.797809362411499\n",
            "epoch: 1 step: 3926, loss is 0.815981388092041\n",
            "epoch: 1 step: 3927, loss is 0.9399957656860352\n",
            "epoch: 1 step: 3928, loss is 1.0194371938705444\n",
            "epoch: 1 step: 3929, loss is 0.897149384021759\n",
            "epoch: 1 step: 3930, loss is 0.9748703837394714\n",
            "epoch: 1 step: 3931, loss is 0.7491047978401184\n",
            "epoch: 1 step: 3932, loss is 0.8630034923553467\n",
            "epoch: 1 step: 3933, loss is 0.9246507883071899\n",
            "epoch: 1 step: 3934, loss is 0.819657027721405\n",
            "epoch: 1 step: 3935, loss is 1.1566150188446045\n",
            "epoch: 1 step: 3936, loss is 0.9713029265403748\n",
            "epoch: 1 step: 3937, loss is 0.778203547000885\n",
            "epoch: 1 step: 3938, loss is 0.8612068295478821\n",
            "epoch: 1 step: 3939, loss is 0.8948190212249756\n",
            "epoch: 1 step: 3940, loss is 0.8378599882125854\n",
            "epoch: 1 step: 3941, loss is 0.8287705183029175\n",
            "epoch: 1 step: 3942, loss is 0.7995089292526245\n",
            "epoch: 1 step: 3943, loss is 0.9038146734237671\n",
            "epoch: 1 step: 3944, loss is 0.9028184413909912\n",
            "epoch: 1 step: 3945, loss is 0.9382784962654114\n",
            "epoch: 1 step: 3946, loss is 0.7719840407371521\n",
            "epoch: 1 step: 3947, loss is 0.9466257691383362\n",
            "epoch: 1 step: 3948, loss is 0.8447311520576477\n",
            "epoch: 1 step: 3949, loss is 0.816596508026123\n",
            "epoch: 1 step: 3950, loss is 0.9638254046440125\n",
            "epoch: 1 step: 3951, loss is 0.9303524494171143\n",
            "epoch: 1 step: 3952, loss is 0.7822997570037842\n",
            "epoch: 1 step: 3953, loss is 1.0498415231704712\n",
            "epoch: 1 step: 3954, loss is 0.8413882255554199\n",
            "epoch: 1 step: 3955, loss is 0.888233482837677\n",
            "epoch: 1 step: 3956, loss is 0.8320342898368835\n",
            "epoch: 1 step: 3957, loss is 0.7824840545654297\n",
            "epoch: 1 step: 3958, loss is 0.8686317801475525\n",
            "epoch: 1 step: 3959, loss is 0.7640069723129272\n",
            "epoch: 1 step: 3960, loss is 0.9128392338752747\n",
            "epoch: 1 step: 3961, loss is 1.0303455591201782\n",
            "epoch: 1 step: 3962, loss is 0.8755029439926147\n",
            "epoch: 1 step: 3963, loss is 0.799013614654541\n",
            "epoch: 1 step: 3964, loss is 1.0886807441711426\n",
            "epoch: 1 step: 3965, loss is 0.7565954923629761\n",
            "epoch: 1 step: 3966, loss is 0.8716276288032532\n",
            "epoch: 1 step: 3967, loss is 0.8141778707504272\n",
            "epoch: 1 step: 3968, loss is 0.9244572520256042\n",
            "epoch: 1 step: 3969, loss is 0.8792238235473633\n",
            "epoch: 1 step: 3970, loss is 1.0307307243347168\n",
            "epoch: 1 step: 3971, loss is 0.8979222774505615\n",
            "epoch: 1 step: 3972, loss is 0.7420830130577087\n",
            "epoch: 1 step: 3973, loss is 0.9740744829177856\n",
            "epoch: 1 step: 3974, loss is 0.871569812297821\n",
            "epoch: 1 step: 3975, loss is 0.7656973600387573\n",
            "epoch: 1 step: 3976, loss is 0.8940646052360535\n",
            "epoch: 1 step: 3977, loss is 0.7654246091842651\n",
            "epoch: 1 step: 3978, loss is 1.0270060300827026\n",
            "epoch: 1 step: 3979, loss is 0.9043911695480347\n",
            "epoch: 1 step: 3980, loss is 0.8874164819717407\n",
            "epoch: 1 step: 3981, loss is 0.9634957313537598\n",
            "epoch: 1 step: 3982, loss is 0.8642263412475586\n",
            "epoch: 1 step: 3983, loss is 0.9573659300804138\n",
            "epoch: 1 step: 3984, loss is 0.9664475917816162\n",
            "epoch: 1 step: 3985, loss is 1.0096327066421509\n",
            "epoch: 1 step: 3986, loss is 0.9504402875900269\n",
            "epoch: 1 step: 3987, loss is 0.8243380784988403\n",
            "epoch: 1 step: 3988, loss is 0.8144409656524658\n",
            "epoch: 1 step: 3989, loss is 0.8418000936508179\n",
            "epoch: 1 step: 3990, loss is 0.9810067415237427\n",
            "epoch: 1 step: 3991, loss is 0.903996467590332\n",
            "epoch: 1 step: 3992, loss is 0.811264157295227\n",
            "epoch: 1 step: 3993, loss is 0.7675131559371948\n",
            "epoch: 1 step: 3994, loss is 0.8860057592391968\n",
            "epoch: 1 step: 3995, loss is 0.8944645524024963\n",
            "epoch: 1 step: 3996, loss is 0.8777995705604553\n",
            "epoch: 1 step: 3997, loss is 0.9600297808647156\n",
            "epoch: 1 step: 3998, loss is 0.6871920824050903\n",
            "epoch: 1 step: 3999, loss is 0.8671286106109619\n",
            "epoch: 1 step: 4000, loss is 0.8471099734306335\n",
            "epoch: 1 step: 4001, loss is 1.0648319721221924\n",
            "epoch: 1 step: 4002, loss is 0.7852025032043457\n",
            "epoch: 1 step: 4003, loss is 0.8324204087257385\n",
            "epoch: 1 step: 4004, loss is 0.9235513806343079\n",
            "epoch: 1 step: 4005, loss is 0.8780196309089661\n",
            "epoch: 1 step: 4006, loss is 0.83395916223526\n",
            "epoch: 1 step: 4007, loss is 0.8294420838356018\n",
            "epoch: 1 step: 4008, loss is 0.8161599636077881\n",
            "epoch: 1 step: 4009, loss is 0.8782680034637451\n",
            "epoch: 1 step: 4010, loss is 0.8669876456260681\n",
            "epoch: 1 step: 4011, loss is 0.8957855105400085\n",
            "epoch: 1 step: 4012, loss is 0.8286522030830383\n",
            "epoch: 1 step: 4013, loss is 0.8093278408050537\n",
            "epoch: 1 step: 4014, loss is 0.7852344512939453\n",
            "epoch: 1 step: 4015, loss is 0.8435078859329224\n",
            "epoch: 1 step: 4016, loss is 1.1266534328460693\n",
            "epoch: 1 step: 4017, loss is 0.8645372986793518\n",
            "epoch: 1 step: 4018, loss is 0.9368176460266113\n",
            "epoch: 1 step: 4019, loss is 0.8404402732849121\n",
            "epoch: 1 step: 4020, loss is 0.7003374099731445\n",
            "epoch: 1 step: 4021, loss is 1.0109107494354248\n",
            "epoch: 1 step: 4022, loss is 0.8630478382110596\n",
            "epoch: 1 step: 4023, loss is 0.8991406559944153\n",
            "epoch: 1 step: 4024, loss is 1.0443713665008545\n",
            "epoch: 1 step: 4025, loss is 0.8985907435417175\n",
            "epoch: 1 step: 4026, loss is 0.7180891633033752\n",
            "epoch: 1 step: 4027, loss is 0.9180824756622314\n",
            "epoch: 1 step: 4028, loss is 1.0122971534729004\n",
            "epoch: 1 step: 4029, loss is 0.8480367660522461\n",
            "epoch: 1 step: 4030, loss is 0.8583644032478333\n",
            "epoch: 1 step: 4031, loss is 0.7452700138092041\n",
            "epoch: 1 step: 4032, loss is 1.0842430591583252\n",
            "epoch: 1 step: 4033, loss is 1.0305315256118774\n",
            "epoch: 1 step: 4034, loss is 0.8573894500732422\n",
            "epoch: 1 step: 4035, loss is 0.9633018970489502\n",
            "epoch: 1 step: 4036, loss is 0.8254798650741577\n",
            "epoch: 1 step: 4037, loss is 0.751341700553894\n",
            "epoch: 1 step: 4038, loss is 1.0215116739273071\n",
            "epoch: 1 step: 4039, loss is 0.8293066024780273\n",
            "epoch: 1 step: 4040, loss is 0.8670015931129456\n",
            "epoch: 1 step: 4041, loss is 0.754019021987915\n",
            "epoch: 1 step: 4042, loss is 0.6792126893997192\n",
            "epoch: 1 step: 4043, loss is 1.0030359029769897\n",
            "epoch: 1 step: 4044, loss is 0.8900784850120544\n",
            "epoch: 1 step: 4045, loss is 0.9183430671691895\n",
            "epoch: 1 step: 4046, loss is 1.0752509832382202\n",
            "epoch: 1 step: 4047, loss is 0.8746830224990845\n",
            "epoch: 1 step: 4048, loss is 0.8447160720825195\n",
            "epoch: 1 step: 4049, loss is 0.9792237281799316\n",
            "epoch: 1 step: 4050, loss is 0.9549064040184021\n",
            "epoch: 1 step: 4051, loss is 1.069172739982605\n",
            "epoch: 1 step: 4052, loss is 0.8738593459129333\n",
            "epoch: 1 step: 4053, loss is 0.9016306400299072\n",
            "epoch: 1 step: 4054, loss is 0.8049829602241516\n",
            "epoch: 1 step: 4055, loss is 0.9039385318756104\n",
            "epoch: 1 step: 4056, loss is 0.8049758076667786\n",
            "epoch: 1 step: 4057, loss is 1.0037670135498047\n",
            "epoch: 1 step: 4058, loss is 0.9532437324523926\n",
            "epoch: 1 step: 4059, loss is 0.8545970916748047\n",
            "epoch: 1 step: 4060, loss is 0.8824277520179749\n",
            "epoch: 1 step: 4061, loss is 0.977532148361206\n",
            "epoch: 1 step: 4062, loss is 0.9259472489356995\n",
            "epoch: 1 step: 4063, loss is 1.1450611352920532\n",
            "epoch: 1 step: 4064, loss is 0.7968652844429016\n",
            "epoch: 1 step: 4065, loss is 0.9570455551147461\n",
            "epoch: 1 step: 4066, loss is 0.6463735699653625\n",
            "epoch: 1 step: 4067, loss is 1.02649986743927\n",
            "epoch: 1 step: 4068, loss is 0.8573606610298157\n",
            "epoch: 1 step: 4069, loss is 0.8948053121566772\n",
            "epoch: 1 step: 4070, loss is 0.9829100370407104\n",
            "epoch: 1 step: 4071, loss is 0.7821902632713318\n",
            "epoch: 1 step: 4072, loss is 0.9793775677680969\n",
            "epoch: 1 step: 4073, loss is 0.8285958766937256\n",
            "epoch: 1 step: 4074, loss is 1.027084231376648\n",
            "epoch: 1 step: 4075, loss is 0.8704947829246521\n",
            "epoch: 1 step: 4076, loss is 0.8759366869926453\n",
            "epoch: 1 step: 4077, loss is 0.8689969182014465\n",
            "epoch: 1 step: 4078, loss is 1.0230728387832642\n",
            "epoch: 1 step: 4079, loss is 0.7570035457611084\n",
            "epoch: 1 step: 4080, loss is 0.9129934906959534\n",
            "epoch: 1 step: 4081, loss is 1.0140347480773926\n",
            "epoch: 1 step: 4082, loss is 0.8644449710845947\n",
            "epoch: 1 step: 4083, loss is 0.7809714078903198\n",
            "epoch: 1 step: 4084, loss is 0.7756916284561157\n",
            "epoch: 1 step: 4085, loss is 1.0660239458084106\n",
            "epoch: 1 step: 4086, loss is 0.9595492482185364\n",
            "epoch: 1 step: 4087, loss is 0.7943974137306213\n",
            "epoch: 1 step: 4088, loss is 0.7261377573013306\n",
            "epoch: 1 step: 4089, loss is 0.8072998523712158\n",
            "epoch: 1 step: 4090, loss is 0.9605099558830261\n",
            "epoch: 1 step: 4091, loss is 0.9761735796928406\n",
            "epoch: 1 step: 4092, loss is 0.7092287540435791\n",
            "epoch: 1 step: 4093, loss is 1.0807886123657227\n",
            "epoch: 1 step: 4094, loss is 0.910588264465332\n",
            "epoch: 1 step: 4095, loss is 1.063910722732544\n",
            "epoch: 1 step: 4096, loss is 0.7391954064369202\n",
            "epoch: 1 step: 4097, loss is 0.9987167716026306\n",
            "epoch: 1 step: 4098, loss is 0.7475666403770447\n",
            "epoch: 1 step: 4099, loss is 0.9356259107589722\n",
            "epoch: 1 step: 4100, loss is 1.013351321220398\n",
            "epoch: 1 step: 4101, loss is 0.8622455596923828\n",
            "epoch: 1 step: 4102, loss is 0.7052513957023621\n",
            "epoch: 1 step: 4103, loss is 0.732621431350708\n",
            "epoch: 1 step: 4104, loss is 0.7973059415817261\n",
            "epoch: 1 step: 4105, loss is 0.9678016304969788\n",
            "epoch: 1 step: 4106, loss is 0.8710561394691467\n",
            "epoch: 1 step: 4107, loss is 0.7402014136314392\n",
            "epoch: 1 step: 4108, loss is 0.842401921749115\n",
            "epoch: 1 step: 4109, loss is 1.0132474899291992\n",
            "epoch: 1 step: 4110, loss is 0.9347500205039978\n",
            "epoch: 1 step: 4111, loss is 0.8430122137069702\n",
            "epoch: 1 step: 4112, loss is 0.970064640045166\n",
            "epoch: 1 step: 4113, loss is 1.0237703323364258\n",
            "epoch: 1 step: 4114, loss is 0.9433534145355225\n",
            "epoch: 1 step: 4115, loss is 0.833951473236084\n",
            "epoch: 1 step: 4116, loss is 0.7383922338485718\n",
            "epoch: 1 step: 4117, loss is 0.815984308719635\n",
            "epoch: 1 step: 4118, loss is 0.9008514285087585\n",
            "epoch: 1 step: 4119, loss is 0.7395125031471252\n",
            "epoch: 1 step: 4120, loss is 0.9233623147010803\n",
            "epoch: 1 step: 4121, loss is 0.8361642360687256\n",
            "epoch: 1 step: 4122, loss is 0.7119697332382202\n",
            "epoch: 1 step: 4123, loss is 0.8307599425315857\n",
            "epoch: 1 step: 4124, loss is 0.7897905707359314\n",
            "epoch: 1 step: 4125, loss is 0.8984912633895874\n",
            "epoch: 1 step: 4126, loss is 1.1973050832748413\n",
            "epoch: 1 step: 4127, loss is 1.0431455373764038\n",
            "epoch: 1 step: 4128, loss is 0.6591489315032959\n",
            "epoch: 1 step: 4129, loss is 0.8091961145401001\n",
            "epoch: 1 step: 4130, loss is 0.9268690347671509\n",
            "epoch: 1 step: 4131, loss is 1.045695185661316\n",
            "epoch: 1 step: 4132, loss is 0.7828657627105713\n",
            "epoch: 1 step: 4133, loss is 0.7157142162322998\n",
            "epoch: 1 step: 4134, loss is 0.7701255679130554\n",
            "epoch: 1 step: 4135, loss is 0.8837215304374695\n",
            "epoch: 1 step: 4136, loss is 0.7961481213569641\n",
            "epoch: 1 step: 4137, loss is 1.0423120260238647\n",
            "epoch: 1 step: 4138, loss is 0.8365359306335449\n",
            "epoch: 1 step: 4139, loss is 0.6908555626869202\n",
            "epoch: 1 step: 4140, loss is 0.8466626405715942\n",
            "epoch: 1 step: 4141, loss is 0.9337627291679382\n",
            "epoch: 1 step: 4142, loss is 0.8688608407974243\n",
            "epoch: 1 step: 4143, loss is 0.7343851327896118\n",
            "epoch: 1 step: 4144, loss is 0.9639098644256592\n",
            "epoch: 1 step: 4145, loss is 0.8273020386695862\n",
            "epoch: 1 step: 4146, loss is 0.878202497959137\n",
            "epoch: 1 step: 4147, loss is 0.8491993546485901\n",
            "epoch: 1 step: 4148, loss is 0.783098578453064\n",
            "epoch: 1 step: 4149, loss is 0.833071768283844\n",
            "epoch: 1 step: 4150, loss is 0.9938495755195618\n",
            "epoch: 1 step: 4151, loss is 0.8368719220161438\n",
            "epoch: 1 step: 4152, loss is 0.816250205039978\n",
            "epoch: 1 step: 4153, loss is 0.7673025131225586\n",
            "epoch: 1 step: 4154, loss is 0.8339633941650391\n",
            "epoch: 1 step: 4155, loss is 1.1118228435516357\n",
            "epoch: 1 step: 4156, loss is 0.9201246500015259\n",
            "epoch: 1 step: 4157, loss is 0.7562392354011536\n",
            "epoch: 1 step: 4158, loss is 0.9282839894294739\n",
            "epoch: 1 step: 4159, loss is 0.9083864092826843\n",
            "epoch: 1 step: 4160, loss is 0.9150970578193665\n",
            "epoch: 1 step: 4161, loss is 1.2958625555038452\n",
            "epoch: 1 step: 4162, loss is 0.9720647931098938\n",
            "epoch: 1 step: 4163, loss is 0.780152440071106\n",
            "epoch: 1 step: 4164, loss is 0.7702077627182007\n",
            "epoch: 1 step: 4165, loss is 1.0139150619506836\n",
            "epoch: 1 step: 4166, loss is 0.8974187970161438\n",
            "epoch: 1 step: 4167, loss is 0.8881556987762451\n",
            "epoch: 1 step: 4168, loss is 0.8087815046310425\n",
            "epoch: 1 step: 4169, loss is 0.7468639016151428\n",
            "epoch: 1 step: 4170, loss is 1.0732406377792358\n",
            "epoch: 1 step: 4171, loss is 0.7783433198928833\n",
            "epoch: 1 step: 4172, loss is 0.8193783164024353\n",
            "epoch: 1 step: 4173, loss is 0.7138978242874146\n",
            "epoch: 1 step: 4174, loss is 1.000576376914978\n",
            "epoch: 1 step: 4175, loss is 0.7064813375473022\n",
            "epoch: 1 step: 4176, loss is 0.9176061153411865\n",
            "epoch: 1 step: 4177, loss is 0.7619170546531677\n",
            "epoch: 1 step: 4178, loss is 0.8690566420555115\n",
            "epoch: 1 step: 4179, loss is 0.8196611404418945\n",
            "epoch: 1 step: 4180, loss is 0.9009312987327576\n",
            "epoch: 1 step: 4181, loss is 0.8924872875213623\n",
            "epoch: 1 step: 4182, loss is 0.6874999403953552\n",
            "epoch: 1 step: 4183, loss is 0.7392118573188782\n",
            "epoch: 1 step: 4184, loss is 0.8426656723022461\n",
            "epoch: 1 step: 4185, loss is 1.0400699377059937\n",
            "epoch: 1 step: 4186, loss is 0.7592017650604248\n",
            "epoch: 1 step: 4187, loss is 0.9874991178512573\n",
            "epoch: 1 step: 4188, loss is 1.0799498558044434\n",
            "epoch: 1 step: 4189, loss is 0.8973817825317383\n",
            "epoch: 1 step: 4190, loss is 0.9290341138839722\n",
            "epoch: 1 step: 4191, loss is 0.8817642331123352\n",
            "epoch: 1 step: 4192, loss is 0.7761660814285278\n",
            "epoch: 1 step: 4193, loss is 0.714347243309021\n",
            "epoch: 1 step: 4194, loss is 0.9075357913970947\n",
            "epoch: 1 step: 4195, loss is 0.9201492667198181\n",
            "epoch: 1 step: 4196, loss is 0.7910943031311035\n",
            "epoch: 1 step: 4197, loss is 0.8208290934562683\n",
            "epoch: 1 step: 4198, loss is 0.8158660531044006\n",
            "epoch: 1 step: 4199, loss is 0.9519899487495422\n",
            "epoch: 1 step: 4200, loss is 0.8615239262580872\n",
            "epoch: 1 step: 4201, loss is 1.0129159688949585\n",
            "epoch: 1 step: 4202, loss is 0.973197877407074\n",
            "epoch: 1 step: 4203, loss is 0.7812913656234741\n",
            "epoch: 1 step: 4204, loss is 0.9468749761581421\n",
            "epoch: 1 step: 4205, loss is 0.9343474507331848\n",
            "epoch: 1 step: 4206, loss is 0.9410362839698792\n",
            "epoch: 1 step: 4207, loss is 1.0028047561645508\n",
            "epoch: 1 step: 4208, loss is 0.944415807723999\n",
            "epoch: 1 step: 4209, loss is 1.0842218399047852\n",
            "epoch: 1 step: 4210, loss is 0.8968926072120667\n",
            "epoch: 1 step: 4211, loss is 0.896540641784668\n",
            "epoch: 1 step: 4212, loss is 0.9275990724563599\n",
            "epoch: 1 step: 4213, loss is 0.7619109153747559\n",
            "epoch: 1 step: 4214, loss is 0.9148267507553101\n",
            "epoch: 1 step: 4215, loss is 0.8050136566162109\n",
            "epoch: 1 step: 4216, loss is 0.8657522201538086\n",
            "epoch: 1 step: 4217, loss is 0.8385900259017944\n",
            "epoch: 1 step: 4218, loss is 0.9093844890594482\n",
            "epoch: 1 step: 4219, loss is 0.9066515564918518\n",
            "epoch: 1 step: 4220, loss is 0.8005816340446472\n",
            "epoch: 1 step: 4221, loss is 0.9684216380119324\n",
            "epoch: 1 step: 4222, loss is 0.8544747829437256\n",
            "epoch: 1 step: 4223, loss is 0.9068579077720642\n",
            "epoch: 1 step: 4224, loss is 0.7819713950157166\n",
            "epoch: 1 step: 4225, loss is 0.9946299195289612\n",
            "epoch: 1 step: 4226, loss is 0.8543098568916321\n",
            "epoch: 1 step: 4227, loss is 0.891926109790802\n",
            "epoch: 1 step: 4228, loss is 0.8289321064949036\n",
            "epoch: 1 step: 4229, loss is 0.8979160785675049\n",
            "epoch: 1 step: 4230, loss is 0.9449391961097717\n",
            "epoch: 1 step: 4231, loss is 0.8724666237831116\n",
            "epoch: 1 step: 4232, loss is 0.8897405862808228\n",
            "epoch: 1 step: 4233, loss is 0.9137133359909058\n",
            "epoch: 1 step: 4234, loss is 0.8595464825630188\n",
            "epoch: 1 step: 4235, loss is 0.7567711472511292\n",
            "epoch: 1 step: 4236, loss is 0.8740130662918091\n",
            "epoch: 1 step: 4237, loss is 0.7604210376739502\n",
            "epoch: 1 step: 4238, loss is 0.7791350483894348\n",
            "epoch: 1 step: 4239, loss is 0.8294076919555664\n",
            "epoch: 1 step: 4240, loss is 0.8737144470214844\n",
            "epoch: 1 step: 4241, loss is 0.9800683259963989\n",
            "epoch: 1 step: 4242, loss is 0.9742186069488525\n",
            "epoch: 1 step: 4243, loss is 0.8955466747283936\n",
            "epoch: 1 step: 4244, loss is 0.8494080305099487\n",
            "epoch: 1 step: 4245, loss is 0.9588637948036194\n",
            "epoch: 1 step: 4246, loss is 0.9423841834068298\n",
            "epoch: 1 step: 4247, loss is 0.8003329634666443\n",
            "epoch: 1 step: 4248, loss is 0.9744815826416016\n",
            "epoch: 1 step: 4249, loss is 0.9968171715736389\n",
            "epoch: 1 step: 4250, loss is 1.026578426361084\n",
            "epoch: 1 step: 4251, loss is 0.9241030216217041\n",
            "epoch: 1 step: 4252, loss is 0.8399418592453003\n",
            "epoch: 1 step: 4253, loss is 0.9817503094673157\n",
            "epoch: 1 step: 4254, loss is 0.84518963098526\n",
            "epoch: 1 step: 4255, loss is 1.039193868637085\n",
            "epoch: 1 step: 4256, loss is 0.8057363629341125\n",
            "epoch: 1 step: 4257, loss is 0.9092103242874146\n",
            "epoch: 1 step: 4258, loss is 0.9584240317344666\n",
            "epoch: 1 step: 4259, loss is 0.9499606490135193\n",
            "epoch: 1 step: 4260, loss is 1.1254340410232544\n",
            "epoch: 1 step: 4261, loss is 0.9189251661300659\n",
            "epoch: 1 step: 4262, loss is 1.0637311935424805\n",
            "epoch: 1 step: 4263, loss is 0.8838375210762024\n",
            "epoch: 1 step: 4264, loss is 0.8345836400985718\n",
            "epoch: 1 step: 4265, loss is 0.9160822629928589\n",
            "epoch: 1 step: 4266, loss is 0.6788586378097534\n",
            "epoch: 1 step: 4267, loss is 0.8535017371177673\n",
            "epoch: 1 step: 4268, loss is 0.8597638607025146\n",
            "epoch: 1 step: 4269, loss is 0.9188177585601807\n",
            "epoch: 1 step: 4270, loss is 0.8403275609016418\n",
            "epoch: 1 step: 4271, loss is 0.777033805847168\n",
            "epoch: 1 step: 4272, loss is 0.8944872617721558\n",
            "epoch: 1 step: 4273, loss is 0.9235771298408508\n",
            "epoch: 1 step: 4274, loss is 0.9019246697425842\n",
            "epoch: 1 step: 4275, loss is 0.8577478528022766\n",
            "epoch: 1 step: 4276, loss is 0.9568772315979004\n",
            "epoch: 1 step: 4277, loss is 0.7000985145568848\n",
            "epoch: 1 step: 4278, loss is 0.9094110131263733\n",
            "epoch: 1 step: 4279, loss is 0.8547302484512329\n",
            "epoch: 1 step: 4280, loss is 0.8498063087463379\n",
            "epoch: 1 step: 4281, loss is 0.8995795845985413\n",
            "epoch: 1 step: 4282, loss is 0.9832747578620911\n",
            "epoch: 1 step: 4283, loss is 0.7698563933372498\n",
            "epoch: 1 step: 4284, loss is 0.8674780130386353\n",
            "epoch: 1 step: 4285, loss is 1.0764003992080688\n",
            "epoch: 1 step: 4286, loss is 0.9513921141624451\n",
            "epoch: 1 step: 4287, loss is 0.8361266255378723\n",
            "epoch: 1 step: 4288, loss is 0.7636137008666992\n",
            "epoch: 1 step: 4289, loss is 0.8301934599876404\n",
            "epoch: 1 step: 4290, loss is 0.9752660393714905\n",
            "epoch: 1 step: 4291, loss is 0.8159776329994202\n",
            "epoch: 1 step: 4292, loss is 1.0802850723266602\n",
            "epoch: 1 step: 4293, loss is 0.9708473682403564\n",
            "epoch: 1 step: 4294, loss is 0.8931280970573425\n",
            "epoch: 1 step: 4295, loss is 0.9685913324356079\n",
            "epoch: 1 step: 4296, loss is 0.9714465141296387\n",
            "epoch: 1 step: 4297, loss is 0.9090253710746765\n",
            "epoch: 1 step: 4298, loss is 0.923043429851532\n",
            "epoch: 1 step: 4299, loss is 0.9005861282348633\n",
            "epoch: 1 step: 4300, loss is 0.8562409281730652\n",
            "epoch: 1 step: 4301, loss is 0.9031643867492676\n",
            "epoch: 1 step: 4302, loss is 0.7079699635505676\n",
            "epoch: 1 step: 4303, loss is 0.7596599459648132\n",
            "epoch: 1 step: 4304, loss is 0.7524762153625488\n",
            "epoch: 1 step: 4305, loss is 0.790702760219574\n",
            "epoch: 1 step: 4306, loss is 0.782741129398346\n",
            "epoch: 1 step: 4307, loss is 0.9959571957588196\n",
            "epoch: 1 step: 4308, loss is 1.0384984016418457\n",
            "epoch: 1 step: 4309, loss is 0.9637340307235718\n",
            "epoch: 1 step: 4310, loss is 0.8670206069946289\n",
            "epoch: 1 step: 4311, loss is 0.9394639730453491\n",
            "epoch: 1 step: 4312, loss is 0.940907895565033\n",
            "epoch: 1 step: 4313, loss is 0.862606406211853\n",
            "epoch: 1 step: 4314, loss is 0.6328045129776001\n",
            "epoch: 1 step: 4315, loss is 0.8351606130599976\n",
            "epoch: 1 step: 4316, loss is 1.0530009269714355\n",
            "epoch: 1 step: 4317, loss is 0.9390876889228821\n",
            "epoch: 1 step: 4318, loss is 0.9176816344261169\n",
            "epoch: 1 step: 4319, loss is 1.0281739234924316\n",
            "epoch: 1 step: 4320, loss is 0.898797869682312\n",
            "epoch: 1 step: 4321, loss is 0.7818948030471802\n",
            "epoch: 1 step: 4322, loss is 0.763145923614502\n",
            "epoch: 1 step: 4323, loss is 0.9823379516601562\n",
            "epoch: 1 step: 4324, loss is 0.8890509605407715\n",
            "epoch: 1 step: 4325, loss is 1.1620153188705444\n",
            "epoch: 1 step: 4326, loss is 1.10726797580719\n",
            "epoch: 1 step: 4327, loss is 1.0384160280227661\n",
            "epoch: 1 step: 4328, loss is 0.7904208302497864\n",
            "epoch: 1 step: 4329, loss is 0.8601034283638\n",
            "epoch: 1 step: 4330, loss is 0.8302351236343384\n",
            "epoch: 1 step: 4331, loss is 1.0053051710128784\n",
            "epoch: 1 step: 4332, loss is 0.9834806323051453\n",
            "epoch: 1 step: 4333, loss is 0.8868362307548523\n",
            "epoch: 1 step: 4334, loss is 0.8684616088867188\n",
            "epoch: 1 step: 4335, loss is 0.8314586281776428\n",
            "epoch: 1 step: 4336, loss is 0.9674579501152039\n",
            "epoch: 1 step: 4337, loss is 0.9604614973068237\n",
            "epoch: 1 step: 4338, loss is 0.9334384202957153\n",
            "epoch: 1 step: 4339, loss is 0.8766260743141174\n",
            "epoch: 1 step: 4340, loss is 1.0771161317825317\n",
            "epoch: 1 step: 4341, loss is 0.8602269887924194\n",
            "epoch: 1 step: 4342, loss is 0.8321987986564636\n",
            "epoch: 1 step: 4343, loss is 0.9759993553161621\n",
            "epoch: 1 step: 4344, loss is 1.059983730316162\n",
            "epoch: 1 step: 4345, loss is 0.9327245354652405\n",
            "epoch: 1 step: 4346, loss is 0.8622840642929077\n",
            "epoch: 1 step: 4347, loss is 0.7993767261505127\n",
            "epoch: 1 step: 4348, loss is 0.7469475865364075\n",
            "epoch: 1 step: 4349, loss is 0.8920944333076477\n",
            "epoch: 1 step: 4350, loss is 0.7761253714561462\n",
            "epoch: 1 step: 4351, loss is 1.0415067672729492\n",
            "epoch: 1 step: 4352, loss is 0.8749188780784607\n",
            "epoch: 1 step: 4353, loss is 0.8553069829940796\n",
            "epoch: 1 step: 4354, loss is 1.0342024564743042\n",
            "epoch: 1 step: 4355, loss is 0.7959057688713074\n",
            "epoch: 1 step: 4356, loss is 0.9612905979156494\n",
            "epoch: 1 step: 4357, loss is 0.9962336421012878\n",
            "epoch: 1 step: 4358, loss is 0.8563572764396667\n",
            "epoch: 1 step: 4359, loss is 0.7765518426895142\n",
            "epoch: 1 step: 4360, loss is 0.8327333927154541\n",
            "epoch: 1 step: 4361, loss is 0.8138132095336914\n",
            "epoch: 1 step: 4362, loss is 0.9102503061294556\n",
            "epoch: 1 step: 4363, loss is 0.8344823718070984\n",
            "epoch: 1 step: 4364, loss is 0.916822612285614\n",
            "epoch: 1 step: 4365, loss is 0.7472678422927856\n",
            "epoch: 1 step: 4366, loss is 0.8864343166351318\n",
            "epoch: 1 step: 4367, loss is 1.0083860158920288\n",
            "epoch: 1 step: 4368, loss is 0.9421859979629517\n",
            "epoch: 1 step: 4369, loss is 0.9016784429550171\n",
            "epoch: 1 step: 4370, loss is 0.9007603526115417\n",
            "epoch: 1 step: 4371, loss is 1.0289922952651978\n",
            "epoch: 1 step: 4372, loss is 1.006203293800354\n",
            "epoch: 1 step: 4373, loss is 0.9694581627845764\n",
            "epoch: 1 step: 4374, loss is 0.802415668964386\n",
            "epoch: 1 step: 4375, loss is 0.9599064588546753\n",
            "epoch: 1 step: 4376, loss is 0.8761756420135498\n",
            "epoch: 1 step: 4377, loss is 1.0792057514190674\n",
            "epoch: 1 step: 4378, loss is 0.9645533561706543\n",
            "epoch: 1 step: 4379, loss is 0.8774174451828003\n",
            "epoch: 1 step: 4380, loss is 0.9733296036720276\n",
            "epoch: 1 step: 4381, loss is 0.894543468952179\n",
            "epoch: 1 step: 4382, loss is 0.800285279750824\n",
            "epoch: 1 step: 4383, loss is 0.8419620990753174\n",
            "epoch: 1 step: 4384, loss is 0.8481495976448059\n",
            "epoch: 1 step: 4385, loss is 0.7613065838813782\n",
            "epoch: 1 step: 4386, loss is 0.7344399690628052\n",
            "epoch: 1 step: 4387, loss is 0.8186858892440796\n",
            "epoch: 1 step: 4388, loss is 0.8410583138465881\n",
            "epoch: 1 step: 4389, loss is 0.8345627188682556\n",
            "epoch: 1 step: 4390, loss is 0.9244629144668579\n",
            "epoch: 1 step: 4391, loss is 0.8311527967453003\n",
            "epoch: 1 step: 4392, loss is 0.8588264584541321\n",
            "epoch: 1 step: 4393, loss is 0.7339799404144287\n",
            "epoch: 1 step: 4394, loss is 0.7837746143341064\n",
            "epoch: 1 step: 4395, loss is 0.9255480766296387\n",
            "epoch: 1 step: 4396, loss is 0.8894724249839783\n",
            "epoch: 1 step: 4397, loss is 0.9488358497619629\n",
            "epoch: 1 step: 4398, loss is 0.9356297850608826\n",
            "epoch: 1 step: 4399, loss is 0.788540244102478\n",
            "epoch: 1 step: 4400, loss is 0.9635220766067505\n",
            "epoch: 1 step: 4401, loss is 1.0140093564987183\n",
            "epoch: 1 step: 4402, loss is 0.9355574250221252\n",
            "epoch: 1 step: 4403, loss is 0.9409828186035156\n",
            "epoch: 1 step: 4404, loss is 0.8634635210037231\n",
            "epoch: 1 step: 4405, loss is 0.8766633868217468\n",
            "epoch: 1 step: 4406, loss is 1.0006020069122314\n",
            "epoch: 1 step: 4407, loss is 0.8434752821922302\n",
            "epoch: 1 step: 4408, loss is 0.8508557081222534\n",
            "epoch: 1 step: 4409, loss is 0.9776703715324402\n",
            "epoch: 1 step: 4410, loss is 0.87494295835495\n",
            "epoch: 1 step: 4411, loss is 0.7068473100662231\n",
            "epoch: 1 step: 4412, loss is 0.9174430966377258\n",
            "epoch: 1 step: 4413, loss is 0.830630898475647\n",
            "epoch: 1 step: 4414, loss is 0.8246039748191833\n",
            "epoch: 1 step: 4415, loss is 1.0313249826431274\n",
            "epoch: 1 step: 4416, loss is 0.8378783464431763\n",
            "epoch: 1 step: 4417, loss is 0.8782126307487488\n",
            "epoch: 1 step: 4418, loss is 0.85536128282547\n",
            "epoch: 1 step: 4419, loss is 0.9266993403434753\n",
            "epoch: 1 step: 4420, loss is 0.8840460777282715\n",
            "epoch: 1 step: 4421, loss is 0.9349977374076843\n",
            "epoch: 1 step: 4422, loss is 0.9137304425239563\n",
            "epoch: 1 step: 4423, loss is 1.0639357566833496\n",
            "epoch: 1 step: 4424, loss is 0.96923828125\n",
            "epoch: 1 step: 4425, loss is 0.892659068107605\n",
            "epoch: 1 step: 4426, loss is 0.9242776036262512\n",
            "epoch: 1 step: 4427, loss is 0.6958464980125427\n",
            "epoch: 1 step: 4428, loss is 0.8550544381141663\n",
            "epoch: 1 step: 4429, loss is 0.9574613571166992\n",
            "epoch: 1 step: 4430, loss is 0.7665517330169678\n",
            "epoch: 1 step: 4431, loss is 0.939460813999176\n",
            "epoch: 1 step: 4432, loss is 0.9890967011451721\n",
            "epoch: 1 step: 4433, loss is 0.9512008428573608\n",
            "epoch: 1 step: 4434, loss is 0.9909377694129944\n",
            "epoch: 1 step: 4435, loss is 0.8373345732688904\n",
            "epoch: 1 step: 4436, loss is 0.6521645784378052\n",
            "epoch: 1 step: 4437, loss is 0.7731037735939026\n",
            "epoch: 1 step: 4438, loss is 1.0321290493011475\n",
            "epoch: 1 step: 4439, loss is 0.690697193145752\n",
            "epoch: 1 step: 4440, loss is 0.9879176020622253\n",
            "epoch: 1 step: 4441, loss is 0.8838491439819336\n",
            "epoch: 1 step: 4442, loss is 0.9500961303710938\n",
            "epoch: 1 step: 4443, loss is 0.911636233329773\n",
            "epoch: 1 step: 4444, loss is 0.7725106477737427\n",
            "epoch: 1 step: 4445, loss is 0.8733220100402832\n",
            "epoch: 1 step: 4446, loss is 0.8654860258102417\n",
            "epoch: 1 step: 4447, loss is 0.8800395131111145\n",
            "epoch: 1 step: 4448, loss is 0.7970222234725952\n",
            "epoch: 1 step: 4449, loss is 1.0455948114395142\n",
            "epoch: 1 step: 4450, loss is 0.7878348231315613\n",
            "epoch: 1 step: 4451, loss is 0.9808729290962219\n",
            "epoch: 1 step: 4452, loss is 1.0586247444152832\n",
            "epoch: 1 step: 4453, loss is 0.7665044069290161\n",
            "epoch: 1 step: 4454, loss is 0.9127375483512878\n",
            "epoch: 1 step: 4455, loss is 0.8561663627624512\n",
            "epoch: 1 step: 4456, loss is 0.8945280313491821\n",
            "epoch: 1 step: 4457, loss is 0.6881610155105591\n",
            "epoch: 1 step: 4458, loss is 1.0533469915390015\n",
            "epoch: 1 step: 4459, loss is 0.7693827748298645\n",
            "epoch: 1 step: 4460, loss is 0.7724408507347107\n",
            "epoch: 1 step: 4461, loss is 0.8390290141105652\n",
            "epoch: 1 step: 4462, loss is 0.8869686722755432\n",
            "epoch: 1 step: 4463, loss is 0.7598098516464233\n",
            "epoch: 1 step: 4464, loss is 0.7720268964767456\n",
            "epoch: 1 step: 4465, loss is 0.6889940500259399\n",
            "epoch: 1 step: 4466, loss is 0.817107081413269\n",
            "epoch: 1 step: 4467, loss is 0.882500171661377\n",
            "epoch: 1 step: 4468, loss is 0.9899898171424866\n",
            "epoch: 1 step: 4469, loss is 1.0374276638031006\n",
            "epoch: 1 step: 4470, loss is 0.8858270049095154\n",
            "epoch: 1 step: 4471, loss is 0.7701743245124817\n",
            "epoch: 1 step: 4472, loss is 0.8643547296524048\n",
            "epoch: 1 step: 4473, loss is 1.0084871053695679\n",
            "epoch: 1 step: 4474, loss is 1.031580924987793\n",
            "epoch: 1 step: 4475, loss is 1.0460059642791748\n",
            "epoch: 1 step: 4476, loss is 0.924517810344696\n",
            "epoch: 1 step: 4477, loss is 0.847668468952179\n",
            "epoch: 1 step: 4478, loss is 0.7877593040466309\n",
            "epoch: 1 step: 4479, loss is 0.884154200553894\n",
            "epoch: 1 step: 4480, loss is 1.0185099840164185\n",
            "epoch: 1 step: 4481, loss is 1.156482219696045\n",
            "epoch: 1 step: 4482, loss is 0.8534108400344849\n",
            "epoch: 1 step: 4483, loss is 0.9568619728088379\n",
            "epoch: 1 step: 4484, loss is 0.9013344049453735\n",
            "epoch: 1 step: 4485, loss is 0.9294928312301636\n",
            "epoch: 1 step: 4486, loss is 0.9793556332588196\n",
            "epoch: 1 step: 4487, loss is 0.9893599152565002\n",
            "epoch: 1 step: 4488, loss is 0.7171235084533691\n",
            "epoch: 1 step: 4489, loss is 1.0288225412368774\n",
            "epoch: 1 step: 4490, loss is 0.8583387136459351\n",
            "epoch: 1 step: 4491, loss is 0.8901507258415222\n",
            "epoch: 1 step: 4492, loss is 0.8509137034416199\n",
            "epoch: 1 step: 4493, loss is 0.8772950172424316\n",
            "epoch: 1 step: 4494, loss is 0.936389684677124\n",
            "epoch: 1 step: 4495, loss is 1.026484489440918\n",
            "epoch: 1 step: 4496, loss is 0.9291292428970337\n",
            "epoch: 1 step: 4497, loss is 1.075556755065918\n",
            "epoch: 1 step: 4498, loss is 0.8074479103088379\n",
            "epoch: 1 step: 4499, loss is 0.8198047280311584\n",
            "epoch: 1 step: 4500, loss is 1.0128734111785889\n",
            "epoch: 1 step: 4501, loss is 0.8315081000328064\n",
            "epoch: 1 step: 4502, loss is 0.8793851733207703\n",
            "epoch: 1 step: 4503, loss is 0.8152194023132324\n",
            "epoch: 1 step: 4504, loss is 1.04473078250885\n",
            "epoch: 1 step: 4505, loss is 0.9245748519897461\n",
            "epoch: 1 step: 4506, loss is 0.8474377393722534\n",
            "epoch: 1 step: 4507, loss is 0.6804202795028687\n",
            "epoch: 1 step: 4508, loss is 0.9409598112106323\n",
            "epoch: 1 step: 4509, loss is 0.8642347455024719\n",
            "epoch: 1 step: 4510, loss is 0.9457567930221558\n",
            "epoch: 1 step: 4511, loss is 0.8161832094192505\n",
            "epoch: 1 step: 4512, loss is 1.0517812967300415\n",
            "epoch: 1 step: 4513, loss is 0.7876219153404236\n",
            "epoch: 1 step: 4514, loss is 0.9102843403816223\n",
            "epoch: 1 step: 4515, loss is 0.9004490375518799\n",
            "epoch: 1 step: 4516, loss is 0.8731554746627808\n",
            "epoch: 1 step: 4517, loss is 0.8612662553787231\n",
            "epoch: 1 step: 4518, loss is 0.8342890739440918\n",
            "epoch: 1 step: 4519, loss is 1.0074764490127563\n",
            "epoch: 1 step: 4520, loss is 0.8373749852180481\n",
            "epoch: 1 step: 4521, loss is 0.986014723777771\n",
            "epoch: 1 step: 4522, loss is 0.8960715532302856\n",
            "epoch: 1 step: 4523, loss is 0.9183014631271362\n",
            "epoch: 1 step: 4524, loss is 0.7245255708694458\n",
            "epoch: 1 step: 4525, loss is 1.061033844947815\n",
            "epoch: 1 step: 4526, loss is 0.9493167996406555\n",
            "epoch: 1 step: 4527, loss is 0.9476822018623352\n",
            "epoch: 1 step: 4528, loss is 0.7789474129676819\n",
            "epoch: 1 step: 4529, loss is 0.9878492951393127\n",
            "epoch: 1 step: 4530, loss is 0.8556315898895264\n",
            "epoch: 1 step: 4531, loss is 0.9491069912910461\n",
            "epoch: 1 step: 4532, loss is 0.9779477119445801\n",
            "epoch: 1 step: 4533, loss is 0.9569986462593079\n",
            "epoch: 1 step: 4534, loss is 0.8011537790298462\n",
            "epoch: 1 step: 4535, loss is 0.9482003450393677\n",
            "epoch: 1 step: 4536, loss is 0.806516170501709\n",
            "epoch: 1 step: 4537, loss is 0.8096908926963806\n",
            "epoch: 1 step: 4538, loss is 0.935169517993927\n",
            "epoch: 1 step: 4539, loss is 0.6964762806892395\n",
            "epoch: 1 step: 4540, loss is 0.9562128782272339\n",
            "epoch: 1 step: 4541, loss is 0.8892338275909424\n",
            "epoch: 1 step: 4542, loss is 0.8176514506340027\n",
            "epoch: 1 step: 4543, loss is 1.0430388450622559\n",
            "epoch: 1 step: 4544, loss is 0.8860164880752563\n",
            "epoch: 1 step: 4545, loss is 1.0810805559158325\n",
            "epoch: 1 step: 4546, loss is 0.8346459269523621\n",
            "epoch: 1 step: 4547, loss is 0.7823652625083923\n",
            "epoch: 1 step: 4548, loss is 0.8256217241287231\n",
            "epoch: 1 step: 4549, loss is 0.9737801551818848\n",
            "epoch: 1 step: 4550, loss is 0.9718320369720459\n",
            "epoch: 1 step: 4551, loss is 0.9127843379974365\n",
            "epoch: 1 step: 4552, loss is 0.7807997465133667\n",
            "epoch: 1 step: 4553, loss is 0.8475110530853271\n",
            "epoch: 1 step: 4554, loss is 0.9702504277229309\n",
            "epoch: 1 step: 4555, loss is 0.8223613500595093\n",
            "epoch: 1 step: 4556, loss is 0.8834882378578186\n",
            "epoch: 1 step: 4557, loss is 1.0537041425704956\n",
            "epoch: 1 step: 4558, loss is 0.9590357542037964\n",
            "epoch: 1 step: 4559, loss is 0.8591529130935669\n",
            "epoch: 1 step: 4560, loss is 0.773975670337677\n",
            "epoch: 1 step: 4561, loss is 1.0168793201446533\n",
            "epoch: 1 step: 4562, loss is 0.961138904094696\n",
            "epoch: 1 step: 4563, loss is 0.8169934749603271\n",
            "epoch: 1 step: 4564, loss is 0.9248749613761902\n",
            "epoch: 1 step: 4565, loss is 0.9683679342269897\n",
            "epoch: 1 step: 4566, loss is 0.8655645251274109\n",
            "epoch: 1 step: 4567, loss is 0.7630361318588257\n",
            "epoch: 1 step: 4568, loss is 0.9323200583457947\n",
            "epoch: 1 step: 4569, loss is 0.8878723978996277\n",
            "epoch: 1 step: 4570, loss is 1.0457638502120972\n",
            "epoch: 1 step: 4571, loss is 0.8149603009223938\n",
            "epoch: 1 step: 4572, loss is 0.9131602644920349\n",
            "epoch: 1 step: 4573, loss is 0.9528152942657471\n",
            "epoch: 1 step: 4574, loss is 0.9965654611587524\n",
            "epoch: 1 step: 4575, loss is 0.9260047674179077\n",
            "epoch: 1 step: 4576, loss is 0.89534991979599\n",
            "epoch: 1 step: 4577, loss is 0.8131123185157776\n",
            "epoch: 1 step: 4578, loss is 0.8641842603683472\n",
            "epoch: 1 step: 4579, loss is 0.9085184931755066\n",
            "epoch: 1 step: 4580, loss is 0.9812285900115967\n",
            "epoch: 1 step: 4581, loss is 0.8143107295036316\n",
            "epoch: 1 step: 4582, loss is 0.9837117195129395\n",
            "epoch: 1 step: 4583, loss is 0.9865564107894897\n",
            "epoch: 1 step: 4584, loss is 0.9104675650596619\n",
            "epoch: 1 step: 4585, loss is 0.8564557433128357\n",
            "epoch: 1 step: 4586, loss is 0.9329720735549927\n",
            "epoch: 1 step: 4587, loss is 1.0639405250549316\n",
            "epoch: 1 step: 4588, loss is 0.8136468529701233\n",
            "epoch: 1 step: 4589, loss is 0.7847903966903687\n",
            "epoch: 1 step: 4590, loss is 0.7430558204650879\n",
            "epoch: 1 step: 4591, loss is 1.0122424364089966\n",
            "epoch: 1 step: 4592, loss is 0.867059051990509\n",
            "epoch: 1 step: 4593, loss is 0.8378021121025085\n",
            "epoch: 1 step: 4594, loss is 0.8713280558586121\n",
            "epoch: 1 step: 4595, loss is 1.0769402980804443\n",
            "epoch: 1 step: 4596, loss is 0.9003798365592957\n",
            "epoch: 1 step: 4597, loss is 0.9166081547737122\n",
            "epoch: 1 step: 4598, loss is 0.7912298440933228\n",
            "epoch: 1 step: 4599, loss is 0.8534271717071533\n",
            "epoch: 1 step: 4600, loss is 0.833453893661499\n",
            "epoch: 1 step: 4601, loss is 0.8812251687049866\n",
            "epoch: 1 step: 4602, loss is 0.7704518437385559\n",
            "epoch: 1 step: 4603, loss is 0.9408950805664062\n",
            "epoch: 1 step: 4604, loss is 0.9796330332756042\n",
            "epoch: 1 step: 4605, loss is 1.0722334384918213\n",
            "epoch: 1 step: 4606, loss is 1.0677047967910767\n",
            "epoch: 1 step: 4607, loss is 0.7473345398902893\n",
            "epoch: 1 step: 4608, loss is 0.9016980528831482\n",
            "epoch: 1 step: 4609, loss is 0.7396179437637329\n",
            "epoch: 1 step: 4610, loss is 0.9410908222198486\n",
            "epoch: 1 step: 4611, loss is 0.8508091568946838\n",
            "epoch: 1 step: 4612, loss is 0.8508803248405457\n",
            "epoch: 1 step: 4613, loss is 0.8828893899917603\n",
            "epoch: 1 step: 4614, loss is 0.7837300896644592\n",
            "epoch: 1 step: 4615, loss is 0.9429280161857605\n",
            "epoch: 1 step: 4616, loss is 0.9275829195976257\n",
            "epoch: 1 step: 4617, loss is 0.723074734210968\n",
            "epoch: 1 step: 4618, loss is 0.7940999865531921\n",
            "epoch: 1 step: 4619, loss is 0.8980549573898315\n",
            "epoch: 1 step: 4620, loss is 1.0284961462020874\n",
            "epoch: 1 step: 4621, loss is 0.6260952353477478\n",
            "epoch: 1 step: 4622, loss is 0.9182913303375244\n",
            "epoch: 1 step: 4623, loss is 0.7931562066078186\n",
            "epoch: 1 step: 4624, loss is 0.8572837710380554\n",
            "epoch: 1 step: 4625, loss is 0.9280475974082947\n",
            "epoch: 1 step: 4626, loss is 1.0387052297592163\n",
            "epoch: 1 step: 4627, loss is 0.9310622215270996\n",
            "epoch: 1 step: 4628, loss is 0.9995389580726624\n",
            "epoch: 1 step: 4629, loss is 0.9107822179794312\n",
            "epoch: 1 step: 4630, loss is 0.8523837327957153\n",
            "epoch: 1 step: 4631, loss is 1.023472547531128\n",
            "epoch: 1 step: 4632, loss is 0.8839710354804993\n",
            "epoch: 1 step: 4633, loss is 0.8604595065116882\n",
            "epoch: 1 step: 4634, loss is 0.8737094402313232\n",
            "epoch: 1 step: 4635, loss is 0.6797130107879639\n",
            "epoch: 1 step: 4636, loss is 0.9937337636947632\n",
            "epoch: 1 step: 4637, loss is 0.860268235206604\n",
            "epoch: 1 step: 4638, loss is 0.8106809258460999\n",
            "epoch: 1 step: 4639, loss is 0.871195912361145\n",
            "epoch: 1 step: 4640, loss is 0.8255242109298706\n",
            "epoch: 1 step: 4641, loss is 0.8314394354820251\n",
            "epoch: 1 step: 4642, loss is 0.997919499874115\n",
            "epoch: 1 step: 4643, loss is 0.9792236089706421\n",
            "epoch: 1 step: 4644, loss is 1.0548664331436157\n",
            "epoch: 1 step: 4645, loss is 0.6624859571456909\n",
            "epoch: 1 step: 4646, loss is 0.7242544889450073\n",
            "epoch: 1 step: 4647, loss is 0.9135162830352783\n",
            "epoch: 1 step: 4648, loss is 0.8304418921470642\n",
            "epoch: 1 step: 4649, loss is 0.8960007429122925\n",
            "epoch: 1 step: 4650, loss is 0.9077935814857483\n",
            "epoch: 1 step: 4651, loss is 0.8944458961486816\n",
            "epoch: 1 step: 4652, loss is 0.9489020109176636\n",
            "epoch: 1 step: 4653, loss is 0.8849759101867676\n",
            "epoch: 1 step: 4654, loss is 0.8247237801551819\n",
            "epoch: 1 step: 4655, loss is 0.7862459421157837\n",
            "epoch: 1 step: 4656, loss is 0.8652251958847046\n",
            "epoch: 1 step: 4657, loss is 0.9895752668380737\n",
            "epoch: 1 step: 4658, loss is 0.844467043876648\n",
            "epoch: 1 step: 4659, loss is 0.760921061038971\n",
            "epoch: 1 step: 4660, loss is 0.9045724272727966\n",
            "epoch: 1 step: 4661, loss is 0.9683154821395874\n",
            "epoch: 1 step: 4662, loss is 0.8944941163063049\n",
            "epoch: 1 step: 4663, loss is 0.8368623852729797\n",
            "epoch: 1 step: 4664, loss is 0.7706705331802368\n",
            "epoch: 1 step: 4665, loss is 1.106078863143921\n",
            "epoch: 1 step: 4666, loss is 0.7151356339454651\n",
            "epoch: 1 step: 4667, loss is 0.9327762722969055\n",
            "epoch: 1 step: 4668, loss is 1.0830217599868774\n",
            "epoch: 1 step: 4669, loss is 0.9356255531311035\n",
            "epoch: 1 step: 4670, loss is 0.8370566368103027\n",
            "epoch: 1 step: 4671, loss is 0.9328538775444031\n",
            "epoch: 1 step: 4672, loss is 0.8326998353004456\n",
            "epoch: 1 step: 4673, loss is 1.0273703336715698\n",
            "epoch: 1 step: 4674, loss is 1.0497770309448242\n",
            "epoch: 1 step: 4675, loss is 0.9489393830299377\n",
            "epoch: 1 step: 4676, loss is 0.9047507047653198\n",
            "epoch: 1 step: 4677, loss is 0.8311185836791992\n",
            "epoch: 1 step: 4678, loss is 0.9032304286956787\n",
            "epoch: 1 step: 4679, loss is 0.8976311087608337\n",
            "epoch: 1 step: 4680, loss is 1.0185097455978394\n",
            "epoch: 1 step: 4681, loss is 0.9841590523719788\n",
            "epoch: 1 step: 4682, loss is 0.9700801968574524\n",
            "epoch: 1 step: 4683, loss is 0.9372246265411377\n",
            "epoch: 1 step: 4684, loss is 0.879582405090332\n",
            "epoch: 1 step: 4685, loss is 0.9218248724937439\n",
            "epoch: 1 step: 4686, loss is 0.8997662663459778\n",
            "epoch: 1 step: 4687, loss is 0.8260022401809692\n",
            "epoch: 1 step: 4688, loss is 0.7653349041938782\n",
            "epoch: 1 step: 4689, loss is 0.8036823272705078\n",
            "epoch: 1 step: 4690, loss is 0.7994962930679321\n",
            "epoch: 1 step: 4691, loss is 0.834298849105835\n",
            "epoch: 1 step: 4692, loss is 0.795562744140625\n",
            "epoch: 1 step: 4693, loss is 0.7991633415222168\n",
            "epoch: 1 step: 4694, loss is 0.8071474432945251\n",
            "epoch: 1 step: 4695, loss is 0.8294661045074463\n",
            "epoch: 1 step: 4696, loss is 0.7162947654724121\n",
            "epoch: 1 step: 4697, loss is 0.9799996018409729\n",
            "epoch: 1 step: 4698, loss is 0.9819700121879578\n",
            "epoch: 1 step: 4699, loss is 0.9646214842796326\n",
            "epoch: 1 step: 4700, loss is 0.9852161407470703\n",
            "epoch: 1 step: 4701, loss is 0.7707149386405945\n",
            "epoch: 1 step: 4702, loss is 0.8990308046340942\n",
            "epoch: 1 step: 4703, loss is 0.8700417280197144\n",
            "epoch: 1 step: 4704, loss is 1.086251974105835\n",
            "epoch: 1 step: 4705, loss is 0.8557678461074829\n",
            "epoch: 1 step: 4706, loss is 1.143398642539978\n",
            "epoch: 1 step: 4707, loss is 0.9025740027427673\n",
            "epoch: 1 step: 4708, loss is 0.9462805986404419\n",
            "epoch: 1 step: 4709, loss is 0.8618776798248291\n",
            "epoch: 1 step: 4710, loss is 0.8557625412940979\n",
            "epoch: 1 step: 4711, loss is 0.9786480069160461\n",
            "epoch: 1 step: 4712, loss is 0.963686466217041\n",
            "epoch: 1 step: 4713, loss is 0.709909975528717\n",
            "epoch: 1 step: 4714, loss is 0.8500266671180725\n",
            "epoch: 1 step: 4715, loss is 0.9875668287277222\n",
            "epoch: 1 step: 4716, loss is 0.9000129699707031\n",
            "epoch: 1 step: 4717, loss is 0.8552485108375549\n",
            "epoch: 1 step: 4718, loss is 0.9518542885780334\n",
            "epoch: 1 step: 4719, loss is 0.9238266348838806\n",
            "epoch: 1 step: 4720, loss is 1.0618776082992554\n",
            "epoch: 1 step: 4721, loss is 0.7128746509552002\n",
            "epoch: 1 step: 4722, loss is 0.7232326865196228\n",
            "epoch: 1 step: 4723, loss is 0.9332392811775208\n",
            "epoch: 1 step: 4724, loss is 0.6714296340942383\n",
            "epoch: 1 step: 4725, loss is 0.9518688917160034\n",
            "epoch: 1 step: 4726, loss is 0.933336079120636\n",
            "epoch: 1 step: 4727, loss is 0.9164769053459167\n",
            "epoch: 1 step: 4728, loss is 0.8386180996894836\n",
            "epoch: 1 step: 4729, loss is 0.8563194870948792\n",
            "epoch: 1 step: 4730, loss is 0.7778735160827637\n",
            "epoch: 1 step: 4731, loss is 0.891883134841919\n",
            "epoch: 1 step: 4732, loss is 0.9357035756111145\n",
            "epoch: 1 step: 4733, loss is 0.790123701095581\n",
            "epoch: 1 step: 4734, loss is 0.7494303584098816\n",
            "epoch: 1 step: 4735, loss is 1.1327581405639648\n",
            "epoch: 1 step: 4736, loss is 0.8839976787567139\n",
            "epoch: 1 step: 4737, loss is 0.8976306319236755\n",
            "epoch: 1 step: 4738, loss is 0.8535014390945435\n",
            "epoch: 1 step: 4739, loss is 0.9242026209831238\n",
            "epoch: 1 step: 4740, loss is 0.8527984619140625\n",
            "epoch: 1 step: 4741, loss is 0.9015974998474121\n",
            "epoch: 1 step: 4742, loss is 0.8720582723617554\n",
            "epoch: 1 step: 4743, loss is 0.7154989838600159\n",
            "epoch: 1 step: 4744, loss is 0.7714181542396545\n",
            "epoch: 1 step: 4745, loss is 0.8387762308120728\n",
            "epoch: 1 step: 4746, loss is 0.9266775250434875\n",
            "epoch: 1 step: 4747, loss is 0.9008423686027527\n",
            "epoch: 1 step: 4748, loss is 1.0303562879562378\n",
            "epoch: 1 step: 4749, loss is 0.8750039935112\n",
            "epoch: 1 step: 4750, loss is 0.8856871724128723\n",
            "epoch: 1 step: 4751, loss is 0.966741681098938\n",
            "epoch: 1 step: 4752, loss is 0.8987122178077698\n",
            "epoch: 1 step: 4753, loss is 0.8893033862113953\n",
            "epoch: 1 step: 4754, loss is 0.8925419449806213\n",
            "epoch: 1 step: 4755, loss is 0.8141816854476929\n",
            "epoch: 1 step: 4756, loss is 0.8262988924980164\n",
            "epoch: 1 step: 4757, loss is 0.7723759412765503\n",
            "epoch: 1 step: 4758, loss is 0.7550956010818481\n",
            "epoch: 1 step: 4759, loss is 0.7862526178359985\n",
            "epoch: 1 step: 4760, loss is 0.9430665373802185\n",
            "epoch: 1 step: 4761, loss is 0.9119040369987488\n",
            "epoch: 1 step: 4762, loss is 0.9230558276176453\n",
            "epoch: 1 step: 4763, loss is 1.0111719369888306\n",
            "epoch: 1 step: 4764, loss is 0.9382448196411133\n",
            "epoch: 1 step: 4765, loss is 1.136655569076538\n",
            "epoch: 1 step: 4766, loss is 0.9380701780319214\n",
            "epoch: 1 step: 4767, loss is 0.8939316868782043\n",
            "epoch: 1 step: 4768, loss is 0.8089273571968079\n",
            "epoch: 1 step: 4769, loss is 0.9257156848907471\n",
            "epoch: 1 step: 4770, loss is 1.0908368825912476\n",
            "epoch: 1 step: 4771, loss is 0.7649068832397461\n",
            "epoch: 1 step: 4772, loss is 0.8343067765235901\n",
            "epoch: 1 step: 4773, loss is 0.8235126733779907\n",
            "epoch: 1 step: 4774, loss is 0.900769829750061\n",
            "epoch: 1 step: 4775, loss is 0.7747352719306946\n",
            "epoch: 1 step: 4776, loss is 0.9883083701133728\n",
            "epoch: 1 step: 4777, loss is 0.7709344625473022\n",
            "epoch: 1 step: 4778, loss is 0.8631511330604553\n",
            "epoch: 1 step: 4779, loss is 0.8953919410705566\n",
            "epoch: 1 step: 4780, loss is 0.7632961869239807\n",
            "epoch: 1 step: 4781, loss is 0.9796014428138733\n",
            "epoch: 1 step: 4782, loss is 0.6914007663726807\n",
            "epoch: 1 step: 4783, loss is 0.8529284596443176\n",
            "epoch: 1 step: 4784, loss is 0.9182930588722229\n",
            "epoch: 1 step: 4785, loss is 0.8133928179740906\n",
            "epoch: 1 step: 4786, loss is 0.8185815811157227\n",
            "epoch: 1 step: 4787, loss is 1.0409507751464844\n",
            "epoch: 1 step: 4788, loss is 0.9127411842346191\n",
            "epoch: 1 step: 4789, loss is 0.9244132041931152\n",
            "epoch: 1 step: 4790, loss is 0.867870032787323\n",
            "epoch: 1 step: 4791, loss is 0.7915862798690796\n",
            "epoch: 1 step: 4792, loss is 0.9233853816986084\n",
            "epoch: 1 step: 4793, loss is 0.8576257824897766\n",
            "epoch: 1 step: 4794, loss is 0.8772608041763306\n",
            "epoch: 1 step: 4795, loss is 0.8875465393066406\n",
            "epoch: 1 step: 4796, loss is 0.9352962970733643\n",
            "epoch: 1 step: 4797, loss is 0.7646502256393433\n",
            "epoch: 1 step: 4798, loss is 0.8124887347221375\n",
            "epoch: 1 step: 4799, loss is 0.9677950739860535\n",
            "epoch: 1 step: 4800, loss is 0.7404637932777405\n",
            "epoch: 1 step: 4801, loss is 0.8029398322105408\n",
            "epoch: 1 step: 4802, loss is 0.8211915493011475\n",
            "epoch: 1 step: 4803, loss is 0.8532084822654724\n",
            "epoch: 1 step: 4804, loss is 0.898476779460907\n",
            "epoch: 1 step: 4805, loss is 1.0335735082626343\n",
            "epoch: 1 step: 4806, loss is 0.8377425670623779\n",
            "epoch: 1 step: 4807, loss is 0.8038090467453003\n",
            "epoch: 1 step: 4808, loss is 0.8590072989463806\n",
            "epoch: 1 step: 4809, loss is 0.7242865562438965\n",
            "epoch: 1 step: 4810, loss is 0.8825669288635254\n",
            "epoch: 1 step: 4811, loss is 0.9748190641403198\n",
            "epoch: 1 step: 4812, loss is 0.8445897698402405\n",
            "epoch: 1 step: 4813, loss is 0.9083308577537537\n",
            "epoch: 1 step: 4814, loss is 0.9409655928611755\n",
            "epoch: 1 step: 4815, loss is 1.0348564386367798\n",
            "epoch: 1 step: 4816, loss is 0.9383621215820312\n",
            "epoch: 1 step: 4817, loss is 0.9393220543861389\n",
            "epoch: 1 step: 4818, loss is 0.9578302502632141\n",
            "epoch: 1 step: 4819, loss is 0.9305587410926819\n",
            "epoch: 1 step: 4820, loss is 0.825391411781311\n",
            "epoch: 1 step: 4821, loss is 0.8189074993133545\n",
            "epoch: 1 step: 4822, loss is 0.8208580017089844\n",
            "epoch: 1 step: 4823, loss is 0.6245562434196472\n",
            "epoch: 1 step: 4824, loss is 0.8177314400672913\n",
            "epoch: 1 step: 4825, loss is 1.1086236238479614\n",
            "epoch: 1 step: 4826, loss is 0.6805375218391418\n",
            "epoch: 1 step: 4827, loss is 0.7379738092422485\n",
            "epoch: 1 step: 4828, loss is 0.7550489902496338\n",
            "epoch: 1 step: 4829, loss is 0.8929824233055115\n",
            "epoch: 1 step: 4830, loss is 0.9612674713134766\n",
            "epoch: 1 step: 4831, loss is 1.1560319662094116\n",
            "epoch: 1 step: 4832, loss is 0.8936953544616699\n",
            "epoch: 1 step: 4833, loss is 0.8351261019706726\n",
            "epoch: 1 step: 4834, loss is 0.899267852306366\n",
            "epoch: 1 step: 4835, loss is 1.1035454273223877\n",
            "epoch: 1 step: 4836, loss is 0.732666552066803\n",
            "epoch: 1 step: 4837, loss is 0.9857977032661438\n",
            "epoch: 1 step: 4838, loss is 0.9765071272850037\n",
            "epoch: 1 step: 4839, loss is 0.8697898983955383\n",
            "epoch: 1 step: 4840, loss is 0.675234317779541\n",
            "epoch: 1 step: 4841, loss is 0.804330050945282\n",
            "epoch: 1 step: 4842, loss is 0.8652028441429138\n",
            "epoch: 1 step: 4843, loss is 0.7794601917266846\n",
            "epoch: 1 step: 4844, loss is 1.2504942417144775\n",
            "epoch: 1 step: 4845, loss is 0.9966269135475159\n",
            "epoch: 1 step: 4846, loss is 0.8908804059028625\n",
            "epoch: 1 step: 4847, loss is 0.8249104619026184\n",
            "epoch: 1 step: 4848, loss is 0.8956699371337891\n",
            "epoch: 1 step: 4849, loss is 1.0069020986557007\n",
            "epoch: 1 step: 4850, loss is 0.6670427322387695\n",
            "epoch: 1 step: 4851, loss is 0.8889819383621216\n",
            "epoch: 1 step: 4852, loss is 0.7776837944984436\n",
            "epoch: 1 step: 4853, loss is 0.8105954527854919\n",
            "epoch: 1 step: 4854, loss is 0.9557362198829651\n",
            "epoch: 1 step: 4855, loss is 0.937678337097168\n",
            "epoch: 1 step: 4856, loss is 0.8765046000480652\n",
            "epoch: 1 step: 4857, loss is 0.8903864622116089\n",
            "epoch: 1 step: 4858, loss is 0.9937434792518616\n",
            "epoch: 1 step: 4859, loss is 0.9529997706413269\n",
            "epoch: 1 step: 4860, loss is 0.9241594672203064\n",
            "epoch: 1 step: 4861, loss is 0.9298779964447021\n",
            "epoch: 1 step: 4862, loss is 1.0591654777526855\n",
            "epoch: 1 step: 4863, loss is 0.9239296317100525\n",
            "epoch: 1 step: 4864, loss is 0.9358798861503601\n",
            "epoch: 1 step: 4865, loss is 0.9323280453681946\n",
            "epoch: 1 step: 4866, loss is 0.7623050808906555\n",
            "epoch: 1 step: 4867, loss is 0.8425562977790833\n",
            "epoch: 1 step: 4868, loss is 0.8472599387168884\n",
            "epoch: 1 step: 4869, loss is 1.026686668395996\n",
            "epoch: 1 step: 4870, loss is 0.8785502910614014\n",
            "epoch: 1 step: 4871, loss is 1.0581028461456299\n",
            "epoch: 1 step: 4872, loss is 1.1186496019363403\n",
            "epoch: 1 step: 4873, loss is 0.9803816080093384\n",
            "epoch: 1 step: 4874, loss is 0.760016679763794\n",
            "epoch: 1 step: 4875, loss is 0.9801865816116333\n",
            "epoch: 1 step: 4876, loss is 0.8578774333000183\n",
            "epoch: 1 step: 4877, loss is 1.0672284364700317\n",
            "epoch: 1 step: 4878, loss is 0.8532410860061646\n",
            "epoch: 1 step: 4879, loss is 0.9286283254623413\n",
            "epoch: 1 step: 4880, loss is 0.848892331123352\n",
            "epoch: 1 step: 4881, loss is 0.931616485118866\n",
            "epoch: 1 step: 4882, loss is 1.095794916152954\n",
            "epoch: 1 step: 4883, loss is 0.888002872467041\n",
            "epoch: 1 step: 4884, loss is 0.9697638750076294\n",
            "epoch: 1 step: 4885, loss is 0.9604476690292358\n",
            "epoch: 1 step: 4886, loss is 0.7219646573066711\n",
            "epoch: 1 step: 4887, loss is 0.9416490197181702\n",
            "epoch: 1 step: 4888, loss is 0.7996824383735657\n",
            "epoch: 1 step: 4889, loss is 0.9615756273269653\n",
            "epoch: 1 step: 4890, loss is 1.0166935920715332\n",
            "epoch: 1 step: 4891, loss is 0.8509404063224792\n",
            "epoch: 1 step: 4892, loss is 1.0465103387832642\n",
            "epoch: 1 step: 4893, loss is 0.9866544008255005\n",
            "epoch: 1 step: 4894, loss is 1.0837655067443848\n",
            "epoch: 1 step: 4895, loss is 0.9044174551963806\n",
            "epoch: 1 step: 4896, loss is 1.127388834953308\n",
            "epoch: 1 step: 4897, loss is 0.896935760974884\n",
            "epoch: 1 step: 4898, loss is 0.8773884177207947\n",
            "epoch: 1 step: 4899, loss is 1.0603344440460205\n",
            "epoch: 1 step: 4900, loss is 0.9422284364700317\n",
            "epoch: 1 step: 4901, loss is 0.8044938445091248\n",
            "epoch: 1 step: 4902, loss is 0.8130741119384766\n",
            "epoch: 1 step: 4903, loss is 0.8708590865135193\n",
            "epoch: 1 step: 4904, loss is 0.8095346689224243\n",
            "epoch: 1 step: 4905, loss is 0.8380364775657654\n",
            "epoch: 1 step: 4906, loss is 0.9308498501777649\n",
            "epoch: 1 step: 4907, loss is 0.8361184000968933\n",
            "epoch: 1 step: 4908, loss is 0.821270763874054\n",
            "epoch: 1 step: 4909, loss is 1.0039925575256348\n",
            "epoch: 1 step: 4910, loss is 0.8745933771133423\n",
            "epoch: 1 step: 4911, loss is 0.7786396145820618\n",
            "epoch: 1 step: 4912, loss is 0.8360533714294434\n",
            "epoch: 1 step: 4913, loss is 0.9327218532562256\n",
            "epoch: 1 step: 4914, loss is 1.141802430152893\n",
            "epoch: 1 step: 4915, loss is 0.772186279296875\n",
            "epoch: 1 step: 4916, loss is 0.9448702335357666\n",
            "epoch: 1 step: 4917, loss is 0.825985848903656\n",
            "epoch: 1 step: 4918, loss is 0.7629909515380859\n",
            "epoch: 1 step: 4919, loss is 1.0211273431777954\n",
            "epoch: 1 step: 4920, loss is 0.7741435170173645\n",
            "epoch: 1 step: 4921, loss is 0.9601511359214783\n",
            "epoch: 1 step: 4922, loss is 1.0398175716400146\n",
            "epoch: 1 step: 4923, loss is 0.7346633672714233\n",
            "epoch: 1 step: 4924, loss is 0.8738277554512024\n",
            "epoch: 1 step: 4925, loss is 0.8852707147598267\n",
            "epoch: 1 step: 4926, loss is 0.8041825294494629\n",
            "epoch: 1 step: 4927, loss is 0.832007646560669\n",
            "epoch: 1 step: 4928, loss is 1.0937849283218384\n",
            "epoch: 1 step: 4929, loss is 0.9977335333824158\n",
            "epoch: 1 step: 4930, loss is 0.7856781482696533\n",
            "epoch: 1 step: 4931, loss is 0.8759828805923462\n",
            "epoch: 1 step: 4932, loss is 0.9762768149375916\n",
            "epoch: 1 step: 4933, loss is 0.9054998159408569\n",
            "epoch: 1 step: 4934, loss is 0.9140353202819824\n",
            "epoch: 1 step: 4935, loss is 0.993334949016571\n",
            "epoch: 1 step: 4936, loss is 0.9010579586029053\n",
            "epoch: 1 step: 4937, loss is 0.9014056921005249\n",
            "epoch: 1 step: 4938, loss is 1.1529871225357056\n",
            "epoch: 1 step: 4939, loss is 0.7822180390357971\n",
            "epoch: 1 step: 4940, loss is 0.9790747165679932\n",
            "epoch: 1 step: 4941, loss is 1.1139981746673584\n",
            "epoch: 1 step: 4942, loss is 0.8007915019989014\n",
            "epoch: 1 step: 4943, loss is 0.7067582011222839\n",
            "epoch: 1 step: 4944, loss is 1.1489958763122559\n",
            "epoch: 1 step: 4945, loss is 0.8185738921165466\n",
            "epoch: 1 step: 4946, loss is 0.8773202300071716\n",
            "epoch: 1 step: 4947, loss is 1.0125778913497925\n",
            "epoch: 1 step: 4948, loss is 0.8317921161651611\n",
            "epoch: 1 step: 4949, loss is 0.9742264747619629\n",
            "epoch: 1 step: 4950, loss is 0.8297228813171387\n",
            "epoch: 1 step: 4951, loss is 1.0587831735610962\n",
            "epoch: 1 step: 4952, loss is 0.8856210112571716\n",
            "epoch: 1 step: 4953, loss is 0.9012937545776367\n",
            "epoch: 1 step: 4954, loss is 0.9265655875205994\n",
            "epoch: 1 step: 4955, loss is 0.8651870489120483\n",
            "epoch: 1 step: 4956, loss is 0.9577959179878235\n",
            "epoch: 1 step: 4957, loss is 0.8415651321411133\n",
            "epoch: 1 step: 4958, loss is 0.8965383172035217\n",
            "epoch: 1 step: 4959, loss is 0.8966906070709229\n",
            "epoch: 1 step: 4960, loss is 1.0970145463943481\n",
            "epoch: 1 step: 4961, loss is 0.8601734638214111\n",
            "epoch: 1 step: 4962, loss is 0.9288829565048218\n",
            "epoch: 1 step: 4963, loss is 0.8168579936027527\n",
            "epoch: 1 step: 4964, loss is 0.9217900037765503\n",
            "epoch: 1 step: 4965, loss is 0.8184200525283813\n",
            "epoch: 1 step: 4966, loss is 0.8581615686416626\n",
            "epoch: 1 step: 4967, loss is 0.8454602360725403\n",
            "epoch: 1 step: 4968, loss is 0.854711651802063\n",
            "epoch: 1 step: 4969, loss is 0.8780971765518188\n",
            "epoch: 1 step: 4970, loss is 0.8050400614738464\n",
            "epoch: 1 step: 4971, loss is 0.9850105047225952\n",
            "epoch: 1 step: 4972, loss is 1.0269490480422974\n",
            "epoch: 1 step: 4973, loss is 0.8433952331542969\n",
            "epoch: 1 step: 4974, loss is 0.9344568252563477\n",
            "epoch: 1 step: 4975, loss is 0.915126383304596\n",
            "epoch: 1 step: 4976, loss is 0.7836527824401855\n",
            "epoch: 1 step: 4977, loss is 0.8197563886642456\n",
            "epoch: 1 step: 4978, loss is 0.9510000348091125\n",
            "epoch: 1 step: 4979, loss is 0.9803228378295898\n",
            "epoch: 1 step: 4980, loss is 0.8923181891441345\n",
            "epoch: 1 step: 4981, loss is 0.8671107888221741\n",
            "epoch: 1 step: 4982, loss is 0.8772037625312805\n",
            "epoch: 1 step: 4983, loss is 0.762320876121521\n",
            "epoch: 1 step: 4984, loss is 0.931829571723938\n",
            "epoch: 1 step: 4985, loss is 0.9487763047218323\n",
            "epoch: 1 step: 4986, loss is 0.7047142386436462\n",
            "epoch: 1 step: 4987, loss is 1.0768195390701294\n",
            "epoch: 1 step: 4988, loss is 0.8197655081748962\n",
            "epoch: 1 step: 4989, loss is 0.901326060295105\n",
            "epoch: 1 step: 4990, loss is 0.9119415879249573\n",
            "epoch: 1 step: 4991, loss is 0.7221492528915405\n",
            "epoch: 1 step: 4992, loss is 0.8866985440254211\n",
            "epoch: 1 step: 4993, loss is 0.9806609749794006\n",
            "epoch: 1 step: 4994, loss is 0.858388364315033\n",
            "epoch: 1 step: 4995, loss is 0.860518753528595\n",
            "epoch: 1 step: 4996, loss is 0.9850590229034424\n",
            "epoch: 1 step: 4997, loss is 0.7955144643783569\n",
            "epoch: 1 step: 4998, loss is 0.7431809902191162\n",
            "epoch: 1 step: 4999, loss is 0.9215040802955627\n",
            "epoch: 1 step: 5000, loss is 1.1361912488937378\n",
            "epoch: 1 step: 5001, loss is 1.0553531646728516\n",
            "epoch: 1 step: 5002, loss is 1.0279179811477661\n",
            "epoch: 1 step: 5003, loss is 0.9022656679153442\n",
            "epoch: 1 step: 5004, loss is 0.8397834300994873\n",
            "epoch: 1 step: 5005, loss is 0.9768673181533813\n",
            "epoch: 1 step: 5006, loss is 0.7697718739509583\n",
            "epoch: 1 step: 5007, loss is 0.7955821752548218\n",
            "epoch: 1 step: 5008, loss is 0.9700230956077576\n",
            "epoch: 1 step: 5009, loss is 0.9241432547569275\n",
            "epoch: 1 step: 5010, loss is 1.0554420948028564\n",
            "epoch: 1 step: 5011, loss is 0.9326871633529663\n",
            "epoch: 1 step: 5012, loss is 0.7821322679519653\n",
            "epoch: 1 step: 5013, loss is 0.7725947499275208\n",
            "epoch: 1 step: 5014, loss is 0.8600248694419861\n",
            "epoch: 1 step: 5015, loss is 0.8715255260467529\n",
            "epoch: 1 step: 5016, loss is 0.86003178358078\n",
            "epoch: 1 step: 5017, loss is 0.8213719129562378\n",
            "epoch: 1 step: 5018, loss is 0.9065905213356018\n",
            "epoch: 1 step: 5019, loss is 0.9611387252807617\n",
            "epoch: 1 step: 5020, loss is 0.7747762203216553\n",
            "epoch: 1 step: 5021, loss is 0.8042570352554321\n",
            "epoch: 1 step: 5022, loss is 0.9090944528579712\n",
            "epoch: 1 step: 5023, loss is 0.849625825881958\n",
            "epoch: 1 step: 5024, loss is 0.7915571331977844\n",
            "epoch: 1 step: 5025, loss is 0.92512446641922\n",
            "epoch: 1 step: 5026, loss is 0.8609210252761841\n",
            "epoch: 1 step: 5027, loss is 0.8364481925964355\n",
            "epoch: 1 step: 5028, loss is 0.9731971025466919\n",
            "epoch: 1 step: 5029, loss is 1.0106432437896729\n",
            "epoch: 1 step: 5030, loss is 0.6422679424285889\n",
            "epoch: 1 step: 5031, loss is 0.8313568830490112\n",
            "epoch: 1 step: 5032, loss is 0.8333148956298828\n",
            "epoch: 1 step: 5033, loss is 0.8739220499992371\n",
            "epoch: 1 step: 5034, loss is 0.9540179967880249\n",
            "epoch: 1 step: 5035, loss is 0.9454416036605835\n",
            "epoch: 1 step: 5036, loss is 1.0128484964370728\n",
            "epoch: 1 step: 5037, loss is 0.967178463935852\n",
            "epoch: 1 step: 5038, loss is 0.9049803614616394\n",
            "epoch: 1 step: 5039, loss is 0.8803277611732483\n",
            "epoch: 1 step: 5040, loss is 0.7834145426750183\n",
            "epoch: 1 step: 5041, loss is 0.9154151082038879\n",
            "epoch: 1 step: 5042, loss is 0.7562049627304077\n",
            "epoch: 1 step: 5043, loss is 0.7951851487159729\n",
            "epoch: 1 step: 5044, loss is 0.8117762804031372\n",
            "epoch: 1 step: 5045, loss is 0.8786841034889221\n",
            "epoch: 1 step: 5046, loss is 0.8861499428749084\n",
            "epoch: 1 step: 5047, loss is 0.888878583908081\n",
            "epoch: 1 step: 5048, loss is 0.7755595445632935\n",
            "epoch: 1 step: 5049, loss is 0.8725417256355286\n",
            "epoch: 1 step: 5050, loss is 1.00322687625885\n",
            "epoch: 1 step: 5051, loss is 0.8796764016151428\n",
            "epoch: 1 step: 5052, loss is 0.9826700687408447\n",
            "epoch: 1 step: 5053, loss is 0.8305088877677917\n",
            "epoch: 1 step: 5054, loss is 0.9136776924133301\n",
            "epoch: 1 step: 5055, loss is 0.8264020681381226\n",
            "epoch: 1 step: 5056, loss is 0.7613284587860107\n",
            "epoch: 1 step: 5057, loss is 0.8337242603302002\n",
            "epoch: 1 step: 5058, loss is 0.8656237721443176\n",
            "epoch: 1 step: 5059, loss is 0.8643394708633423\n",
            "epoch: 1 step: 5060, loss is 0.8350792527198792\n",
            "epoch: 1 step: 5061, loss is 0.9603444337844849\n",
            "epoch: 1 step: 5062, loss is 0.8884891867637634\n",
            "epoch: 1 step: 5063, loss is 0.9745526313781738\n",
            "epoch: 1 step: 5064, loss is 0.9747463464736938\n",
            "epoch: 1 step: 5065, loss is 0.8042181730270386\n",
            "epoch: 1 step: 5066, loss is 0.9797843098640442\n",
            "epoch: 1 step: 5067, loss is 1.043290138244629\n",
            "epoch: 1 step: 5068, loss is 0.786081075668335\n",
            "epoch: 1 step: 5069, loss is 0.9540115594863892\n",
            "epoch: 1 step: 5070, loss is 0.9079853892326355\n",
            "epoch: 1 step: 5071, loss is 0.9003462195396423\n",
            "epoch: 1 step: 5072, loss is 0.8012046217918396\n",
            "epoch: 1 step: 5073, loss is 1.1099061965942383\n",
            "epoch: 1 step: 5074, loss is 0.967319905757904\n",
            "epoch: 1 step: 5075, loss is 0.9223505258560181\n",
            "epoch: 1 step: 5076, loss is 0.8634804487228394\n",
            "epoch: 1 step: 5077, loss is 0.9179627299308777\n",
            "epoch: 1 step: 5078, loss is 0.6669769287109375\n",
            "epoch: 1 step: 5079, loss is 0.9037904143333435\n",
            "epoch: 1 step: 5080, loss is 0.8817970156669617\n",
            "epoch: 1 step: 5081, loss is 0.8572883009910583\n",
            "epoch: 1 step: 5082, loss is 0.9425715804100037\n",
            "epoch: 1 step: 5083, loss is 0.8595032691955566\n",
            "epoch: 1 step: 5084, loss is 0.9423428177833557\n",
            "epoch: 1 step: 5085, loss is 0.7775446772575378\n",
            "epoch: 1 step: 5086, loss is 1.067755937576294\n",
            "epoch: 1 step: 5087, loss is 0.7872543334960938\n",
            "epoch: 1 step: 5088, loss is 0.9733625054359436\n",
            "epoch: 1 step: 5089, loss is 0.7380113005638123\n",
            "epoch: 1 step: 5090, loss is 0.8480859398841858\n",
            "epoch: 1 step: 5091, loss is 0.7557985782623291\n",
            "epoch: 1 step: 5092, loss is 0.8299103379249573\n",
            "epoch: 1 step: 5093, loss is 0.9747983813285828\n",
            "epoch: 1 step: 5094, loss is 0.8661240339279175\n",
            "epoch: 1 step: 5095, loss is 0.7799311280250549\n",
            "epoch: 1 step: 5096, loss is 0.940971851348877\n",
            "epoch: 1 step: 5097, loss is 0.8468278050422668\n",
            "epoch: 1 step: 5098, loss is 0.972301721572876\n",
            "epoch: 1 step: 5099, loss is 0.8292054533958435\n",
            "epoch: 1 step: 5100, loss is 0.8399280309677124\n",
            "epoch: 1 step: 5101, loss is 0.7487614750862122\n",
            "epoch: 1 step: 5102, loss is 1.0574593544006348\n",
            "epoch: 1 step: 5103, loss is 0.7153674960136414\n",
            "epoch: 1 step: 5104, loss is 0.9004682302474976\n",
            "epoch: 1 step: 5105, loss is 0.9494340419769287\n",
            "epoch: 1 step: 5106, loss is 0.9693243503570557\n",
            "epoch: 1 step: 5107, loss is 1.1218048334121704\n",
            "epoch: 1 step: 5108, loss is 0.902308464050293\n",
            "epoch: 1 step: 5109, loss is 0.9865590333938599\n",
            "epoch: 1 step: 5110, loss is 0.8001125454902649\n",
            "epoch: 1 step: 5111, loss is 1.014014482498169\n",
            "epoch: 1 step: 5112, loss is 0.7455025911331177\n",
            "epoch: 1 step: 5113, loss is 0.9972975254058838\n",
            "epoch: 1 step: 5114, loss is 0.8792480826377869\n",
            "epoch: 1 step: 5115, loss is 0.9100267291069031\n",
            "epoch: 1 step: 5116, loss is 0.9407961964607239\n",
            "epoch: 1 step: 5117, loss is 0.818694531917572\n",
            "epoch: 1 step: 5118, loss is 1.0409594774246216\n",
            "epoch: 1 step: 5119, loss is 1.2151479721069336\n",
            "epoch: 1 step: 5120, loss is 0.9406227469444275\n",
            "epoch: 1 step: 5121, loss is 0.9361881613731384\n",
            "epoch: 1 step: 5122, loss is 0.8270904421806335\n",
            "epoch: 1 step: 5123, loss is 1.0571472644805908\n",
            "epoch: 1 step: 5124, loss is 0.7119760513305664\n",
            "epoch: 1 step: 5125, loss is 0.8455080389976501\n",
            "epoch: 1 step: 5126, loss is 0.890389084815979\n",
            "epoch: 1 step: 5127, loss is 0.8680769801139832\n",
            "epoch: 1 step: 5128, loss is 0.7740358710289001\n",
            "epoch: 1 step: 5129, loss is 0.784527063369751\n",
            "epoch: 1 step: 5130, loss is 0.9440527558326721\n",
            "epoch: 1 step: 5131, loss is 0.7194686532020569\n",
            "epoch: 1 step: 5132, loss is 0.794867992401123\n",
            "epoch: 1 step: 5133, loss is 0.9766082167625427\n",
            "epoch: 1 step: 5134, loss is 0.9078030586242676\n",
            "epoch: 1 step: 5135, loss is 0.8957845568656921\n",
            "epoch: 1 step: 5136, loss is 0.8199533820152283\n",
            "epoch: 1 step: 5137, loss is 0.796815037727356\n",
            "epoch: 1 step: 5138, loss is 1.0186549425125122\n",
            "epoch: 1 step: 5139, loss is 0.8757632374763489\n",
            "epoch: 1 step: 5140, loss is 0.862261950969696\n",
            "epoch: 1 step: 5141, loss is 0.7586016654968262\n",
            "epoch: 1 step: 5142, loss is 0.8715056777000427\n",
            "epoch: 1 step: 5143, loss is 0.9746454358100891\n",
            "epoch: 1 step: 5144, loss is 0.8424331545829773\n",
            "epoch: 1 step: 5145, loss is 1.1006988286972046\n",
            "epoch: 1 step: 5146, loss is 0.9532379508018494\n",
            "epoch: 1 step: 5147, loss is 0.9408223628997803\n",
            "epoch: 1 step: 5148, loss is 0.7900539636611938\n",
            "epoch: 1 step: 5149, loss is 0.8048842549324036\n",
            "epoch: 1 step: 5150, loss is 0.8618237972259521\n",
            "epoch: 1 step: 5151, loss is 0.7043805718421936\n",
            "epoch: 1 step: 5152, loss is 0.9491031765937805\n",
            "epoch: 1 step: 5153, loss is 0.9811331629753113\n",
            "epoch: 1 step: 5154, loss is 0.8964727520942688\n",
            "epoch: 1 step: 5155, loss is 0.8522210717201233\n",
            "epoch: 1 step: 5156, loss is 0.9697257876396179\n",
            "epoch: 1 step: 5157, loss is 0.793755829334259\n",
            "epoch: 1 step: 5158, loss is 1.0419209003448486\n",
            "epoch: 1 step: 5159, loss is 0.8769696950912476\n",
            "epoch: 1 step: 5160, loss is 0.8160912394523621\n",
            "epoch: 1 step: 5161, loss is 1.013528823852539\n",
            "epoch: 1 step: 5162, loss is 0.9528482556343079\n",
            "epoch: 1 step: 5163, loss is 0.8525950908660889\n",
            "epoch: 1 step: 5164, loss is 0.9872695803642273\n",
            "epoch: 1 step: 5165, loss is 0.8953713774681091\n",
            "epoch: 1 step: 5166, loss is 0.8388771414756775\n",
            "epoch: 1 step: 5167, loss is 0.7717908620834351\n",
            "epoch: 1 step: 5168, loss is 1.209236979484558\n",
            "epoch: 1 step: 5169, loss is 0.8758105635643005\n",
            "epoch: 1 step: 5170, loss is 0.8349003195762634\n",
            "epoch: 1 step: 5171, loss is 0.8207786083221436\n",
            "epoch: 1 step: 5172, loss is 0.8172788619995117\n",
            "epoch: 1 step: 5173, loss is 0.8949883580207825\n",
            "epoch: 1 step: 5174, loss is 0.8908985257148743\n",
            "epoch: 1 step: 5175, loss is 0.9686131477355957\n",
            "epoch: 1 step: 5176, loss is 0.7659711837768555\n",
            "epoch: 1 step: 5177, loss is 0.8569733500480652\n",
            "epoch: 1 step: 5178, loss is 0.8031187653541565\n",
            "epoch: 1 step: 5179, loss is 0.9059221744537354\n",
            "epoch: 1 step: 5180, loss is 0.8213557600975037\n",
            "epoch: 1 step: 5181, loss is 0.963923454284668\n",
            "epoch: 1 step: 5182, loss is 1.0442124605178833\n",
            "epoch: 1 step: 5183, loss is 0.9357506036758423\n",
            "epoch: 1 step: 5184, loss is 0.868249773979187\n",
            "epoch: 1 step: 5185, loss is 0.9929099678993225\n",
            "epoch: 1 step: 5186, loss is 0.7924058437347412\n",
            "epoch: 1 step: 5187, loss is 0.819149911403656\n",
            "epoch: 1 step: 5188, loss is 0.8690751194953918\n",
            "epoch: 1 step: 5189, loss is 0.8413696885108948\n",
            "epoch: 1 step: 5190, loss is 0.9950708150863647\n",
            "epoch: 1 step: 5191, loss is 0.8698503971099854\n",
            "epoch: 1 step: 5192, loss is 0.9972444176673889\n",
            "epoch: 1 step: 5193, loss is 1.046925663948059\n",
            "epoch: 1 step: 5194, loss is 0.9178761839866638\n",
            "epoch: 1 step: 5195, loss is 0.7890170216560364\n",
            "epoch: 1 step: 5196, loss is 0.775242269039154\n",
            "epoch: 1 step: 5197, loss is 0.8018983602523804\n",
            "epoch: 1 step: 5198, loss is 0.9537963271141052\n",
            "epoch: 1 step: 5199, loss is 0.7636292576789856\n",
            "epoch: 1 step: 5200, loss is 0.9706164598464966\n",
            "epoch: 1 step: 5201, loss is 0.8148301839828491\n",
            "epoch: 1 step: 5202, loss is 1.081109642982483\n",
            "epoch: 1 step: 5203, loss is 0.8640729188919067\n",
            "epoch: 1 step: 5204, loss is 0.8683260679244995\n",
            "epoch: 1 step: 5205, loss is 0.9474244117736816\n",
            "epoch: 1 step: 5206, loss is 0.8781054019927979\n",
            "epoch: 1 step: 5207, loss is 0.9377905130386353\n",
            "epoch: 1 step: 5208, loss is 0.9155659079551697\n",
            "epoch: 1 step: 5209, loss is 0.939850926399231\n",
            "epoch: 1 step: 5210, loss is 0.9565436840057373\n",
            "epoch: 1 step: 5211, loss is 1.1098965406417847\n",
            "epoch: 1 step: 5212, loss is 0.9200145602226257\n",
            "epoch: 1 step: 5213, loss is 1.1079227924346924\n",
            "epoch: 1 step: 5214, loss is 1.0995107889175415\n",
            "epoch: 1 step: 5215, loss is 0.7924217581748962\n",
            "epoch: 1 step: 5216, loss is 0.9630393385887146\n",
            "epoch: 1 step: 5217, loss is 0.8053468465805054\n",
            "epoch: 1 step: 5218, loss is 0.8055800199508667\n",
            "epoch: 1 step: 5219, loss is 0.8252890110015869\n",
            "epoch: 1 step: 5220, loss is 0.7926058769226074\n",
            "epoch: 1 step: 5221, loss is 0.9775194525718689\n",
            "epoch: 1 step: 5222, loss is 0.8117097020149231\n",
            "epoch: 1 step: 5223, loss is 0.9025362730026245\n",
            "epoch: 1 step: 5224, loss is 0.7366496324539185\n",
            "epoch: 1 step: 5225, loss is 0.9135547876358032\n",
            "epoch: 1 step: 5226, loss is 0.9640849232673645\n",
            "epoch: 1 step: 5227, loss is 0.9963728189468384\n",
            "epoch: 1 step: 5228, loss is 0.9585345983505249\n",
            "epoch: 1 step: 5229, loss is 0.8938567042350769\n",
            "epoch: 1 step: 5230, loss is 0.8595890998840332\n",
            "epoch: 1 step: 5231, loss is 0.8330224752426147\n",
            "epoch: 1 step: 5232, loss is 0.8530821800231934\n",
            "epoch: 1 step: 5233, loss is 1.0379979610443115\n",
            "epoch: 1 step: 5234, loss is 0.7277785539627075\n",
            "epoch: 1 step: 5235, loss is 0.8882197737693787\n",
            "epoch: 1 step: 5236, loss is 0.8624451160430908\n",
            "epoch: 1 step: 5237, loss is 0.9710628986358643\n",
            "epoch: 1 step: 5238, loss is 0.7420235872268677\n",
            "epoch: 1 step: 5239, loss is 0.9487901926040649\n",
            "epoch: 1 step: 5240, loss is 0.8056617975234985\n",
            "epoch: 1 step: 5241, loss is 0.8471324443817139\n",
            "epoch: 1 step: 5242, loss is 0.9540446400642395\n",
            "epoch: 1 step: 5243, loss is 0.9215923547744751\n",
            "epoch: 1 step: 5244, loss is 0.876924991607666\n",
            "epoch: 1 step: 5245, loss is 0.8306610584259033\n",
            "epoch: 1 step: 5246, loss is 0.9104344844818115\n",
            "epoch: 1 step: 5247, loss is 0.8303198218345642\n",
            "epoch: 1 step: 5248, loss is 0.7535921335220337\n",
            "epoch: 1 step: 5249, loss is 0.7229116559028625\n",
            "epoch: 1 step: 5250, loss is 0.9661505818367004\n",
            "epoch: 1 step: 5251, loss is 0.8144851326942444\n",
            "epoch: 1 step: 5252, loss is 0.8584359884262085\n",
            "epoch: 1 step: 5253, loss is 0.7861418128013611\n",
            "epoch: 1 step: 5254, loss is 1.06993567943573\n",
            "epoch: 1 step: 5255, loss is 0.893524169921875\n",
            "epoch: 1 step: 5256, loss is 0.9240077137947083\n",
            "epoch: 1 step: 5257, loss is 0.8967801332473755\n",
            "epoch: 1 step: 5258, loss is 0.9653910398483276\n",
            "epoch: 1 step: 5259, loss is 0.9296411871910095\n",
            "epoch: 1 step: 5260, loss is 0.9150750041007996\n",
            "epoch: 1 step: 5261, loss is 0.8695324659347534\n",
            "epoch: 1 step: 5262, loss is 0.9180171489715576\n",
            "epoch: 1 step: 5263, loss is 0.8439809679985046\n",
            "epoch: 1 step: 5264, loss is 1.050096035003662\n",
            "epoch: 1 step: 5265, loss is 1.0221823453903198\n",
            "epoch: 1 step: 5266, loss is 0.9149699807167053\n",
            "epoch: 1 step: 5267, loss is 0.8481244444847107\n",
            "epoch: 1 step: 5268, loss is 0.9608806371688843\n",
            "epoch: 1 step: 5269, loss is 0.9760769009590149\n",
            "epoch: 1 step: 5270, loss is 0.9067596197128296\n",
            "epoch: 1 step: 5271, loss is 0.8525466322898865\n",
            "epoch: 1 step: 5272, loss is 0.9316968321800232\n",
            "epoch: 1 step: 5273, loss is 0.958135187625885\n",
            "epoch: 1 step: 5274, loss is 0.9118939638137817\n",
            "epoch: 1 step: 5275, loss is 1.1508522033691406\n",
            "epoch: 1 step: 5276, loss is 0.8242928385734558\n",
            "epoch: 1 step: 5277, loss is 0.9213570356369019\n",
            "epoch: 1 step: 5278, loss is 0.9032670259475708\n",
            "epoch: 1 step: 5279, loss is 0.9039275050163269\n",
            "epoch: 1 step: 5280, loss is 0.8614603281021118\n",
            "epoch: 1 step: 5281, loss is 0.953895628452301\n",
            "epoch: 1 step: 5282, loss is 0.9263041615486145\n",
            "epoch: 1 step: 5283, loss is 0.9190621376037598\n",
            "epoch: 1 step: 5284, loss is 0.7243130803108215\n",
            "epoch: 1 step: 5285, loss is 0.8828796148300171\n",
            "epoch: 1 step: 5286, loss is 0.7844995260238647\n",
            "epoch: 1 step: 5287, loss is 0.8237797021865845\n",
            "epoch: 1 step: 5288, loss is 0.8165972828865051\n",
            "epoch: 1 step: 5289, loss is 0.9282670021057129\n",
            "epoch: 1 step: 5290, loss is 0.9698668718338013\n",
            "epoch: 1 step: 5291, loss is 0.8241870403289795\n",
            "epoch: 1 step: 5292, loss is 1.1054608821868896\n",
            "epoch: 1 step: 5293, loss is 0.7771847248077393\n",
            "epoch: 1 step: 5294, loss is 0.8654021620750427\n",
            "epoch: 1 step: 5295, loss is 0.8601502180099487\n",
            "epoch: 1 step: 5296, loss is 0.9718363881111145\n",
            "epoch: 1 step: 5297, loss is 0.9806477427482605\n",
            "epoch: 1 step: 5298, loss is 1.0665377378463745\n",
            "epoch: 1 step: 5299, loss is 0.8384835124015808\n",
            "epoch: 1 step: 5300, loss is 1.0167428255081177\n",
            "epoch: 1 step: 5301, loss is 0.9237515926361084\n"
          ]
        }
      ]
    }
  ]
}